{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26165,"status":"ok","timestamp":1717520824011,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"aWO1vAVp7HkG","outputId":"b6e4a52f-ea8a-4492-97b2-4d4147b862e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4839,"status":"ok","timestamp":1717520846533,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"6I2s5Q0iDpDE"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.models import Model"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10774,"status":"ok","timestamp":1717520857303,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"WtJVQemz7klg"},"outputs":[],"source":["# Load data\n","data = pd.read_csv('/content/drive/My Drive/M2 GL/PFE/Data/hisb_60_and_Al_fatihah_audio_with_transcript_and_MFCC_and_ahkam_indexing_v3.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1717520857304,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"7A9PSizPHQos"},"outputs":[],"source":["export_dir = '/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v3'"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1717520857305,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"KoKbTBQr7nY8"},"outputs":[],"source":["abdul_basit = data[data['recitor_en'] == 'Abdul Basit']\n","yassin_aljazaery = data[data['recitor_en'] == 'Yassin Al Jazaery']\n","ibrahim_aldosary = data[data['recitor_en'] == 'Ibrahim_Aldosary']\n","al_husary = data[data['recitor_en'] == 'Al husary']"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1717520857305,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"_CenkKQf-AXc"},"outputs":[],"source":["splitted_data_info_np = np.empty((0, 6))\n","models_information_np = np.empty((0, 5))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1717520857306,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"H950tSVTMxlL"},"outputs":[],"source":["max_Y = {'madd_6_Lazim': 2, 'madd_246': 3, 'madd_6': 6, 'madd_2': 5, 'Ikhfaa': 9, 'Idgham': 13, 'tafkhim': 24, 'qalqala': 6, 'imala': 7}\n","max_X = 8000"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1717520857306,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"pVbn5z76-K4O"},"outputs":[],"source":["def data_preparation(reciter_data, tajweed_rule):\n","  data_filtered = reciter_data[reciter_data[tajweed_rule].apply(lambda x: x != '[-1]')]\n","\n","  # Extract 'mfcc' and tajweed_rule columns as lists of strings\n","  X_raw = data_filtered['mfcc'].astype(str).tolist()\n","  Y_raw = data_filtered[tajweed_rule].astype(str).tolist()\n","\n","  # Preprocess the input data\n","  X = [tf.constant(eval(x)) for x in X_raw]\n","  Y = [tf.constant(eval(x)) for x in Y_raw]\n","\n","  # Pad sequences in Y and in X to ensure all have the same length\n","  Y_padded = tf.keras.preprocessing.sequence.pad_sequences(Y, maxlen=max_Y[tajweed_rule], padding='post', dtype='int32', value=-1)\n","  X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_X, padding='post', dtype='float32')\n","\n","  # Split the data into training and testing sets\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_padded, test_size=0.2, random_state=10)\n","  return X_train, X_test, Y_train, Y_test"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1717520857307,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"n3ycP8uz8zp8"},"outputs":[],"source":["def tajweed_rule_model(reciter1, reciter2, reciter3, reciter4, tajweed_rule):\n","  global splitted_data_info_np, models_information_np, data\n","  # data preparation\n","  reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test = data_preparation(reciter1, tajweed_rule)\n","  reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test = data_preparation(reciter2, tajweed_rule)\n","  reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test = data_preparation(reciter3, tajweed_rule)\n","  reciter4_X_train, reciter4_X_test, reciter4_Y_train, reciter4_Y_test = data_preparation(reciter4, tajweed_rule)\n","\n","  # Update splitted_data_info with information about each reciter\n","  for reciter_X_train, reciter_X_test, reciter_Y_train, reciter_Y_test, reciter_data in [\n","      (reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test, reciter1),\n","      (reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test, reciter2),\n","      (reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test, reciter3),\n","      (reciter4_X_train, reciter4_X_test, reciter4_Y_train, reciter4_Y_test, reciter4)]:\n","\n","      splitted_data_info_np = np.append(splitted_data_info_np, [[\n","              tajweed_rule,\n","              reciter_data.iloc[0]['recitor_en'],\n","              len(reciter_X_train),\n","              len(reciter_X_test),\n","              len(reciter_Y_train),\n","              len(reciter_Y_test)\n","              ]], axis=0)\n","\n","  # concatenate data\n","  # training data\n","  X_train = np.concatenate([reciter1_X_train, reciter2_X_train, reciter3_X_train, reciter4_X_train], axis=0)\n","  Y_train = np.concatenate([reciter1_Y_train, reciter2_Y_train, reciter3_Y_train, reciter4_Y_train], axis=0)\n","\n","  # testing data\n","  X_test = np.concatenate([reciter1_X_test, reciter2_X_test, reciter3_X_test, reciter4_X_test], axis=0)\n","  Y_test = np.concatenate([reciter1_Y_test, reciter2_Y_test, reciter3_Y_test, reciter4_Y_test], axis=0)\n","\n","  splitted_data_info_np = np.append(splitted_data_info_np, [[\n","          tajweed_rule,\n","          'all reciters',\n","          len(X_train),\n","          len(X_test),\n","          len(Y_train),\n","          len(Y_test)\n","          ]], axis=0)\n","\n","  # Normalize input data by scaling each sequence individually\n","  scaler = StandardScaler()\n","  X_train_scaled = np.array([scaler.fit_transform(seq) for seq in X_train])\n","  X_test_scaled = np.array([scaler.transform(seq) for seq in X_test])\n","\n","  # Define a simple neural network model\n","  input_shape = X_train_scaled[0].shape  # Shape of each mfcc sequence\n","  output_shape = Y_train.shape[1]  # Dimension of output (number of units in output layer)\n","\n","  input_layer = Input(shape=input_shape)\n","  flatten_layer = Flatten()(input_layer)  # Flatten the sequence to a 1D vector\n","  hidden_layer = Dense(64, activation='relu')(flatten_layer)\n","  output_layer = Dense(output_shape, activation='linear')(hidden_layer)\n","\n","  model = Model(inputs=input_layer, outputs=output_layer)\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n","\n","  # Train the model\n","  model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, validation_split=0.1)\n","\n","  #export the model\n","  model_filename = f'{tajweed_rule}_tajweed_rule_model'\n","  model_path = os.path.join(export_dir, model_filename)\n","  keras.models.save_model(model, model_path)\n","\n","  # Make predictions on test data\n","  predictions = model.predict(X_test_scaled)\n","\n","  # Evaluate the model with adjusted predictions\n","  predictions[predictions < 0] = -1\n","  predictions = np.round(predictions).astype('int32')\n","  loss, accuracy = model.evaluate(X_test_scaled, predictions)\n","\n","  print(f\"Test Loss: {loss:.4f}, Test accuracy : {accuracy:.4f}\")\n","  models_information_np = np.append(models_information_np, [[\n","          model_filename,\n","          \"{:.4f}\".format(loss),\n","          \"{:.4f}\".format(accuracy),\n","          \"{:.2f}\".format(accuracy*100),\n","          model_path]], axis=0)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":735079,"status":"ok","timestamp":1717521592378,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"n6qxa6hWLAeZ","outputId":"c5bd1889-3940-4cb1-ec1b-1eed3fc8be27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","1/1 [==============================] - 1s 1s/step - loss: 193.8395 - accuracy: 0.6667 - val_loss: 18.3559 - val_accuracy: 1.0000\n","Epoch 2/50\n","1/1 [==============================] - 0s 111ms/step - loss: 1187.8859 - accuracy: 1.0000 - val_loss: 59.2315 - val_accuracy: 0.0000e+00\n","Epoch 3/50\n","1/1 [==============================] - 0s 103ms/step - loss: 1443.8926 - accuracy: 0.0000e+00 - val_loss: 111.4493 - val_accuracy: 0.0000e+00\n","Epoch 4/50\n","1/1 [==============================] - 0s 100ms/step - loss: 284.3051 - accuracy: 0.0000e+00 - val_loss: 146.1148 - val_accuracy: 1.0000\n","Epoch 5/50\n","1/1 [==============================] - 0s 114ms/step - loss: 61.5881 - accuracy: 0.6667 - val_loss: 193.3012 - val_accuracy: 1.0000\n","Epoch 6/50\n","1/1 [==============================] - 0s 108ms/step - loss: 237.1864 - accuracy: 1.0000 - val_loss: 200.1121 - val_accuracy: 1.0000\n","Epoch 7/50\n","1/1 [==============================] - 0s 102ms/step - loss: 231.6621 - accuracy: 1.0000 - val_loss: 185.4765 - val_accuracy: 1.0000\n","Epoch 8/50\n","1/1 [==============================] - 0s 115ms/step - loss: 110.6906 - accuracy: 1.0000 - val_loss: 172.4749 - val_accuracy: 1.0000\n","Epoch 9/50\n","1/1 [==============================] - 0s 102ms/step - loss: 76.3102 - accuracy: 0.3333 - val_loss: 168.3615 - val_accuracy: 0.0000e+00\n","Epoch 10/50\n","1/1 [==============================] - 0s 107ms/step - loss: 108.9306 - accuracy: 0.3333 - val_loss: 167.9754 - val_accuracy: 0.0000e+00\n","Epoch 11/50\n","1/1 [==============================] - 0s 98ms/step - loss: 106.6016 - accuracy: 0.0000e+00 - val_loss: 171.7749 - val_accuracy: 1.0000\n","Epoch 12/50\n","1/1 [==============================] - 0s 116ms/step - loss: 74.4564 - accuracy: 0.0000e+00 - val_loss: 179.7013 - val_accuracy: 1.0000\n","Epoch 13/50\n","1/1 [==============================] - 0s 113ms/step - loss: 65.8888 - accuracy: 0.6667 - val_loss: 187.4022 - val_accuracy: 1.0000\n","Epoch 14/50\n","1/1 [==============================] - 0s 108ms/step - loss: 80.7389 - accuracy: 0.6667 - val_loss: 187.3448 - val_accuracy: 1.0000\n","Epoch 15/50\n","1/1 [==============================] - 0s 106ms/step - loss: 74.9002 - accuracy: 0.6667 - val_loss: 179.5201 - val_accuracy: 1.0000\n","Epoch 16/50\n","1/1 [==============================] - 0s 108ms/step - loss: 42.9631 - accuracy: 0.6667 - val_loss: 167.4237 - val_accuracy: 1.0000\n","Epoch 17/50\n","1/1 [==============================] - 0s 99ms/step - loss: 23.9823 - accuracy: 0.6667 - val_loss: 157.4979 - val_accuracy: 1.0000\n","Epoch 18/50\n","1/1 [==============================] - 0s 109ms/step - loss: 37.4385 - accuracy: 0.3333 - val_loss: 153.2897 - val_accuracy: 1.0000\n","Epoch 19/50\n","1/1 [==============================] - 0s 99ms/step - loss: 56.0770 - accuracy: 0.3333 - val_loss: 154.4795 - val_accuracy: 1.0000\n","Epoch 20/50\n","1/1 [==============================] - 0s 104ms/step - loss: 50.1691 - accuracy: 0.3333 - val_loss: 159.9381 - val_accuracy: 1.0000\n","Epoch 21/50\n","1/1 [==============================] - 0s 93ms/step - loss: 27.4802 - accuracy: 0.6667 - val_loss: 167.4122 - val_accuracy: 1.0000\n","Epoch 22/50\n","1/1 [==============================] - 0s 111ms/step - loss: 14.1160 - accuracy: 1.0000 - val_loss: 172.7568 - val_accuracy: 1.0000\n","Epoch 23/50\n","1/1 [==============================] - 0s 99ms/step - loss: 17.9910 - accuracy: 0.6667 - val_loss: 173.2154 - val_accuracy: 1.0000\n","Epoch 24/50\n","1/1 [==============================] - 0s 99ms/step - loss: 25.4066 - accuracy: 0.6667 - val_loss: 170.0342 - val_accuracy: 1.0000\n","Epoch 25/50\n","1/1 [==============================] - 0s 109ms/step - loss: 24.2640 - accuracy: 0.3333 - val_loss: 166.6625 - val_accuracy: 1.0000\n","Epoch 26/50\n","1/1 [==============================] - 0s 125ms/step - loss: 16.3299 - accuracy: 0.3333 - val_loss: 165.3034 - val_accuracy: 1.0000\n","Epoch 27/50\n","1/1 [==============================] - 0s 115ms/step - loss: 9.1595 - accuracy: 0.0000e+00 - val_loss: 166.4645 - val_accuracy: 1.0000\n","Epoch 28/50\n","1/1 [==============================] - 0s 103ms/step - loss: 6.5652 - accuracy: 0.0000e+00 - val_loss: 169.7394 - val_accuracy: 1.0000\n","Epoch 29/50\n","1/1 [==============================] - 0s 107ms/step - loss: 7.7438 - accuracy: 0.6667 - val_loss: 174.1128 - val_accuracy: 1.0000\n","Epoch 30/50\n","1/1 [==============================] - 0s 105ms/step - loss: 9.7426 - accuracy: 0.6667 - val_loss: 177.8694 - val_accuracy: 1.0000\n","Epoch 31/50\n","1/1 [==============================] - 0s 97ms/step - loss: 9.9015 - accuracy: 0.6667 - val_loss: 179.4619 - val_accuracy: 1.0000\n","Epoch 32/50\n","1/1 [==============================] - 0s 98ms/step - loss: 8.2485 - accuracy: 1.0000 - val_loss: 178.7788 - val_accuracy: 1.0000\n","Epoch 33/50\n","1/1 [==============================] - 0s 102ms/step - loss: 6.9746 - accuracy: 1.0000 - val_loss: 177.0650 - val_accuracy: 1.0000\n","Epoch 34/50\n","1/1 [==============================] - 0s 99ms/step - loss: 6.9231 - accuracy: 0.6667 - val_loss: 175.6532 - val_accuracy: 1.0000\n","Epoch 35/50\n","1/1 [==============================] - 0s 131ms/step - loss: 6.8718 - accuracy: 0.3333 - val_loss: 175.1727 - val_accuracy: 1.0000\n","Epoch 36/50\n","1/1 [==============================] - 0s 145ms/step - loss: 6.6671 - accuracy: 0.3333 - val_loss: 175.6497 - val_accuracy: 1.0000\n","Epoch 37/50\n","1/1 [==============================] - 0s 168ms/step - loss: 7.3316 - accuracy: 0.3333 - val_loss: 176.6390 - val_accuracy: 1.0000\n","Epoch 38/50\n","1/1 [==============================] - 0s 136ms/step - loss: 7.8095 - accuracy: 0.3333 - val_loss: 177.2914 - val_accuracy: 1.0000\n","Epoch 39/50\n","1/1 [==============================] - 0s 149ms/step - loss: 5.8858 - accuracy: 0.3333 - val_loss: 176.8797 - val_accuracy: 1.0000\n","Epoch 40/50\n","1/1 [==============================] - 0s 137ms/step - loss: 2.9358 - accuracy: 1.0000 - val_loss: 175.4251 - val_accuracy: 1.0000\n","Epoch 41/50\n","1/1 [==============================] - 0s 150ms/step - loss: 2.7261 - accuracy: 1.0000 - val_loss: 173.6352 - val_accuracy: 1.0000\n","Epoch 42/50\n","1/1 [==============================] - 0s 147ms/step - loss: 4.9012 - accuracy: 0.6667 - val_loss: 172.2841 - val_accuracy: 1.0000\n","Epoch 43/50\n","1/1 [==============================] - 0s 155ms/step - loss: 5.0956 - accuracy: 0.6667 - val_loss: 171.7524 - val_accuracy: 1.0000\n","Epoch 44/50\n","1/1 [==============================] - 0s 139ms/step - loss: 2.3867 - accuracy: 0.6667 - val_loss: 171.9260 - val_accuracy: 1.0000\n","Epoch 45/50\n","1/1 [==============================] - 0s 156ms/step - loss: 0.7335 - accuracy: 1.0000 - val_loss: 172.2881 - val_accuracy: 1.0000\n","Epoch 46/50\n","1/1 [==============================] - 0s 141ms/step - loss: 1.9473 - accuracy: 0.3333 - val_loss: 172.2670 - val_accuracy: 1.0000\n","Epoch 47/50\n","1/1 [==============================] - 0s 132ms/step - loss: 3.0825 - accuracy: 0.3333 - val_loss: 171.7241 - val_accuracy: 1.0000\n","Epoch 48/50\n","1/1 [==============================] - 0s 157ms/step - loss: 2.1667 - accuracy: 0.3333 - val_loss: 171.0629 - val_accuracy: 1.0000\n","Epoch 49/50\n","1/1 [==============================] - 0s 128ms/step - loss: 1.0290 - accuracy: 0.6667 - val_loss: 170.8359 - val_accuracy: 1.0000\n","Epoch 50/50\n","1/1 [==============================] - 0s 152ms/step - loss: 1.1286 - accuracy: 0.6667 - val_loss: 171.3145 - val_accuracy: 1.0000\n","1/1 [==============================] - 0s 76ms/step\n","1/1 [==============================] - 0s 28ms/step - loss: 62.7340 - accuracy: 1.0000\n","Test Loss: 62.7340, Test accuracy : 1.0000\n","Epoch 1/50\n","7/7 [==============================] - 2s 177ms/step - loss: 373.9070 - accuracy: 0.7265 - val_loss: 207.5334 - val_accuracy: 0.8400\n","Epoch 2/50\n","7/7 [==============================] - 1s 93ms/step - loss: 183.1541 - accuracy: 0.8700 - val_loss: 199.7566 - val_accuracy: 0.8400\n","Epoch 3/50\n","7/7 [==============================] - 1s 96ms/step - loss: 121.6811 - accuracy: 0.8700 - val_loss: 154.3231 - val_accuracy: 0.8800\n","Epoch 4/50\n","7/7 [==============================] - 1s 130ms/step - loss: 92.7914 - accuracy: 0.8700 - val_loss: 141.4343 - val_accuracy: 0.8800\n","Epoch 5/50\n","7/7 [==============================] - 1s 124ms/step - loss: 70.0149 - accuracy: 0.8789 - val_loss: 137.4932 - val_accuracy: 0.8800\n","Epoch 6/50\n","7/7 [==============================] - 1s 128ms/step - loss: 54.5568 - accuracy: 0.8969 - val_loss: 139.8685 - val_accuracy: 0.8800\n","Epoch 7/50\n","7/7 [==============================] - 1s 138ms/step - loss: 43.9324 - accuracy: 0.8969 - val_loss: 138.8351 - val_accuracy: 0.8800\n","Epoch 8/50\n","7/7 [==============================] - 1s 143ms/step - loss: 35.8002 - accuracy: 0.9058 - val_loss: 139.5997 - val_accuracy: 0.8800\n","Epoch 9/50\n","7/7 [==============================] - 1s 119ms/step - loss: 28.0504 - accuracy: 0.9148 - val_loss: 138.1081 - val_accuracy: 0.8800\n","Epoch 10/50\n","7/7 [==============================] - 1s 96ms/step - loss: 24.4482 - accuracy: 0.9238 - val_loss: 136.1425 - val_accuracy: 0.8800\n","Epoch 11/50\n","7/7 [==============================] - 1s 96ms/step - loss: 20.3717 - accuracy: 0.9327 - val_loss: 136.6260 - val_accuracy: 0.8800\n","Epoch 12/50\n","7/7 [==============================] - 1s 93ms/step - loss: 16.2511 - accuracy: 0.9372 - val_loss: 138.6690 - val_accuracy: 0.8800\n","Epoch 13/50\n","7/7 [==============================] - 1s 92ms/step - loss: 13.8802 - accuracy: 0.9372 - val_loss: 141.0842 - val_accuracy: 0.8400\n","Epoch 14/50\n","7/7 [==============================] - 1s 93ms/step - loss: 11.8255 - accuracy: 0.9417 - val_loss: 138.1855 - val_accuracy: 0.8800\n","Epoch 15/50\n","7/7 [==============================] - 1s 89ms/step - loss: 10.0688 - accuracy: 0.9462 - val_loss: 138.1548 - val_accuracy: 0.8400\n","Epoch 16/50\n","7/7 [==============================] - 1s 93ms/step - loss: 8.9114 - accuracy: 0.9507 - val_loss: 140.7820 - val_accuracy: 0.8400\n","Epoch 17/50\n","7/7 [==============================] - 1s 98ms/step - loss: 7.6378 - accuracy: 0.9641 - val_loss: 139.9573 - val_accuracy: 0.8400\n","Epoch 18/50\n","7/7 [==============================] - 1s 96ms/step - loss: 6.9252 - accuracy: 0.9552 - val_loss: 140.8625 - val_accuracy: 0.8400\n","Epoch 19/50\n","7/7 [==============================] - 1s 94ms/step - loss: 6.6719 - accuracy: 0.9596 - val_loss: 141.4160 - val_accuracy: 0.8400\n","Epoch 20/50\n","7/7 [==============================] - 1s 94ms/step - loss: 6.1068 - accuracy: 0.9596 - val_loss: 140.5984 - val_accuracy: 0.8400\n","Epoch 21/50\n","7/7 [==============================] - 1s 92ms/step - loss: 6.3613 - accuracy: 0.9596 - val_loss: 140.0936 - val_accuracy: 0.8400\n","Epoch 22/50\n","7/7 [==============================] - 1s 94ms/step - loss: 10.9958 - accuracy: 0.9641 - val_loss: 143.5925 - val_accuracy: 0.8800\n","Epoch 23/50\n","7/7 [==============================] - 1s 95ms/step - loss: 8.5001 - accuracy: 0.9596 - val_loss: 142.3663 - val_accuracy: 0.8400\n","Epoch 24/50\n","7/7 [==============================] - 1s 93ms/step - loss: 6.8071 - accuracy: 0.9686 - val_loss: 140.1298 - val_accuracy: 0.8400\n","Epoch 25/50\n","7/7 [==============================] - 1s 129ms/step - loss: 7.0645 - accuracy: 0.9686 - val_loss: 139.3088 - val_accuracy: 0.8400\n","Epoch 26/50\n","7/7 [==============================] - 1s 137ms/step - loss: 8.2829 - accuracy: 0.9596 - val_loss: 141.8230 - val_accuracy: 0.8400\n","Epoch 27/50\n","7/7 [==============================] - 1s 133ms/step - loss: 9.6386 - accuracy: 0.9596 - val_loss: 137.8744 - val_accuracy: 0.8400\n","Epoch 28/50\n","7/7 [==============================] - 1s 174ms/step - loss: 5.9805 - accuracy: 0.9686 - val_loss: 140.5598 - val_accuracy: 0.8400\n","Epoch 29/50\n","7/7 [==============================] - 1s 151ms/step - loss: 8.3402 - accuracy: 0.9686 - val_loss: 139.7164 - val_accuracy: 0.8400\n","Epoch 30/50\n","7/7 [==============================] - 1s 92ms/step - loss: 6.5848 - accuracy: 0.9552 - val_loss: 139.2881 - val_accuracy: 0.8400\n","Epoch 31/50\n","7/7 [==============================] - 1s 89ms/step - loss: 5.9572 - accuracy: 0.9686 - val_loss: 139.1614 - val_accuracy: 0.8400\n","Epoch 32/50\n","7/7 [==============================] - 1s 107ms/step - loss: 6.5127 - accuracy: 0.9731 - val_loss: 136.6248 - val_accuracy: 0.8400\n","Epoch 33/50\n","7/7 [==============================] - 1s 86ms/step - loss: 5.3958 - accuracy: 0.9686 - val_loss: 139.3194 - val_accuracy: 0.8400\n","Epoch 34/50\n","7/7 [==============================] - 1s 90ms/step - loss: 4.8506 - accuracy: 0.9731 - val_loss: 138.6323 - val_accuracy: 0.8800\n","Epoch 35/50\n","7/7 [==============================] - 1s 134ms/step - loss: 4.2787 - accuracy: 0.9641 - val_loss: 139.1378 - val_accuracy: 0.8400\n","Epoch 36/50\n","7/7 [==============================] - 1s 94ms/step - loss: 4.1827 - accuracy: 0.9686 - val_loss: 139.3056 - val_accuracy: 0.8400\n","Epoch 37/50\n","7/7 [==============================] - 1s 91ms/step - loss: 3.6371 - accuracy: 0.9686 - val_loss: 133.6500 - val_accuracy: 0.8400\n","Epoch 38/50\n","7/7 [==============================] - 1s 93ms/step - loss: 2.9922 - accuracy: 0.9865 - val_loss: 136.7643 - val_accuracy: 0.8800\n","Epoch 39/50\n","7/7 [==============================] - 1s 91ms/step - loss: 2.4432 - accuracy: 0.9731 - val_loss: 138.2295 - val_accuracy: 0.8800\n","Epoch 40/50\n","7/7 [==============================] - 1s 94ms/step - loss: 3.1126 - accuracy: 0.9596 - val_loss: 136.9662 - val_accuracy: 0.8400\n","Epoch 41/50\n","7/7 [==============================] - 1s 97ms/step - loss: 2.6881 - accuracy: 0.9731 - val_loss: 138.9123 - val_accuracy: 0.8400\n","Epoch 42/50\n","7/7 [==============================] - 1s 92ms/step - loss: 2.6134 - accuracy: 0.9776 - val_loss: 137.8736 - val_accuracy: 0.8800\n","Epoch 43/50\n","7/7 [==============================] - 1s 96ms/step - loss: 3.7103 - accuracy: 0.9776 - val_loss: 138.1724 - val_accuracy: 0.8400\n","Epoch 44/50\n","7/7 [==============================] - 1s 127ms/step - loss: 2.8198 - accuracy: 0.9776 - val_loss: 137.7699 - val_accuracy: 0.8800\n","Epoch 45/50\n","7/7 [==============================] - 1s 133ms/step - loss: 8.1106 - accuracy: 0.9910 - val_loss: 138.2611 - val_accuracy: 0.8400\n","Epoch 46/50\n","7/7 [==============================] - 1s 126ms/step - loss: 4.4273 - accuracy: 0.9731 - val_loss: 139.8443 - val_accuracy: 0.8400\n","Epoch 47/50\n","7/7 [==============================] - 1s 141ms/step - loss: 3.8266 - accuracy: 0.9776 - val_loss: 137.7258 - val_accuracy: 0.8400\n","Epoch 48/50\n","7/7 [==============================] - 1s 136ms/step - loss: 5.0021 - accuracy: 0.9731 - val_loss: 138.7985 - val_accuracy: 0.8400\n","Epoch 49/50\n","7/7 [==============================] - 1s 125ms/step - loss: 3.2546 - accuracy: 0.9776 - val_loss: 139.1333 - val_accuracy: 0.8400\n","Epoch 50/50\n","7/7 [==============================] - 1s 92ms/step - loss: 3.1920 - accuracy: 0.9731 - val_loss: 138.7781 - val_accuracy: 0.8800\n","2/2 [==============================] - 0s 17ms/step\n","2/2 [==============================] - 0s 23ms/step - loss: 32.5090 - accuracy: 0.9844\n","Test Loss: 32.5090, Test accuracy : 0.9844\n","Epoch 1/50\n","6/6 [==============================] - 1s 118ms/step - loss: 519.8063 - accuracy: 0.3373 - val_loss: 266.2949 - val_accuracy: 0.2632\n","Epoch 2/50\n","6/6 [==============================] - 1s 92ms/step - loss: 167.6828 - accuracy: 0.1361 - val_loss: 186.5552 - val_accuracy: 0.1579\n","Epoch 3/50\n","6/6 [==============================] - 1s 92ms/step - loss: 153.3432 - accuracy: 0.0888 - val_loss: 157.0216 - val_accuracy: 0.2105\n","Epoch 4/50\n","6/6 [==============================] - 1s 87ms/step - loss: 119.7745 - accuracy: 0.4438 - val_loss: 164.4528 - val_accuracy: 0.0526\n","Epoch 5/50\n","6/6 [==============================] - 1s 88ms/step - loss: 109.5341 - accuracy: 0.4675 - val_loss: 160.4250 - val_accuracy: 0.4211\n","Epoch 6/50\n","6/6 [==============================] - 1s 87ms/step - loss: 94.2497 - accuracy: 0.6391 - val_loss: 156.4205 - val_accuracy: 0.7368\n","Epoch 7/50\n","6/6 [==============================] - 1s 90ms/step - loss: 84.3367 - accuracy: 0.5858 - val_loss: 155.8231 - val_accuracy: 0.7368\n","Epoch 8/50\n","6/6 [==============================] - 1s 90ms/step - loss: 75.8766 - accuracy: 0.7160 - val_loss: 162.2461 - val_accuracy: 0.7368\n","Epoch 9/50\n","6/6 [==============================] - 1s 93ms/step - loss: 69.2964 - accuracy: 0.6805 - val_loss: 167.2924 - val_accuracy: 0.5789\n","Epoch 10/50\n","6/6 [==============================] - 1s 89ms/step - loss: 63.7625 - accuracy: 0.6568 - val_loss: 166.9662 - val_accuracy: 0.7368\n","Epoch 11/50\n","6/6 [==============================] - 1s 91ms/step - loss: 58.8287 - accuracy: 0.6450 - val_loss: 162.8906 - val_accuracy: 0.7368\n","Epoch 12/50\n","6/6 [==============================] - 1s 119ms/step - loss: 55.3954 - accuracy: 0.5562 - val_loss: 167.8116 - val_accuracy: 0.6842\n","Epoch 13/50\n","6/6 [==============================] - 1s 127ms/step - loss: 52.3722 - accuracy: 0.5325 - val_loss: 171.6054 - val_accuracy: 0.7368\n","Epoch 14/50\n","6/6 [==============================] - 1s 129ms/step - loss: 48.6387 - accuracy: 0.5266 - val_loss: 165.4345 - val_accuracy: 0.7368\n","Epoch 15/50\n","6/6 [==============================] - 1s 132ms/step - loss: 45.7130 - accuracy: 0.6272 - val_loss: 161.5804 - val_accuracy: 0.6316\n","Epoch 16/50\n","6/6 [==============================] - 1s 132ms/step - loss: 44.6990 - accuracy: 0.6923 - val_loss: 159.0920 - val_accuracy: 0.6316\n","Epoch 17/50\n","6/6 [==============================] - 1s 129ms/step - loss: 40.0347 - accuracy: 0.7041 - val_loss: 162.0364 - val_accuracy: 0.6316\n","Epoch 18/50\n","6/6 [==============================] - 1s 125ms/step - loss: 39.8273 - accuracy: 0.7278 - val_loss: 158.9576 - val_accuracy: 0.6316\n","Epoch 19/50\n","6/6 [==============================] - 1s 90ms/step - loss: 38.1973 - accuracy: 0.6627 - val_loss: 157.0522 - val_accuracy: 0.6316\n","Epoch 20/50\n","6/6 [==============================] - 1s 90ms/step - loss: 34.5715 - accuracy: 0.6805 - val_loss: 154.3075 - val_accuracy: 0.6316\n","Epoch 21/50\n","6/6 [==============================] - 1s 89ms/step - loss: 38.0334 - accuracy: 0.6095 - val_loss: 152.3469 - val_accuracy: 0.6316\n","Epoch 22/50\n","6/6 [==============================] - 1s 87ms/step - loss: 32.9775 - accuracy: 0.7219 - val_loss: 151.8057 - val_accuracy: 0.6316\n","Epoch 23/50\n","6/6 [==============================] - 1s 92ms/step - loss: 36.2587 - accuracy: 0.7041 - val_loss: 154.6852 - val_accuracy: 0.6316\n","Epoch 24/50\n","6/6 [==============================] - 1s 89ms/step - loss: 31.1437 - accuracy: 0.7456 - val_loss: 153.7524 - val_accuracy: 0.6316\n","Epoch 25/50\n","6/6 [==============================] - 1s 91ms/step - loss: 30.8841 - accuracy: 0.7041 - val_loss: 153.0615 - val_accuracy: 0.6316\n","Epoch 26/50\n","6/6 [==============================] - 1s 88ms/step - loss: 30.1059 - accuracy: 0.7633 - val_loss: 151.1921 - val_accuracy: 0.6316\n","Epoch 27/50\n","6/6 [==============================] - 1s 93ms/step - loss: 30.1016 - accuracy: 0.7278 - val_loss: 152.6849 - val_accuracy: 0.6316\n","Epoch 28/50\n","6/6 [==============================] - 1s 92ms/step - loss: 29.6337 - accuracy: 0.7870 - val_loss: 154.2115 - val_accuracy: 0.6316\n","Epoch 29/50\n","6/6 [==============================] - 1s 92ms/step - loss: 28.5570 - accuracy: 0.7219 - val_loss: 151.3227 - val_accuracy: 0.6316\n","Epoch 30/50\n","6/6 [==============================] - 1s 92ms/step - loss: 28.5766 - accuracy: 0.7219 - val_loss: 150.2586 - val_accuracy: 0.6316\n","Epoch 31/50\n","6/6 [==============================] - 1s 91ms/step - loss: 28.0801 - accuracy: 0.6923 - val_loss: 154.7112 - val_accuracy: 0.6316\n","Epoch 32/50\n","6/6 [==============================] - 1s 90ms/step - loss: 28.3053 - accuracy: 0.7041 - val_loss: 148.3569 - val_accuracy: 0.6316\n","Epoch 33/50\n","6/6 [==============================] - 1s 92ms/step - loss: 33.8199 - accuracy: 0.6450 - val_loss: 156.9107 - val_accuracy: 0.6316\n","Epoch 34/50\n","6/6 [==============================] - 1s 88ms/step - loss: 52.8561 - accuracy: 0.6805 - val_loss: 153.9840 - val_accuracy: 0.5789\n","Epoch 35/50\n","6/6 [==============================] - 1s 89ms/step - loss: 35.8099 - accuracy: 0.6391 - val_loss: 151.5856 - val_accuracy: 0.5789\n","Epoch 36/50\n","6/6 [==============================] - 1s 90ms/step - loss: 32.7974 - accuracy: 0.5799 - val_loss: 152.8146 - val_accuracy: 0.5263\n","Epoch 37/50\n","6/6 [==============================] - 1s 117ms/step - loss: 33.8490 - accuracy: 0.5444 - val_loss: 149.7773 - val_accuracy: 0.5789\n","Epoch 38/50\n","6/6 [==============================] - 1s 129ms/step - loss: 29.4566 - accuracy: 0.5680 - val_loss: 154.0605 - val_accuracy: 0.5263\n","Epoch 39/50\n","6/6 [==============================] - 1s 125ms/step - loss: 28.9652 - accuracy: 0.6686 - val_loss: 159.8971 - val_accuracy: 0.5263\n","Epoch 40/50\n","6/6 [==============================] - 1s 127ms/step - loss: 30.6768 - accuracy: 0.5207 - val_loss: 156.6501 - val_accuracy: 0.5263\n","Epoch 41/50\n","6/6 [==============================] - 1s 130ms/step - loss: 25.4343 - accuracy: 0.5385 - val_loss: 161.3817 - val_accuracy: 0.5789\n","Epoch 42/50\n","6/6 [==============================] - 1s 128ms/step - loss: 24.0579 - accuracy: 0.4615 - val_loss: 158.5523 - val_accuracy: 0.5789\n","Epoch 43/50\n","6/6 [==============================] - 1s 117ms/step - loss: 21.9081 - accuracy: 0.6095 - val_loss: 153.9847 - val_accuracy: 0.4211\n","Epoch 44/50\n","6/6 [==============================] - 1s 94ms/step - loss: 23.3839 - accuracy: 0.5266 - val_loss: 154.0499 - val_accuracy: 0.6316\n","Epoch 45/50\n","6/6 [==============================] - 1s 90ms/step - loss: 19.0471 - accuracy: 0.5503 - val_loss: 152.7763 - val_accuracy: 0.4211\n","Epoch 46/50\n","6/6 [==============================] - 1s 93ms/step - loss: 17.0085 - accuracy: 0.4970 - val_loss: 152.6409 - val_accuracy: 0.3684\n","Epoch 47/50\n","6/6 [==============================] - 1s 91ms/step - loss: 16.3284 - accuracy: 0.5976 - val_loss: 153.6125 - val_accuracy: 0.4211\n","Epoch 48/50\n","6/6 [==============================] - 1s 94ms/step - loss: 15.4115 - accuracy: 0.5148 - val_loss: 156.3714 - val_accuracy: 0.4737\n","Epoch 49/50\n","6/6 [==============================] - 1s 91ms/step - loss: 14.0039 - accuracy: 0.5621 - val_loss: 150.6895 - val_accuracy: 0.5789\n","Epoch 50/50\n","6/6 [==============================] - 1s 88ms/step - loss: 13.0231 - accuracy: 0.5325 - val_loss: 155.8862 - val_accuracy: 0.4737\n","2/2 [==============================] - 0s 14ms/step\n","2/2 [==============================] - 0s 16ms/step - loss: 454.8297 - accuracy: 0.8542\n","Test Loss: 454.8297, Test accuracy : 0.8542\n","Epoch 1/50\n","9/9 [==============================] - 2s 162ms/step - loss: 630.3657 - accuracy: 0.3275 - val_loss: 381.4215 - val_accuracy: 0.2812\n","Epoch 2/50\n","9/9 [==============================] - 1s 128ms/step - loss: 311.5056 - accuracy: 0.3099 - val_loss: 307.6118 - val_accuracy: 0.8125\n","Epoch 3/50\n","9/9 [==============================] - 1s 95ms/step - loss: 194.7659 - accuracy: 0.6197 - val_loss: 298.1313 - val_accuracy: 0.7812\n","Epoch 4/50\n","9/9 [==============================] - 1s 95ms/step - loss: 168.2023 - accuracy: 0.6232 - val_loss: 301.0621 - val_accuracy: 0.7812\n","Epoch 5/50\n","9/9 [==============================] - 1s 99ms/step - loss: 154.3346 - accuracy: 0.5880 - val_loss: 301.1052 - val_accuracy: 0.8125\n","Epoch 6/50\n","9/9 [==============================] - 1s 92ms/step - loss: 143.6360 - accuracy: 0.6444 - val_loss: 305.8221 - val_accuracy: 0.8125\n","Epoch 7/50\n","9/9 [==============================] - 1s 95ms/step - loss: 133.7068 - accuracy: 0.6092 - val_loss: 302.4225 - val_accuracy: 0.8125\n","Epoch 8/50\n","9/9 [==============================] - 1s 94ms/step - loss: 128.9017 - accuracy: 0.6232 - val_loss: 299.9449 - val_accuracy: 0.8125\n","Epoch 9/50\n","9/9 [==============================] - 1s 94ms/step - loss: 124.7567 - accuracy: 0.6056 - val_loss: 299.6496 - val_accuracy: 0.8125\n","Epoch 10/50\n","9/9 [==============================] - 1s 98ms/step - loss: 120.0419 - accuracy: 0.6796 - val_loss: 299.5561 - val_accuracy: 0.8125\n","Epoch 11/50\n","9/9 [==============================] - 1s 93ms/step - loss: 116.1790 - accuracy: 0.7254 - val_loss: 298.3862 - val_accuracy: 0.8125\n","Epoch 12/50\n","9/9 [==============================] - 1s 94ms/step - loss: 111.1451 - accuracy: 0.7113 - val_loss: 298.8986 - val_accuracy: 0.8125\n","Epoch 13/50\n","9/9 [==============================] - 1s 94ms/step - loss: 105.7015 - accuracy: 0.7042 - val_loss: 299.1468 - val_accuracy: 0.8125\n","Epoch 14/50\n","9/9 [==============================] - 1s 114ms/step - loss: 96.7508 - accuracy: 0.8345 - val_loss: 298.0049 - val_accuracy: 0.8125\n","Epoch 15/50\n","9/9 [==============================] - 1s 137ms/step - loss: 88.1354 - accuracy: 0.8627 - val_loss: 301.6285 - val_accuracy: 0.8125\n","Epoch 16/50\n","9/9 [==============================] - 1s 131ms/step - loss: 81.7502 - accuracy: 0.8662 - val_loss: 302.8129 - val_accuracy: 0.8125\n","Epoch 17/50\n","9/9 [==============================] - 1s 135ms/step - loss: 75.8083 - accuracy: 0.8662 - val_loss: 295.1511 - val_accuracy: 0.8125\n","Epoch 18/50\n","9/9 [==============================] - 1s 143ms/step - loss: 71.4294 - accuracy: 0.8592 - val_loss: 297.1317 - val_accuracy: 0.8125\n","Epoch 19/50\n","9/9 [==============================] - 1s 124ms/step - loss: 66.9218 - accuracy: 0.8732 - val_loss: 293.3404 - val_accuracy: 0.8125\n","Epoch 20/50\n","9/9 [==============================] - 1s 92ms/step - loss: 64.1179 - accuracy: 0.8697 - val_loss: 289.4982 - val_accuracy: 0.8125\n","Epoch 21/50\n","9/9 [==============================] - 1s 98ms/step - loss: 62.0707 - accuracy: 0.8697 - val_loss: 287.2454 - val_accuracy: 0.8125\n","Epoch 22/50\n","9/9 [==============================] - 1s 95ms/step - loss: 57.6748 - accuracy: 0.8662 - val_loss: 281.5184 - val_accuracy: 0.8125\n","Epoch 23/50\n","9/9 [==============================] - 1s 96ms/step - loss: 59.1256 - accuracy: 0.8662 - val_loss: 281.8755 - val_accuracy: 0.8125\n","Epoch 24/50\n","9/9 [==============================] - 1s 91ms/step - loss: 64.6851 - accuracy: 0.8627 - val_loss: 280.5961 - val_accuracy: 0.8125\n","Epoch 25/50\n","9/9 [==============================] - 1s 97ms/step - loss: 63.9634 - accuracy: 0.8697 - val_loss: 279.0637 - val_accuracy: 0.8125\n","Epoch 26/50\n","9/9 [==============================] - 1s 91ms/step - loss: 53.0241 - accuracy: 0.8627 - val_loss: 278.2000 - val_accuracy: 0.8125\n","Epoch 27/50\n","9/9 [==============================] - 1s 95ms/step - loss: 47.4712 - accuracy: 0.8697 - val_loss: 275.3658 - val_accuracy: 0.8125\n","Epoch 28/50\n","9/9 [==============================] - 1s 94ms/step - loss: 46.3473 - accuracy: 0.8662 - val_loss: 273.6561 - val_accuracy: 0.8125\n","Epoch 29/50\n","9/9 [==============================] - 1s 112ms/step - loss: 43.1632 - accuracy: 0.8697 - val_loss: 274.6577 - val_accuracy: 0.7812\n","Epoch 30/50\n","9/9 [==============================] - 1s 136ms/step - loss: 41.6191 - accuracy: 0.8697 - val_loss: 273.5462 - val_accuracy: 0.8125\n","Epoch 31/50\n","9/9 [==============================] - 1s 124ms/step - loss: 38.3924 - accuracy: 0.8768 - val_loss: 275.9843 - val_accuracy: 0.8125\n","Epoch 32/50\n","9/9 [==============================] - 1s 139ms/step - loss: 36.1741 - accuracy: 0.8908 - val_loss: 271.6200 - val_accuracy: 0.7812\n","Epoch 33/50\n","9/9 [==============================] - 1s 135ms/step - loss: 34.2561 - accuracy: 0.8944 - val_loss: 268.5723 - val_accuracy: 0.8125\n","Epoch 34/50\n","9/9 [==============================] - 1s 96ms/step - loss: 33.0088 - accuracy: 0.8944 - val_loss: 266.4316 - val_accuracy: 0.8125\n","Epoch 35/50\n","9/9 [==============================] - 1s 93ms/step - loss: 31.7247 - accuracy: 0.8979 - val_loss: 266.4586 - val_accuracy: 0.8125\n","Epoch 36/50\n","9/9 [==============================] - 1s 96ms/step - loss: 30.6654 - accuracy: 0.9014 - val_loss: 265.6719 - val_accuracy: 0.7812\n","Epoch 37/50\n","9/9 [==============================] - 1s 93ms/step - loss: 31.0203 - accuracy: 0.9120 - val_loss: 264.1041 - val_accuracy: 0.8125\n","Epoch 38/50\n","9/9 [==============================] - 1s 96ms/step - loss: 30.5454 - accuracy: 0.9085 - val_loss: 264.6348 - val_accuracy: 0.8125\n","Epoch 39/50\n","9/9 [==============================] - 1s 95ms/step - loss: 32.3384 - accuracy: 0.9049 - val_loss: 264.3828 - val_accuracy: 0.7812\n","Epoch 40/50\n","9/9 [==============================] - 1s 95ms/step - loss: 32.1284 - accuracy: 0.9190 - val_loss: 266.0181 - val_accuracy: 0.8125\n","Epoch 41/50\n","9/9 [==============================] - 1s 94ms/step - loss: 29.8799 - accuracy: 0.9261 - val_loss: 264.0458 - val_accuracy: 0.8125\n","Epoch 42/50\n","9/9 [==============================] - 1s 95ms/step - loss: 25.5252 - accuracy: 0.9190 - val_loss: 268.2817 - val_accuracy: 0.7812\n","Epoch 43/50\n","9/9 [==============================] - 1s 94ms/step - loss: 25.2063 - accuracy: 0.9401 - val_loss: 265.2775 - val_accuracy: 0.8125\n","Epoch 44/50\n","9/9 [==============================] - 1s 92ms/step - loss: 24.1049 - accuracy: 0.9296 - val_loss: 267.2690 - val_accuracy: 0.8125\n","Epoch 45/50\n","9/9 [==============================] - 1s 106ms/step - loss: 23.5155 - accuracy: 0.9401 - val_loss: 267.4430 - val_accuracy: 0.8125\n","Epoch 46/50\n","9/9 [==============================] - 1s 136ms/step - loss: 22.3126 - accuracy: 0.9401 - val_loss: 267.3492 - val_accuracy: 0.8125\n","Epoch 47/50\n","9/9 [==============================] - 1s 134ms/step - loss: 21.5015 - accuracy: 0.9401 - val_loss: 270.1568 - val_accuracy: 0.8125\n","Epoch 48/50\n","9/9 [==============================] - 1s 142ms/step - loss: 22.1168 - accuracy: 0.9437 - val_loss: 266.2488 - val_accuracy: 0.8125\n","Epoch 49/50\n","9/9 [==============================] - 1s 112ms/step - loss: 21.8271 - accuracy: 0.9472 - val_loss: 270.7528 - val_accuracy: 0.8125\n","Epoch 50/50\n","9/9 [==============================] - 1s 92ms/step - loss: 25.3355 - accuracy: 0.9437 - val_loss: 267.5317 - val_accuracy: 0.8125\n","3/3 [==============================] - 0s 21ms/step\n","3/3 [==============================] - 0s 19ms/step - loss: 87.7179 - accuracy: 1.0000\n","Test Loss: 87.7179, Test accuracy : 1.0000\n","Epoch 1/50\n","14/14 [==============================] - 2s 104ms/step - loss: 189.2144 - accuracy: 0.1355 - val_loss: 115.8613 - val_accuracy: 0.2500\n","Epoch 2/50\n","14/14 [==============================] - 1s 91ms/step - loss: 141.6397 - accuracy: 0.2033 - val_loss: 95.6911 - val_accuracy: 0.2083\n","Epoch 3/50\n","14/14 [==============================] - 1s 93ms/step - loss: 112.7328 - accuracy: 0.2290 - val_loss: 93.5646 - val_accuracy: 0.2500\n","Epoch 4/50\n","14/14 [==============================] - 1s 99ms/step - loss: 94.9763 - accuracy: 0.2780 - val_loss: 88.4447 - val_accuracy: 0.2708\n","Epoch 5/50\n","14/14 [==============================] - 1s 90ms/step - loss: 82.5079 - accuracy: 0.2897 - val_loss: 76.8123 - val_accuracy: 0.3125\n","Epoch 6/50\n","14/14 [==============================] - 1s 92ms/step - loss: 69.6593 - accuracy: 0.3318 - val_loss: 77.5621 - val_accuracy: 0.3958\n","Epoch 7/50\n","14/14 [==============================] - 1s 98ms/step - loss: 60.9349 - accuracy: 0.3341 - val_loss: 81.4804 - val_accuracy: 0.3125\n","Epoch 8/50\n","14/14 [==============================] - 2s 130ms/step - loss: 54.6960 - accuracy: 0.3388 - val_loss: 75.8157 - val_accuracy: 0.3958\n","Epoch 9/50\n","14/14 [==============================] - 2s 126ms/step - loss: 48.9719 - accuracy: 0.3458 - val_loss: 76.9018 - val_accuracy: 0.4167\n","Epoch 10/50\n","14/14 [==============================] - 2s 115ms/step - loss: 44.6928 - accuracy: 0.3668 - val_loss: 74.6859 - val_accuracy: 0.4375\n","Epoch 11/50\n","14/14 [==============================] - 1s 92ms/step - loss: 41.1691 - accuracy: 0.3668 - val_loss: 76.2355 - val_accuracy: 0.4167\n","Epoch 12/50\n","14/14 [==============================] - 1s 92ms/step - loss: 37.2078 - accuracy: 0.3598 - val_loss: 77.4977 - val_accuracy: 0.4375\n","Epoch 13/50\n","14/14 [==============================] - 1s 88ms/step - loss: 34.4668 - accuracy: 0.3715 - val_loss: 75.0410 - val_accuracy: 0.4167\n","Epoch 14/50\n","14/14 [==============================] - 1s 91ms/step - loss: 32.2048 - accuracy: 0.3715 - val_loss: 77.2783 - val_accuracy: 0.4167\n","Epoch 15/50\n","14/14 [==============================] - 1s 91ms/step - loss: 30.8573 - accuracy: 0.3715 - val_loss: 73.0671 - val_accuracy: 0.4167\n","Epoch 16/50\n","14/14 [==============================] - 1s 91ms/step - loss: 29.8326 - accuracy: 0.3738 - val_loss: 80.9016 - val_accuracy: 0.3542\n","Epoch 17/50\n","14/14 [==============================] - 1s 89ms/step - loss: 30.2366 - accuracy: 0.3411 - val_loss: 72.6372 - val_accuracy: 0.4167\n","Epoch 18/50\n","14/14 [==============================] - 1s 106ms/step - loss: 30.5302 - accuracy: 0.3738 - val_loss: 79.5679 - val_accuracy: 0.3125\n","Epoch 19/50\n","14/14 [==============================] - 2s 127ms/step - loss: 37.6425 - accuracy: 0.3855 - val_loss: 73.1110 - val_accuracy: 0.3750\n","Epoch 20/50\n","14/14 [==============================] - 2s 127ms/step - loss: 30.7041 - accuracy: 0.4089 - val_loss: 78.8990 - val_accuracy: 0.3125\n","Epoch 21/50\n","14/14 [==============================] - 2s 108ms/step - loss: 27.6641 - accuracy: 0.4089 - val_loss: 74.8450 - val_accuracy: 0.3542\n","Epoch 22/50\n","14/14 [==============================] - 1s 90ms/step - loss: 24.9544 - accuracy: 0.3879 - val_loss: 77.7055 - val_accuracy: 0.2917\n","Epoch 23/50\n","14/14 [==============================] - 1s 94ms/step - loss: 24.7041 - accuracy: 0.4393 - val_loss: 72.4193 - val_accuracy: 0.3750\n","Epoch 24/50\n","14/14 [==============================] - 1s 89ms/step - loss: 25.3234 - accuracy: 0.4463 - val_loss: 70.8811 - val_accuracy: 0.3958\n","Epoch 25/50\n","14/14 [==============================] - 1s 87ms/step - loss: 24.6107 - accuracy: 0.4276 - val_loss: 77.9946 - val_accuracy: 0.3333\n","Epoch 26/50\n","14/14 [==============================] - 1s 93ms/step - loss: 22.0000 - accuracy: 0.4206 - val_loss: 75.6563 - val_accuracy: 0.2292\n","Epoch 27/50\n","14/14 [==============================] - 1s 93ms/step - loss: 19.9658 - accuracy: 0.4322 - val_loss: 75.9568 - val_accuracy: 0.2708\n","Epoch 28/50\n","14/14 [==============================] - 1s 89ms/step - loss: 20.0585 - accuracy: 0.3832 - val_loss: 75.0940 - val_accuracy: 0.2708\n","Epoch 29/50\n","14/14 [==============================] - 2s 112ms/step - loss: 20.5668 - accuracy: 0.3575 - val_loss: 75.3655 - val_accuracy: 0.2917\n","Epoch 30/50\n","14/14 [==============================] - 2s 126ms/step - loss: 20.7247 - accuracy: 0.3575 - val_loss: 75.9993 - val_accuracy: 0.2292\n","Epoch 31/50\n","14/14 [==============================] - 2s 136ms/step - loss: 18.1291 - accuracy: 0.3481 - val_loss: 74.1455 - val_accuracy: 0.2500\n","Epoch 32/50\n","14/14 [==============================] - 2s 136ms/step - loss: 17.2010 - accuracy: 0.3621 - val_loss: 73.5912 - val_accuracy: 0.2708\n","Epoch 33/50\n","14/14 [==============================] - 2s 132ms/step - loss: 16.6539 - accuracy: 0.3388 - val_loss: 72.3524 - val_accuracy: 0.2917\n","Epoch 34/50\n","14/14 [==============================] - 2s 130ms/step - loss: 15.7929 - accuracy: 0.3598 - val_loss: 74.2475 - val_accuracy: 0.2500\n","Epoch 35/50\n","14/14 [==============================] - 1s 93ms/step - loss: 15.2711 - accuracy: 0.3411 - val_loss: 74.7696 - val_accuracy: 0.2708\n","Epoch 36/50\n","14/14 [==============================] - 1s 95ms/step - loss: 15.0830 - accuracy: 0.3364 - val_loss: 76.1108 - val_accuracy: 0.2708\n","Epoch 37/50\n","14/14 [==============================] - 1s 92ms/step - loss: 15.0322 - accuracy: 0.3435 - val_loss: 73.8173 - val_accuracy: 0.2917\n","Epoch 38/50\n","14/14 [==============================] - 1s 93ms/step - loss: 15.4960 - accuracy: 0.3411 - val_loss: 77.5261 - val_accuracy: 0.2500\n","Epoch 39/50\n","14/14 [==============================] - 2s 125ms/step - loss: 15.8040 - accuracy: 0.3505 - val_loss: 72.2334 - val_accuracy: 0.2708\n","Epoch 40/50\n","14/14 [==============================] - 2s 125ms/step - loss: 17.7017 - accuracy: 0.3505 - val_loss: 75.0662 - val_accuracy: 0.2917\n","Epoch 41/50\n","14/14 [==============================] - 2s 134ms/step - loss: 19.2501 - accuracy: 0.3528 - val_loss: 75.1535 - val_accuracy: 0.2292\n","Epoch 42/50\n","14/14 [==============================] - 1s 96ms/step - loss: 16.5347 - accuracy: 0.3575 - val_loss: 73.0389 - val_accuracy: 0.2500\n","Epoch 43/50\n","14/14 [==============================] - 1s 92ms/step - loss: 16.0591 - accuracy: 0.3575 - val_loss: 74.1492 - val_accuracy: 0.2708\n","Epoch 44/50\n","14/14 [==============================] - 1s 89ms/step - loss: 16.1280 - accuracy: 0.3645 - val_loss: 73.5212 - val_accuracy: 0.2500\n","Epoch 45/50\n","14/14 [==============================] - 1s 93ms/step - loss: 15.0896 - accuracy: 0.3575 - val_loss: 74.1905 - val_accuracy: 0.2500\n","Epoch 46/50\n","14/14 [==============================] - 1s 92ms/step - loss: 14.5517 - accuracy: 0.3435 - val_loss: 74.0075 - val_accuracy: 0.2708\n","Epoch 47/50\n","14/14 [==============================] - 1s 97ms/step - loss: 14.6085 - accuracy: 0.3528 - val_loss: 75.0380 - val_accuracy: 0.2500\n","Epoch 48/50\n","14/14 [==============================] - 1s 89ms/step - loss: 14.7293 - accuracy: 0.3598 - val_loss: 73.9187 - val_accuracy: 0.2500\n","Epoch 49/50\n","14/14 [==============================] - 1s 95ms/step - loss: 14.4971 - accuracy: 0.3575 - val_loss: 75.4068 - val_accuracy: 0.2708\n","Epoch 50/50\n","14/14 [==============================] - 2s 132ms/step - loss: 14.2531 - accuracy: 0.3458 - val_loss: 73.2656 - val_accuracy: 0.2500\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b2f459b4310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 0s 14ms/step\n","4/4 [==============================] - 0s 15ms/step - loss: 1.4711 - accuracy: 0.8583\n","Test Loss: 1.4711, Test accuracy : 0.8583\n","Epoch 1/50\n","13/13 [==============================] - 2s 113ms/step - loss: 246.0573 - accuracy: 0.3244 - val_loss: 255.3740 - val_accuracy: 0.3696\n","Epoch 2/50\n","13/13 [==============================] - 2s 131ms/step - loss: 167.5283 - accuracy: 0.2049 - val_loss: 232.5591 - val_accuracy: 0.4130\n","Epoch 3/50\n","13/13 [==============================] - 2s 125ms/step - loss: 136.7315 - accuracy: 0.3195 - val_loss: 223.1269 - val_accuracy: 0.3913\n","Epoch 4/50\n","13/13 [==============================] - 1s 109ms/step - loss: 123.9555 - accuracy: 0.2976 - val_loss: 241.8249 - val_accuracy: 0.5000\n","Epoch 5/50\n","13/13 [==============================] - 1s 89ms/step - loss: 113.1095 - accuracy: 0.3341 - val_loss: 241.2582 - val_accuracy: 0.4130\n","Epoch 6/50\n","13/13 [==============================] - 1s 89ms/step - loss: 93.6194 - accuracy: 0.3732 - val_loss: 233.4183 - val_accuracy: 0.4783\n","Epoch 7/50\n","13/13 [==============================] - 1s 88ms/step - loss: 80.4121 - accuracy: 0.4220 - val_loss: 236.8863 - val_accuracy: 0.4130\n","Epoch 8/50\n","13/13 [==============================] - 1s 86ms/step - loss: 70.4627 - accuracy: 0.5073 - val_loss: 242.7815 - val_accuracy: 0.5000\n","Epoch 9/50\n","13/13 [==============================] - 1s 90ms/step - loss: 63.1636 - accuracy: 0.5610 - val_loss: 235.4756 - val_accuracy: 0.4565\n","Epoch 10/50\n","13/13 [==============================] - 1s 88ms/step - loss: 57.3757 - accuracy: 0.5951 - val_loss: 240.4169 - val_accuracy: 0.5000\n","Epoch 11/50\n","13/13 [==============================] - 1s 89ms/step - loss: 53.0409 - accuracy: 0.5780 - val_loss: 230.0131 - val_accuracy: 0.5000\n","Epoch 12/50\n","13/13 [==============================] - 1s 88ms/step - loss: 47.6750 - accuracy: 0.6293 - val_loss: 236.7972 - val_accuracy: 0.4783\n","Epoch 13/50\n","13/13 [==============================] - 1s 113ms/step - loss: 47.7014 - accuracy: 0.6341 - val_loss: 235.1379 - val_accuracy: 0.5000\n","Epoch 14/50\n","13/13 [==============================] - 2s 126ms/step - loss: 55.8804 - accuracy: 0.6146 - val_loss: 230.8893 - val_accuracy: 0.5217\n","Epoch 15/50\n","13/13 [==============================] - 2s 126ms/step - loss: 48.4981 - accuracy: 0.6341 - val_loss: 238.7369 - val_accuracy: 0.5217\n","Epoch 16/50\n","13/13 [==============================] - 1s 97ms/step - loss: 43.7357 - accuracy: 0.6341 - val_loss: 231.6509 - val_accuracy: 0.5217\n","Epoch 17/50\n","13/13 [==============================] - 1s 92ms/step - loss: 40.0389 - accuracy: 0.6878 - val_loss: 241.3220 - val_accuracy: 0.5000\n","Epoch 18/50\n","13/13 [==============================] - 1s 91ms/step - loss: 37.7386 - accuracy: 0.6805 - val_loss: 223.8566 - val_accuracy: 0.4783\n","Epoch 19/50\n","13/13 [==============================] - 1s 86ms/step - loss: 34.9608 - accuracy: 0.7024 - val_loss: 232.8881 - val_accuracy: 0.4783\n","Epoch 20/50\n","13/13 [==============================] - 1s 87ms/step - loss: 33.2104 - accuracy: 0.7024 - val_loss: 229.2588 - val_accuracy: 0.5000\n","Epoch 21/50\n","13/13 [==============================] - 1s 87ms/step - loss: 33.4049 - accuracy: 0.7268 - val_loss: 238.7098 - val_accuracy: 0.4348\n","Epoch 22/50\n","13/13 [==============================] - 1s 84ms/step - loss: 32.8164 - accuracy: 0.7049 - val_loss: 232.1639 - val_accuracy: 0.5000\n","Epoch 23/50\n","13/13 [==============================] - 1s 89ms/step - loss: 32.8199 - accuracy: 0.7024 - val_loss: 222.7838 - val_accuracy: 0.5000\n","Epoch 24/50\n","13/13 [==============================] - 1s 84ms/step - loss: 32.6433 - accuracy: 0.7122 - val_loss: 237.8473 - val_accuracy: 0.4565\n","Epoch 25/50\n","13/13 [==============================] - 1s 116ms/step - loss: 30.2053 - accuracy: 0.6659 - val_loss: 232.8871 - val_accuracy: 0.4565\n","Epoch 26/50\n","13/13 [==============================] - 2s 123ms/step - loss: 27.1594 - accuracy: 0.7390 - val_loss: 228.4778 - val_accuracy: 0.4783\n","Epoch 27/50\n","13/13 [==============================] - 2s 121ms/step - loss: 25.9839 - accuracy: 0.7000 - val_loss: 227.5801 - val_accuracy: 0.4783\n","Epoch 28/50\n","13/13 [==============================] - 1s 91ms/step - loss: 24.8432 - accuracy: 0.7317 - val_loss: 226.7804 - val_accuracy: 0.4783\n","Epoch 29/50\n","13/13 [==============================] - 1s 90ms/step - loss: 24.1262 - accuracy: 0.6976 - val_loss: 229.6652 - val_accuracy: 0.5000\n","Epoch 30/50\n","13/13 [==============================] - 1s 86ms/step - loss: 24.0688 - accuracy: 0.7146 - val_loss: 231.8266 - val_accuracy: 0.4783\n","Epoch 31/50\n","13/13 [==============================] - 1s 87ms/step - loss: 23.4893 - accuracy: 0.7122 - val_loss: 231.3797 - val_accuracy: 0.4783\n","Epoch 32/50\n","13/13 [==============================] - 1s 84ms/step - loss: 22.9895 - accuracy: 0.7341 - val_loss: 230.2481 - val_accuracy: 0.4565\n","Epoch 33/50\n","13/13 [==============================] - 1s 86ms/step - loss: 24.1202 - accuracy: 0.7146 - val_loss: 227.6781 - val_accuracy: 0.4565\n","Epoch 34/50\n","13/13 [==============================] - 1s 85ms/step - loss: 23.8155 - accuracy: 0.7390 - val_loss: 232.8420 - val_accuracy: 0.5000\n","Epoch 35/50\n","13/13 [==============================] - 1s 88ms/step - loss: 25.2625 - accuracy: 0.7049 - val_loss: 237.9600 - val_accuracy: 0.5000\n","Epoch 36/50\n","13/13 [==============================] - 1s 91ms/step - loss: 27.9859 - accuracy: 0.7512 - val_loss: 231.2527 - val_accuracy: 0.4565\n","Epoch 37/50\n","13/13 [==============================] - 2s 119ms/step - loss: 25.8023 - accuracy: 0.7512 - val_loss: 234.2876 - val_accuracy: 0.5000\n","Epoch 38/50\n","13/13 [==============================] - 2s 125ms/step - loss: 25.9958 - accuracy: 0.7561 - val_loss: 231.9245 - val_accuracy: 0.4565\n","Epoch 39/50\n","13/13 [==============================] - 2s 126ms/step - loss: 25.3111 - accuracy: 0.7341 - val_loss: 232.4930 - val_accuracy: 0.5000\n","Epoch 40/50\n","13/13 [==============================] - 1s 96ms/step - loss: 26.3866 - accuracy: 0.7537 - val_loss: 218.9510 - val_accuracy: 0.4783\n","Epoch 41/50\n","13/13 [==============================] - 1s 89ms/step - loss: 26.6270 - accuracy: 0.7512 - val_loss: 238.7199 - val_accuracy: 0.5000\n","Epoch 42/50\n","13/13 [==============================] - 1s 89ms/step - loss: 29.9973 - accuracy: 0.7512 - val_loss: 212.9552 - val_accuracy: 0.4565\n","Epoch 43/50\n","13/13 [==============================] - 1s 93ms/step - loss: 28.2904 - accuracy: 0.7585 - val_loss: 236.1431 - val_accuracy: 0.5435\n","Epoch 44/50\n","13/13 [==============================] - 1s 86ms/step - loss: 26.2098 - accuracy: 0.7659 - val_loss: 219.0828 - val_accuracy: 0.4783\n","Epoch 45/50\n","13/13 [==============================] - 1s 86ms/step - loss: 27.8019 - accuracy: 0.7537 - val_loss: 233.8481 - val_accuracy: 0.5000\n","Epoch 46/50\n","13/13 [==============================] - 1s 92ms/step - loss: 28.5693 - accuracy: 0.7585 - val_loss: 220.9062 - val_accuracy: 0.5000\n","Epoch 47/50\n","13/13 [==============================] - 1s 89ms/step - loss: 29.7569 - accuracy: 0.7707 - val_loss: 226.5253 - val_accuracy: 0.5217\n","Epoch 48/50\n","13/13 [==============================] - 1s 86ms/step - loss: 32.1937 - accuracy: 0.7683 - val_loss: 233.0020 - val_accuracy: 0.4783\n","Epoch 49/50\n","13/13 [==============================] - 1s 114ms/step - loss: 31.5418 - accuracy: 0.7317 - val_loss: 227.4035 - val_accuracy: 0.5217\n","Epoch 50/50\n","13/13 [==============================] - 1s 112ms/step - loss: 24.8657 - accuracy: 0.7585 - val_loss: 228.3815 - val_accuracy: 0.4783\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b2f44d93be0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 0s 23ms/step\n","4/4 [==============================] - 0s 28ms/step - loss: 1.3463 - accuracy: 0.9483\n","Test Loss: 1.3463, Test accuracy : 0.9483\n","Epoch 1/50\n","21/21 [==============================] - 3s 99ms/step - loss: 186.8522 - accuracy: 0.0839 - val_loss: 208.7280 - val_accuracy: 0.1528\n","Epoch 2/50\n","21/21 [==============================] - 2s 86ms/step - loss: 128.7260 - accuracy: 0.0994 - val_loss: 193.3203 - val_accuracy: 0.0833\n","Epoch 3/50\n","21/21 [==============================] - 2s 87ms/step - loss: 100.7790 - accuracy: 0.1630 - val_loss: 175.7058 - val_accuracy: 0.0833\n","Epoch 4/50\n","21/21 [==============================] - 2s 83ms/step - loss: 95.4977 - accuracy: 0.1879 - val_loss: 160.1384 - val_accuracy: 0.0972\n","Epoch 5/50\n","21/21 [==============================] - 2s 95ms/step - loss: 82.3725 - accuracy: 0.1848 - val_loss: 167.0680 - val_accuracy: 0.1667\n","Epoch 6/50\n","21/21 [==============================] - 2s 119ms/step - loss: 76.2252 - accuracy: 0.2189 - val_loss: 159.0716 - val_accuracy: 0.1806\n","Epoch 7/50\n","21/21 [==============================] - 2s 119ms/step - loss: 74.8083 - accuracy: 0.2096 - val_loss: 171.5371 - val_accuracy: 0.1389\n","Epoch 8/50\n","21/21 [==============================] - 2s 86ms/step - loss: 71.4767 - accuracy: 0.2314 - val_loss: 148.5151 - val_accuracy: 0.1250\n","Epoch 9/50\n","21/21 [==============================] - 2s 84ms/step - loss: 109.5023 - accuracy: 0.1724 - val_loss: 162.1003 - val_accuracy: 0.1528\n","Epoch 10/50\n","21/21 [==============================] - 2s 84ms/step - loss: 69.5897 - accuracy: 0.1739 - val_loss: 195.8874 - val_accuracy: 0.1667\n","Epoch 11/50\n","21/21 [==============================] - 2s 85ms/step - loss: 58.9112 - accuracy: 0.2780 - val_loss: 181.1533 - val_accuracy: 0.2222\n","Epoch 12/50\n","21/21 [==============================] - 2s 83ms/step - loss: 49.6717 - accuracy: 0.2904 - val_loss: 173.2953 - val_accuracy: 0.2500\n","Epoch 13/50\n","21/21 [==============================] - 2s 97ms/step - loss: 45.2705 - accuracy: 0.3214 - val_loss: 177.4351 - val_accuracy: 0.2778\n","Epoch 14/50\n","21/21 [==============================] - 2s 118ms/step - loss: 45.5626 - accuracy: 0.3323 - val_loss: 168.3664 - val_accuracy: 0.2639\n","Epoch 15/50\n","21/21 [==============================] - 2s 114ms/step - loss: 58.7984 - accuracy: 0.3401 - val_loss: 168.6306 - val_accuracy: 0.2778\n","Epoch 16/50\n","21/21 [==============================] - 2s 89ms/step - loss: 55.3244 - accuracy: 0.3276 - val_loss: 172.2585 - val_accuracy: 0.2361\n","Epoch 17/50\n","21/21 [==============================] - 2s 86ms/step - loss: 38.8236 - accuracy: 0.3789 - val_loss: 167.6112 - val_accuracy: 0.2917\n","Epoch 18/50\n","21/21 [==============================] - 2s 87ms/step - loss: 37.3440 - accuracy: 0.3882 - val_loss: 170.8779 - val_accuracy: 0.2778\n","Epoch 19/50\n","21/21 [==============================] - 2s 82ms/step - loss: 35.2870 - accuracy: 0.3789 - val_loss: 167.1555 - val_accuracy: 0.2361\n","Epoch 20/50\n","21/21 [==============================] - 2s 84ms/step - loss: 33.0279 - accuracy: 0.3960 - val_loss: 168.0306 - val_accuracy: 0.2361\n","Epoch 21/50\n","21/21 [==============================] - 2s 105ms/step - loss: 31.7309 - accuracy: 0.4161 - val_loss: 164.2188 - val_accuracy: 0.2500\n","Epoch 22/50\n","21/21 [==============================] - 3s 121ms/step - loss: 31.5120 - accuracy: 0.4037 - val_loss: 179.4095 - val_accuracy: 0.2083\n","Epoch 23/50\n","21/21 [==============================] - 2s 100ms/step - loss: 30.4594 - accuracy: 0.3898 - val_loss: 164.2443 - val_accuracy: 0.2500\n","Epoch 24/50\n","21/21 [==============================] - 2s 85ms/step - loss: 29.9178 - accuracy: 0.4193 - val_loss: 189.4545 - val_accuracy: 0.2222\n","Epoch 25/50\n","21/21 [==============================] - 2s 83ms/step - loss: 30.7939 - accuracy: 0.3866 - val_loss: 161.4826 - val_accuracy: 0.2222\n","Epoch 26/50\n","21/21 [==============================] - 2s 83ms/step - loss: 30.3044 - accuracy: 0.4224 - val_loss: 185.9082 - val_accuracy: 0.2083\n","Epoch 27/50\n","21/21 [==============================] - 2s 86ms/step - loss: 30.5482 - accuracy: 0.4099 - val_loss: 144.5735 - val_accuracy: 0.2361\n","Epoch 28/50\n","21/21 [==============================] - 2s 90ms/step - loss: 31.4792 - accuracy: 0.3929 - val_loss: 207.5489 - val_accuracy: 0.2083\n","Epoch 29/50\n","21/21 [==============================] - 2s 112ms/step - loss: 35.6240 - accuracy: 0.4037 - val_loss: 153.9619 - val_accuracy: 0.2083\n","Epoch 30/50\n","21/21 [==============================] - 2s 117ms/step - loss: 31.5116 - accuracy: 0.4363 - val_loss: 174.1395 - val_accuracy: 0.1944\n","Epoch 31/50\n","21/21 [==============================] - 2s 90ms/step - loss: 29.5137 - accuracy: 0.4457 - val_loss: 169.5961 - val_accuracy: 0.1806\n","Epoch 32/50\n","21/21 [==============================] - 2s 86ms/step - loss: 26.1400 - accuracy: 0.4457 - val_loss: 174.2331 - val_accuracy: 0.2083\n","Epoch 33/50\n","21/21 [==============================] - 2s 83ms/step - loss: 24.8586 - accuracy: 0.4550 - val_loss: 167.0080 - val_accuracy: 0.2083\n","Epoch 34/50\n","21/21 [==============================] - 2s 86ms/step - loss: 24.7201 - accuracy: 0.4425 - val_loss: 172.5050 - val_accuracy: 0.2222\n","Epoch 35/50\n","21/21 [==============================] - 2s 84ms/step - loss: 25.5838 - accuracy: 0.4534 - val_loss: 165.9348 - val_accuracy: 0.2361\n","Epoch 36/50\n","21/21 [==============================] - 2s 111ms/step - loss: 23.4206 - accuracy: 0.4596 - val_loss: 174.6443 - val_accuracy: 0.1944\n","Epoch 37/50\n","21/21 [==============================] - 4s 187ms/step - loss: 22.5547 - accuracy: 0.4689 - val_loss: 165.9040 - val_accuracy: 0.1944\n","Epoch 38/50\n","21/21 [==============================] - 2s 114ms/step - loss: 20.9797 - accuracy: 0.4596 - val_loss: 170.3891 - val_accuracy: 0.2222\n","Epoch 39/50\n","21/21 [==============================] - 2s 84ms/step - loss: 21.2201 - accuracy: 0.4519 - val_loss: 164.9760 - val_accuracy: 0.2361\n","Epoch 40/50\n","21/21 [==============================] - 2s 84ms/step - loss: 20.7240 - accuracy: 0.4612 - val_loss: 169.0341 - val_accuracy: 0.2500\n","Epoch 41/50\n","21/21 [==============================] - 2s 86ms/step - loss: 20.1843 - accuracy: 0.4581 - val_loss: 165.2123 - val_accuracy: 0.2361\n","Epoch 42/50\n","21/21 [==============================] - 2s 87ms/step - loss: 21.3616 - accuracy: 0.4612 - val_loss: 172.0018 - val_accuracy: 0.2361\n","Epoch 43/50\n","21/21 [==============================] - 2s 84ms/step - loss: 24.8110 - accuracy: 0.4457 - val_loss: 157.1230 - val_accuracy: 0.2500\n","Epoch 44/50\n","21/21 [==============================] - 2s 114ms/step - loss: 27.6088 - accuracy: 0.4953 - val_loss: 169.9044 - val_accuracy: 0.2778\n","Epoch 45/50\n","21/21 [==============================] - 3s 138ms/step - loss: 30.7288 - accuracy: 0.4658 - val_loss: 164.7941 - val_accuracy: 0.2500\n","Epoch 46/50\n","21/21 [==============================] - 3s 146ms/step - loss: 24.6282 - accuracy: 0.4643 - val_loss: 173.8939 - val_accuracy: 0.2222\n","Epoch 47/50\n","21/21 [==============================] - 2s 109ms/step - loss: 29.3003 - accuracy: 0.4612 - val_loss: 167.7337 - val_accuracy: 0.1944\n","Epoch 48/50\n","21/21 [==============================] - 2s 86ms/step - loss: 134.5784 - accuracy: 0.4519 - val_loss: 182.6522 - val_accuracy: 0.2222\n","Epoch 49/50\n","21/21 [==============================] - 2s 87ms/step - loss: 46.2716 - accuracy: 0.4348 - val_loss: 182.1884 - val_accuracy: 0.2639\n","Epoch 50/50\n","21/21 [==============================] - 2s 84ms/step - loss: 34.2534 - accuracy: 0.4332 - val_loss: 186.9748 - val_accuracy: 0.2222\n","6/6 [==============================] - 0s 18ms/step\n","6/6 [==============================] - 0s 19ms/step - loss: 72.3717 - accuracy: 0.7500\n","Test Loss: 72.3717, Test accuracy : 0.7500\n","Epoch 1/50\n","9/9 [==============================] - 2s 105ms/step - loss: 251.3703 - accuracy: 0.3077 - val_loss: 208.6955 - val_accuracy: 0.4516\n","Epoch 2/50\n","9/9 [==============================] - 1s 88ms/step - loss: 125.2524 - accuracy: 0.4249 - val_loss: 133.7400 - val_accuracy: 0.2581\n","Epoch 3/50\n","9/9 [==============================] - 1s 89ms/step - loss: 100.2232 - accuracy: 0.3993 - val_loss: 144.3232 - val_accuracy: 0.1935\n","Epoch 4/50\n","9/9 [==============================] - 1s 84ms/step - loss: 81.0287 - accuracy: 0.4579 - val_loss: 134.6009 - val_accuracy: 0.2258\n","Epoch 5/50\n","9/9 [==============================] - 1s 81ms/step - loss: 66.5258 - accuracy: 0.4652 - val_loss: 130.7176 - val_accuracy: 0.1935\n","Epoch 6/50\n","9/9 [==============================] - 1s 83ms/step - loss: 58.8795 - accuracy: 0.5458 - val_loss: 136.5739 - val_accuracy: 0.2258\n","Epoch 7/50\n","9/9 [==============================] - 1s 83ms/step - loss: 50.3694 - accuracy: 0.5128 - val_loss: 127.6123 - val_accuracy: 0.1935\n","Epoch 8/50\n","9/9 [==============================] - 1s 85ms/step - loss: 45.0174 - accuracy: 0.4615 - val_loss: 132.6322 - val_accuracy: 0.2258\n","Epoch 9/50\n","9/9 [==============================] - 1s 87ms/step - loss: 39.3699 - accuracy: 0.5238 - val_loss: 131.5668 - val_accuracy: 0.2258\n","Epoch 10/50\n","9/9 [==============================] - 1s 85ms/step - loss: 35.5132 - accuracy: 0.4799 - val_loss: 131.9536 - val_accuracy: 0.2258\n","Epoch 11/50\n","9/9 [==============================] - 1s 84ms/step - loss: 33.0163 - accuracy: 0.5348 - val_loss: 133.0741 - val_accuracy: 0.2581\n","Epoch 12/50\n","9/9 [==============================] - 1s 85ms/step - loss: 30.7352 - accuracy: 0.5311 - val_loss: 133.0819 - val_accuracy: 0.2903\n","Epoch 13/50\n","9/9 [==============================] - 1s 108ms/step - loss: 29.1690 - accuracy: 0.5201 - val_loss: 132.2491 - val_accuracy: 0.2903\n","Epoch 14/50\n","9/9 [==============================] - 1s 131ms/step - loss: 27.1299 - accuracy: 0.5092 - val_loss: 132.9601 - val_accuracy: 0.2903\n","Epoch 15/50\n","9/9 [==============================] - 1s 113ms/step - loss: 25.8203 - accuracy: 0.5165 - val_loss: 133.9496 - val_accuracy: 0.2903\n","Epoch 16/50\n","9/9 [==============================] - 1s 122ms/step - loss: 24.5243 - accuracy: 0.5055 - val_loss: 133.8172 - val_accuracy: 0.2903\n","Epoch 17/50\n","9/9 [==============================] - 1s 131ms/step - loss: 23.7327 - accuracy: 0.5201 - val_loss: 137.1961 - val_accuracy: 0.2903\n","Epoch 18/50\n","9/9 [==============================] - 1s 100ms/step - loss: 22.9205 - accuracy: 0.5018 - val_loss: 134.3677 - val_accuracy: 0.2581\n","Epoch 19/50\n","9/9 [==============================] - 1s 86ms/step - loss: 21.6233 - accuracy: 0.5055 - val_loss: 138.4902 - val_accuracy: 0.2903\n","Epoch 20/50\n","9/9 [==============================] - 1s 89ms/step - loss: 20.4997 - accuracy: 0.5678 - val_loss: 134.6739 - val_accuracy: 0.2581\n","Epoch 21/50\n","9/9 [==============================] - 1s 87ms/step - loss: 20.1523 - accuracy: 0.4908 - val_loss: 141.0480 - val_accuracy: 0.2903\n","Epoch 22/50\n","9/9 [==============================] - 1s 85ms/step - loss: 19.6012 - accuracy: 0.6081 - val_loss: 137.7675 - val_accuracy: 0.2581\n","Epoch 23/50\n","9/9 [==============================] - 1s 89ms/step - loss: 18.3661 - accuracy: 0.5275 - val_loss: 138.6680 - val_accuracy: 0.2903\n","Epoch 24/50\n","9/9 [==============================] - 1s 89ms/step - loss: 17.2121 - accuracy: 0.6190 - val_loss: 140.2424 - val_accuracy: 0.2581\n","Epoch 25/50\n","9/9 [==============================] - 1s 87ms/step - loss: 16.8619 - accuracy: 0.5385 - val_loss: 136.7543 - val_accuracy: 0.2903\n","Epoch 26/50\n","9/9 [==============================] - 1s 89ms/step - loss: 15.8359 - accuracy: 0.5531 - val_loss: 141.6775 - val_accuracy: 0.2581\n","Epoch 27/50\n","9/9 [==============================] - 1s 94ms/step - loss: 15.5505 - accuracy: 0.5824 - val_loss: 137.6868 - val_accuracy: 0.2903\n","Epoch 28/50\n","9/9 [==============================] - 1s 92ms/step - loss: 14.4979 - accuracy: 0.5971 - val_loss: 139.0996 - val_accuracy: 0.2581\n","Epoch 29/50\n","9/9 [==============================] - 1s 88ms/step - loss: 14.0649 - accuracy: 0.6081 - val_loss: 144.6123 - val_accuracy: 0.2581\n","Epoch 30/50\n","9/9 [==============================] - 1s 84ms/step - loss: 13.9370 - accuracy: 0.6081 - val_loss: 137.3031 - val_accuracy: 0.3226\n","Epoch 31/50\n","9/9 [==============================] - 1s 131ms/step - loss: 13.8140 - accuracy: 0.5971 - val_loss: 143.2612 - val_accuracy: 0.2581\n","Epoch 32/50\n","9/9 [==============================] - 1s 125ms/step - loss: 14.3054 - accuracy: 0.5897 - val_loss: 140.9272 - val_accuracy: 0.3226\n","Epoch 33/50\n","9/9 [==============================] - 1s 122ms/step - loss: 17.2537 - accuracy: 0.6007 - val_loss: 146.6656 - val_accuracy: 0.2903\n","Epoch 34/50\n","9/9 [==============================] - 1s 122ms/step - loss: 17.6215 - accuracy: 0.6007 - val_loss: 138.3367 - val_accuracy: 0.3226\n","Epoch 35/50\n","9/9 [==============================] - 1s 103ms/step - loss: 15.6173 - accuracy: 0.5788 - val_loss: 148.7771 - val_accuracy: 0.2903\n","Epoch 36/50\n","9/9 [==============================] - 1s 87ms/step - loss: 18.1080 - accuracy: 0.5751 - val_loss: 143.8233 - val_accuracy: 0.2903\n","Epoch 37/50\n","9/9 [==============================] - 1s 91ms/step - loss: 18.1407 - accuracy: 0.6044 - val_loss: 142.9198 - val_accuracy: 0.3548\n","Epoch 38/50\n","9/9 [==============================] - 1s 88ms/step - loss: 17.7783 - accuracy: 0.6300 - val_loss: 140.9672 - val_accuracy: 0.2903\n","Epoch 39/50\n","9/9 [==============================] - 1s 89ms/step - loss: 14.3840 - accuracy: 0.5861 - val_loss: 148.0445 - val_accuracy: 0.2903\n","Epoch 40/50\n","9/9 [==============================] - 1s 89ms/step - loss: 13.6457 - accuracy: 0.6337 - val_loss: 139.0663 - val_accuracy: 0.3226\n","Epoch 41/50\n","9/9 [==============================] - 1s 90ms/step - loss: 13.6733 - accuracy: 0.6007 - val_loss: 142.9639 - val_accuracy: 0.2903\n","Epoch 42/50\n","9/9 [==============================] - 1s 90ms/step - loss: 12.0483 - accuracy: 0.6410 - val_loss: 147.6522 - val_accuracy: 0.2903\n","Epoch 43/50\n","9/9 [==============================] - 1s 86ms/step - loss: 11.7011 - accuracy: 0.5971 - val_loss: 139.5431 - val_accuracy: 0.3226\n","Epoch 44/50\n","9/9 [==============================] - 1s 85ms/step - loss: 11.2012 - accuracy: 0.6337 - val_loss: 142.9671 - val_accuracy: 0.2581\n","Epoch 45/50\n","9/9 [==============================] - 1s 87ms/step - loss: 11.2845 - accuracy: 0.6007 - val_loss: 144.6343 - val_accuracy: 0.2903\n","Epoch 46/50\n","9/9 [==============================] - 1s 87ms/step - loss: 13.5337 - accuracy: 0.5934 - val_loss: 138.0153 - val_accuracy: 0.3226\n","Epoch 47/50\n","9/9 [==============================] - 1s 88ms/step - loss: 12.7020 - accuracy: 0.6520 - val_loss: 154.4641 - val_accuracy: 0.2581\n","Epoch 48/50\n","9/9 [==============================] - 1s 135ms/step - loss: 12.7804 - accuracy: 0.6337 - val_loss: 135.5768 - val_accuracy: 0.3871\n","Epoch 49/50\n","9/9 [==============================] - 1s 124ms/step - loss: 12.0208 - accuracy: 0.6300 - val_loss: 141.3544 - val_accuracy: 0.3226\n","Epoch 50/50\n","9/9 [==============================] - 1s 116ms/step - loss: 12.8154 - accuracy: 0.6886 - val_loss: 135.5679 - val_accuracy: 0.3226\n","3/3 [==============================] - 0s 14ms/step\n","3/3 [==============================] - 0s 14ms/step - loss: 3158.4004 - accuracy: 0.9875\n","Test Loss: 3158.4004, Test accuracy : 0.9875\n","Epoch 1/50\n","6/6 [==============================] - 2s 151ms/step - loss: 219.2532 - accuracy: 0.2995 - val_loss: 76.3078 - val_accuracy: 0.2857\n","Epoch 2/50\n","6/6 [==============================] - 1s 96ms/step - loss: 157.6176 - accuracy: 0.5668 - val_loss: 55.5690 - val_accuracy: 0.0476\n","Epoch 3/50\n","6/6 [==============================] - 1s 87ms/step - loss: 69.6771 - accuracy: 0.5294 - val_loss: 56.5120 - val_accuracy: 0.4762\n","Epoch 4/50\n","6/6 [==============================] - 1s 89ms/step - loss: 71.4146 - accuracy: 0.2727 - val_loss: 61.6841 - val_accuracy: 0.2381\n","Epoch 5/50\n","6/6 [==============================] - 0s 83ms/step - loss: 49.3581 - accuracy: 0.4225 - val_loss: 53.6460 - val_accuracy: 0.4762\n","Epoch 6/50\n","6/6 [==============================] - 1s 95ms/step - loss: 33.0370 - accuracy: 0.4011 - val_loss: 55.4428 - val_accuracy: 0.0952\n","Epoch 7/50\n","6/6 [==============================] - 1s 87ms/step - loss: 25.7491 - accuracy: 0.3209 - val_loss: 48.6273 - val_accuracy: 0.6190\n","Epoch 8/50\n","6/6 [==============================] - 1s 85ms/step - loss: 22.6809 - accuracy: 0.4225 - val_loss: 48.9191 - val_accuracy: 0.6667\n","Epoch 9/50\n","6/6 [==============================] - 0s 82ms/step - loss: 16.5716 - accuracy: 0.5989 - val_loss: 51.4577 - val_accuracy: 0.2857\n","Epoch 10/50\n","6/6 [==============================] - 1s 97ms/step - loss: 14.2493 - accuracy: 0.3369 - val_loss: 51.1461 - val_accuracy: 0.4286\n","Epoch 11/50\n","6/6 [==============================] - 1s 89ms/step - loss: 12.1511 - accuracy: 0.5134 - val_loss: 49.6398 - val_accuracy: 0.4286\n","Epoch 12/50\n","6/6 [==============================] - 1s 91ms/step - loss: 11.1443 - accuracy: 0.5401 - val_loss: 49.7347 - val_accuracy: 0.3810\n","Epoch 13/50\n","6/6 [==============================] - 1s 88ms/step - loss: 10.2900 - accuracy: 0.3850 - val_loss: 50.9309 - val_accuracy: 0.3333\n","Epoch 14/50\n","6/6 [==============================] - 1s 89ms/step - loss: 10.1556 - accuracy: 0.5989 - val_loss: 50.3202 - val_accuracy: 0.3810\n","Epoch 15/50\n","6/6 [==============================] - 1s 88ms/step - loss: 8.9902 - accuracy: 0.4973 - val_loss: 51.0144 - val_accuracy: 0.4286\n","Epoch 16/50\n","6/6 [==============================] - 1s 93ms/step - loss: 8.2392 - accuracy: 0.5187 - val_loss: 53.1801 - val_accuracy: 0.2381\n","Epoch 17/50\n","6/6 [==============================] - 1s 93ms/step - loss: 8.0271 - accuracy: 0.5882 - val_loss: 50.3256 - val_accuracy: 0.5238\n","Epoch 18/50\n","6/6 [==============================] - 1s 91ms/step - loss: 7.3214 - accuracy: 0.5722 - val_loss: 51.5321 - val_accuracy: 0.2857\n","Epoch 19/50\n","6/6 [==============================] - 1s 108ms/step - loss: 7.0478 - accuracy: 0.5775 - val_loss: 50.9036 - val_accuracy: 0.4286\n","Epoch 20/50\n","6/6 [==============================] - 1s 129ms/step - loss: 6.4688 - accuracy: 0.4813 - val_loss: 51.4157 - val_accuracy: 0.3333\n","Epoch 21/50\n","6/6 [==============================] - 1s 142ms/step - loss: 6.2825 - accuracy: 0.5134 - val_loss: 51.0532 - val_accuracy: 0.3333\n","Epoch 22/50\n","6/6 [==============================] - 1s 131ms/step - loss: 5.7782 - accuracy: 0.6096 - val_loss: 51.9872 - val_accuracy: 0.3333\n","Epoch 23/50\n","6/6 [==============================] - 1s 117ms/step - loss: 5.8040 - accuracy: 0.5668 - val_loss: 51.3638 - val_accuracy: 0.4286\n","Epoch 24/50\n","6/6 [==============================] - 1s 136ms/step - loss: 6.0279 - accuracy: 0.5294 - val_loss: 52.9989 - val_accuracy: 0.2381\n","Epoch 25/50\n","6/6 [==============================] - 1s 124ms/step - loss: 5.7746 - accuracy: 0.5615 - val_loss: 51.7151 - val_accuracy: 0.4762\n","Epoch 26/50\n","6/6 [==============================] - 1s 90ms/step - loss: 5.5672 - accuracy: 0.5241 - val_loss: 54.7306 - val_accuracy: 0.2381\n","Epoch 27/50\n","6/6 [==============================] - 1s 90ms/step - loss: 4.9489 - accuracy: 0.5187 - val_loss: 51.1797 - val_accuracy: 0.5714\n","Epoch 28/50\n","6/6 [==============================] - 1s 84ms/step - loss: 5.4673 - accuracy: 0.5989 - val_loss: 52.3875 - val_accuracy: 0.3810\n","Epoch 29/50\n","6/6 [==============================] - 1s 86ms/step - loss: 4.6622 - accuracy: 0.5508 - val_loss: 51.7166 - val_accuracy: 0.4286\n","Epoch 30/50\n","6/6 [==============================] - 1s 83ms/step - loss: 6.5327 - accuracy: 0.5829 - val_loss: 53.6367 - val_accuracy: 0.3333\n","Epoch 31/50\n","6/6 [==============================] - 1s 89ms/step - loss: 5.3926 - accuracy: 0.5348 - val_loss: 52.1922 - val_accuracy: 0.4762\n","Epoch 32/50\n","6/6 [==============================] - 1s 97ms/step - loss: 7.4522 - accuracy: 0.5615 - val_loss: 53.7005 - val_accuracy: 0.3333\n","Epoch 33/50\n","6/6 [==============================] - 1s 91ms/step - loss: 7.1942 - accuracy: 0.5241 - val_loss: 55.4060 - val_accuracy: 0.2857\n","Epoch 34/50\n","6/6 [==============================] - 1s 89ms/step - loss: 5.1094 - accuracy: 0.5241 - val_loss: 53.0765 - val_accuracy: 0.3810\n","Epoch 35/50\n","6/6 [==============================] - 1s 88ms/step - loss: 5.2418 - accuracy: 0.5134 - val_loss: 55.3309 - val_accuracy: 0.3810\n","Epoch 36/50\n","6/6 [==============================] - 1s 89ms/step - loss: 4.2361 - accuracy: 0.4759 - val_loss: 53.9928 - val_accuracy: 0.4762\n","Epoch 37/50\n","6/6 [==============================] - 1s 85ms/step - loss: 5.1637 - accuracy: 0.5080 - val_loss: 55.9553 - val_accuracy: 0.3810\n","Epoch 38/50\n","6/6 [==============================] - 1s 86ms/step - loss: 4.2974 - accuracy: 0.5027 - val_loss: 54.2922 - val_accuracy: 0.4286\n","Epoch 39/50\n","6/6 [==============================] - 1s 85ms/step - loss: 5.0602 - accuracy: 0.5455 - val_loss: 53.8978 - val_accuracy: 0.4286\n","Epoch 40/50\n","6/6 [==============================] - 1s 90ms/step - loss: 4.9867 - accuracy: 0.5508 - val_loss: 56.9048 - val_accuracy: 0.3810\n","Epoch 41/50\n","6/6 [==============================] - 1s 95ms/step - loss: 3.6282 - accuracy: 0.5561 - val_loss: 54.7040 - val_accuracy: 0.3810\n","Epoch 42/50\n","6/6 [==============================] - 1s 91ms/step - loss: 3.5344 - accuracy: 0.5508 - val_loss: 55.6871 - val_accuracy: 0.2381\n","Epoch 43/50\n","6/6 [==============================] - 1s 87ms/step - loss: 3.2659 - accuracy: 0.5722 - val_loss: 54.0380 - val_accuracy: 0.3810\n","Epoch 44/50\n","6/6 [==============================] - 1s 95ms/step - loss: 5.1830 - accuracy: 0.5561 - val_loss: 55.5111 - val_accuracy: 0.4286\n","Epoch 45/50\n","6/6 [==============================] - 1s 121ms/step - loss: 4.4558 - accuracy: 0.5615 - val_loss: 60.4455 - val_accuracy: 0.1905\n","Epoch 46/50\n","6/6 [==============================] - 1s 113ms/step - loss: 8.1346 - accuracy: 0.5134 - val_loss: 52.4651 - val_accuracy: 0.3333\n","Epoch 47/50\n","6/6 [==============================] - 1s 124ms/step - loss: 8.2952 - accuracy: 0.4545 - val_loss: 55.3371 - val_accuracy: 0.2381\n","Epoch 48/50\n","6/6 [==============================] - 1s 114ms/step - loss: 7.5819 - accuracy: 0.4278 - val_loss: 59.3276 - val_accuracy: 0.5238\n","Epoch 49/50\n","6/6 [==============================] - 1s 128ms/step - loss: 5.5290 - accuracy: 0.6150 - val_loss: 60.3560 - val_accuracy: 0.4762\n","Epoch 50/50\n","6/6 [==============================] - 1s 124ms/step - loss: 6.0353 - accuracy: 0.5508 - val_loss: 58.1212 - val_accuracy: 0.5238\n","2/2 [==============================] - 0s 25ms/step\n","2/2 [==============================] - 0s 33ms/step - loss: 18.0654 - accuracy: 0.9643\n","Test Loss: 18.0654, Test accuracy : 0.9643\n"]}],"source":["for rule in max_Y.keys():\n","  tajweed_rule_model(abdul_basit, yassin_aljazaery, ibrahim_aldosary, al_husary, rule)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717521592379,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"i4FV1w-iDDFU"},"outputs":[],"source":["def print_model_summary(loaded_model, tajweed_rule):\n","  print(f'******* Tajweed rule {tajweed_rule} model *******')\n","  loaded_model.summary()\n","  print('\\n')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4859,"status":"ok","timestamp":1717521597233,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"WbRKnuWnLZTc","outputId":"238ec5b7-cdc9-4bb9-bc8a-3a159391b0b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["******* Tajweed rule madd_6_Lazim model *******\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten (Flatten)           (None, 104000)            0         \n","                                                                 \n"," dense (Dense)               (None, 64)                6656064   \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 6656194 (25.39 MB)\n","Trainable params: 6656194 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_246 model *******\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 195       \n","                                                                 \n","=================================================================\n","Total params: 6656259 (25.39 MB)\n","Trainable params: 6656259 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_6 model *******\n","Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_5 (Dense)             (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 6656454 (25.39 MB)\n","Trainable params: 6656454 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_2 model *******\n","Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_6 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_7 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 6656389 (25.39 MB)\n","Trainable params: 6656389 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Ikhfaa model *******\n","Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_4 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_8 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_9 (Dense)             (None, 9)                 585       \n","                                                                 \n","=================================================================\n","Total params: 6656649 (25.39 MB)\n","Trainable params: 6656649 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Idgham model *******\n","Model: \"model_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_6 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_5 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_10 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_11 (Dense)            (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 6656909 (25.39 MB)\n","Trainable params: 6656909 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule tafkhim model *******\n","Model: \"model_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_6 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_12 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_13 (Dense)            (None, 24)                1560      \n","                                                                 \n","=================================================================\n","Total params: 6657624 (25.40 MB)\n","Trainable params: 6657624 (25.40 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule qalqala model *******\n","Model: \"model_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_8 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_7 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_14 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_15 (Dense)            (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 6656454 (25.39 MB)\n","Trainable params: 6656454 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule imala model *******\n","Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_9 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_8 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_16 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_17 (Dense)            (None, 7)                 455       \n","                                                                 \n","=================================================================\n","Total params: 6656519 (25.39 MB)\n","Trainable params: 6656519 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n"]}],"source":["for rule in max_Y.keys():\n","    model_filename = f'{rule}_tajweed_rule_model'\n","    model_path = os.path.join(export_dir, model_filename)\n","\n","    # Load the saved model\n","    loaded_model = tf.keras.models.load_model(model_path)\n","\n","    print_model_summary(loaded_model, rule)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1717521597234,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"txrZx0e_4Zmw"},"outputs":[],"source":["# how data is splitted\n","columns1 = ['tajweed_rule', 'data_of', 'X_train_nb_samples', 'X_test_nb_samples', 'Y_train_nb_samples', 'X_test_nb_samples']\n","splitted_data_info = pd.DataFrame(data=splitted_data_info_np, columns=columns1)\n","\n","# save models information\n","columns2 = ['Model', 'Loss', 'Accuracy', 'Accuracy %', 'Path_to_the_model']\n","models_information = pd.DataFrame(data=models_information_np, columns=columns2)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1717521597235,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"8D4mYXj0Rcqb","outputId":"f681e2d2-ceed-4838-b043-f205359b0a97"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"splitted_data_info\",\n  \"rows\": 45,\n  \"fields\": [\n    {\n      \"column\": \"tajweed_rule\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala\",\n          \"madd_246\",\n          \"Idgham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_of\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Yassin Al Jazaery\",\n          \"all reciters\",\n          \"Ibrahim_Aldosary\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"1\",\n          \"4\",\n          \"119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"1\",\n          \"4\",\n          \"48\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"1\",\n          \"4\",\n          \"119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"1\",\n          \"4\",\n          \"48\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"splitted_data_info"},"text/html":["\n","  <div id=\"df-8148229b-5218-4af5-b098-19db1d1f3c4c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tajweed_rule</th>\n","      <th>data_of</th>\n","      <th>X_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","      <th>Y_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Abdul Basit</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Al husary</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>madd_6_Lazim</td>\n","      <td>all reciters</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>madd_246</td>\n","      <td>Abdul Basit</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>madd_246</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>madd_246</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>madd_246</td>\n","      <td>Al husary</td>\n","      <td>62</td>\n","      <td>16</td>\n","      <td>62</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>madd_246</td>\n","      <td>all reciters</td>\n","      <td>248</td>\n","      <td>64</td>\n","      <td>248</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>madd_6</td>\n","      <td>Abdul Basit</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>madd_6</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>madd_6</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>madd_6</td>\n","      <td>Al husary</td>\n","      <td>47</td>\n","      <td>12</td>\n","      <td>47</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>madd_6</td>\n","      <td>all reciters</td>\n","      <td>188</td>\n","      <td>48</td>\n","      <td>188</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>madd_2</td>\n","      <td>Abdul Basit</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>madd_2</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>madd_2</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>madd_2</td>\n","      <td>Al husary</td>\n","      <td>79</td>\n","      <td>20</td>\n","      <td>79</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>madd_2</td>\n","      <td>all reciters</td>\n","      <td>316</td>\n","      <td>80</td>\n","      <td>316</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Ikhfaa</td>\n","      <td>Abdul Basit</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Ikhfaa</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Ikhfaa</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Ikhfaa</td>\n","      <td>Al husary</td>\n","      <td>119</td>\n","      <td>30</td>\n","      <td>119</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Ikhfaa</td>\n","      <td>all reciters</td>\n","      <td>476</td>\n","      <td>120</td>\n","      <td>476</td>\n","      <td>120</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Idgham</td>\n","      <td>Abdul Basit</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Idgham</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Idgham</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Idgham</td>\n","      <td>Al husary</td>\n","      <td>114</td>\n","      <td>29</td>\n","      <td>114</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Idgham</td>\n","      <td>all reciters</td>\n","      <td>456</td>\n","      <td>116</td>\n","      <td>456</td>\n","      <td>116</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>tafkhim</td>\n","      <td>Abdul Basit</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>tafkhim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>tafkhim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>tafkhim</td>\n","      <td>Al husary</td>\n","      <td>179</td>\n","      <td>45</td>\n","      <td>179</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>tafkhim</td>\n","      <td>all reciters</td>\n","      <td>716</td>\n","      <td>180</td>\n","      <td>716</td>\n","      <td>180</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>qalqala</td>\n","      <td>Abdul Basit</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>qalqala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>qalqala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>qalqala</td>\n","      <td>Al husary</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>76</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>qalqala</td>\n","      <td>all reciters</td>\n","      <td>304</td>\n","      <td>80</td>\n","      <td>304</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>imala</td>\n","      <td>Abdul Basit</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>imala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>imala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>imala</td>\n","      <td>Al husary</td>\n","      <td>52</td>\n","      <td>14</td>\n","      <td>52</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>imala</td>\n","      <td>all reciters</td>\n","      <td>208</td>\n","      <td>56</td>\n","      <td>208</td>\n","      <td>56</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8148229b-5218-4af5-b098-19db1d1f3c4c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8148229b-5218-4af5-b098-19db1d1f3c4c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8148229b-5218-4af5-b098-19db1d1f3c4c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-577a521f-bf6f-4227-83e5-2959c18c04e8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-577a521f-bf6f-4227-83e5-2959c18c04e8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-577a521f-bf6f-4227-83e5-2959c18c04e8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["    tajweed_rule            data_of X_train_nb_samples X_test_nb_samples  \\\n","0   madd_6_Lazim        Abdul Basit                  1                 1   \n","1   madd_6_Lazim  Yassin Al Jazaery                  1                 1   \n","2   madd_6_Lazim   Ibrahim_Aldosary                  1                 1   \n","3   madd_6_Lazim          Al husary                  1                 1   \n","4   madd_6_Lazim       all reciters                  4                 4   \n","5       madd_246        Abdul Basit                 62                16   \n","6       madd_246  Yassin Al Jazaery                 62                16   \n","7       madd_246   Ibrahim_Aldosary                 62                16   \n","8       madd_246          Al husary                 62                16   \n","9       madd_246       all reciters                248                64   \n","10        madd_6        Abdul Basit                 47                12   \n","11        madd_6  Yassin Al Jazaery                 47                12   \n","12        madd_6   Ibrahim_Aldosary                 47                12   \n","13        madd_6          Al husary                 47                12   \n","14        madd_6       all reciters                188                48   \n","15        madd_2        Abdul Basit                 79                20   \n","16        madd_2  Yassin Al Jazaery                 79                20   \n","17        madd_2   Ibrahim_Aldosary                 79                20   \n","18        madd_2          Al husary                 79                20   \n","19        madd_2       all reciters                316                80   \n","20        Ikhfaa        Abdul Basit                119                30   \n","21        Ikhfaa  Yassin Al Jazaery                119                30   \n","22        Ikhfaa   Ibrahim_Aldosary                119                30   \n","23        Ikhfaa          Al husary                119                30   \n","24        Ikhfaa       all reciters                476               120   \n","25        Idgham        Abdul Basit                114                29   \n","26        Idgham  Yassin Al Jazaery                114                29   \n","27        Idgham   Ibrahim_Aldosary                114                29   \n","28        Idgham          Al husary                114                29   \n","29        Idgham       all reciters                456               116   \n","30       tafkhim        Abdul Basit                179                45   \n","31       tafkhim  Yassin Al Jazaery                179                45   \n","32       tafkhim   Ibrahim_Aldosary                179                45   \n","33       tafkhim          Al husary                179                45   \n","34       tafkhim       all reciters                716               180   \n","35       qalqala        Abdul Basit                 76                20   \n","36       qalqala  Yassin Al Jazaery                 76                20   \n","37       qalqala   Ibrahim_Aldosary                 76                20   \n","38       qalqala          Al husary                 76                20   \n","39       qalqala       all reciters                304                80   \n","40         imala        Abdul Basit                 52                14   \n","41         imala  Yassin Al Jazaery                 52                14   \n","42         imala   Ibrahim_Aldosary                 52                14   \n","43         imala          Al husary                 52                14   \n","44         imala       all reciters                208                56   \n","\n","   Y_train_nb_samples X_test_nb_samples  \n","0                   1                 1  \n","1                   1                 1  \n","2                   1                 1  \n","3                   1                 1  \n","4                   4                 4  \n","5                  62                16  \n","6                  62                16  \n","7                  62                16  \n","8                  62                16  \n","9                 248                64  \n","10                 47                12  \n","11                 47                12  \n","12                 47                12  \n","13                 47                12  \n","14                188                48  \n","15                 79                20  \n","16                 79                20  \n","17                 79                20  \n","18                 79                20  \n","19                316                80  \n","20                119                30  \n","21                119                30  \n","22                119                30  \n","23                119                30  \n","24                476               120  \n","25                114                29  \n","26                114                29  \n","27                114                29  \n","28                114                29  \n","29                456               116  \n","30                179                45  \n","31                179                45  \n","32                179                45  \n","33                179                45  \n","34                716               180  \n","35                 76                20  \n","36                 76                20  \n","37                 76                20  \n","38                 76                20  \n","39                304                80  \n","40                 52                14  \n","41                 52                14  \n","42                 52                14  \n","43                 52                14  \n","44                208                56  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["splitted_data_info"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":837},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1717521597236,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"},"user_tz":-60},"id":"h3kOCPqVuemS","outputId":"ba2d470f-a6bf-46a4-97fe-07d484b2d689"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"models_information\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala_tajweed_rule_model\",\n          \"madd_246_tajweed_rule_model\",\n          \"Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Loss\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"3158.4004\",\n          \"32.5090\",\n          \"1.3463\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"0.9844\",\n          \"0.7500\",\n          \"1.0000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy %\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"98.44\",\n          \"75.00\",\n          \"100.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Path_to_the_model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v3/qalqala_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v3/madd_246_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v3/Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"models_information"},"text/html":["\n","  <div id=\"df-afee2d6b-456e-41d9-9fdd-de9c6ed78a8f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Loss</th>\n","      <th>Accuracy</th>\n","      <th>Accuracy %</th>\n","      <th>Path_to_the_model</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim_tajweed_rule_model</td>\n","      <td>62.7340</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_246_tajweed_rule_model</td>\n","      <td>32.5090</td>\n","      <td>0.9844</td>\n","      <td>98.44</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_tajweed_rule_model</td>\n","      <td>454.8297</td>\n","      <td>0.8542</td>\n","      <td>85.42</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_2_tajweed_rule_model</td>\n","      <td>87.7179</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ikhfaa_tajweed_rule_model</td>\n","      <td>1.4711</td>\n","      <td>0.8583</td>\n","      <td>85.83</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Idgham_tajweed_rule_model</td>\n","      <td>1.3463</td>\n","      <td>0.9483</td>\n","      <td>94.83</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tafkhim_tajweed_rule_model</td>\n","      <td>72.3717</td>\n","      <td>0.7500</td>\n","      <td>75.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>qalqala_tajweed_rule_model</td>\n","      <td>3158.4004</td>\n","      <td>0.9875</td>\n","      <td>98.75</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>imala_tajweed_rule_model</td>\n","      <td>18.0654</td>\n","      <td>0.9643</td>\n","      <td>96.43</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-afee2d6b-456e-41d9-9fdd-de9c6ed78a8f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-afee2d6b-456e-41d9-9fdd-de9c6ed78a8f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-afee2d6b-456e-41d9-9fdd-de9c6ed78a8f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-742c8eab-537a-4201-b7b6-b293c38e1c31\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-742c8eab-537a-4201-b7b6-b293c38e1c31')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-742c8eab-537a-4201-b7b6-b293c38e1c31 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                             Model       Loss Accuracy Accuracy %  \\\n","0  madd_6_Lazim_tajweed_rule_model    62.7340   1.0000     100.00   \n","1      madd_246_tajweed_rule_model    32.5090   0.9844      98.44   \n","2        madd_6_tajweed_rule_model   454.8297   0.8542      85.42   \n","3        madd_2_tajweed_rule_model    87.7179   1.0000     100.00   \n","4        Ikhfaa_tajweed_rule_model     1.4711   0.8583      85.83   \n","5        Idgham_tajweed_rule_model     1.3463   0.9483      94.83   \n","6       tafkhim_tajweed_rule_model    72.3717   0.7500      75.00   \n","7       qalqala_tajweed_rule_model  3158.4004   0.9875      98.75   \n","8         imala_tajweed_rule_model    18.0654   0.9643      96.43   \n","\n","                                   Path_to_the_model  \n","0  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","1  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","2  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","3  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","4  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","5  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","6  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","7  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","8  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["models_information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyNcQMGlOFzq"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPfY9EwZHdTYyORV/6cB2af","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
