{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuSuBN5gvpTj3784Le1vpa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aWO1vAVp7HkG","executionInfo":{"status":"ok","timestamp":1717522119292,"user_tz":-60,"elapsed":30584,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"2f9f4015-8c8f-4360-a732-38bc2465fc3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.models import Model"],"metadata":{"id":"6I2s5Q0iDpDE","executionInfo":{"status":"ok","timestamp":1717522125359,"user_tz":-60,"elapsed":6077,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Load data\n","data = pd.read_csv('/content/drive/My Drive/M2 GL/PFE/Data/hisb_60_and_Al_fatihah_audio_with_transcript_and_MFCC_and_ahkam_indexing_v3.csv')\n","safa_data = pd.read_csv('/content/drive/My Drive/M2 GL/PFE/Data/safa_hisb_60_and_Al_fatihah_audio_with_transcript_and_MFCC_and_ahkam_indexing.csv')"],"metadata":{"id":"WtJVQemz7klg","executionInfo":{"status":"ok","timestamp":1717522137150,"user_tz":-60,"elapsed":11799,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["export_dir = '/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v4'"],"metadata":{"id":"7A9PSizPHQos","executionInfo":{"status":"ok","timestamp":1717522137152,"user_tz":-60,"elapsed":15,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["abdul_basit = data[data['recitor_en'] == 'Abdul Basit']\n","yassin_aljazaery = data[data['recitor_en'] == 'Yassin Al Jazaery']\n","ibrahim_aldosary = data[data['recitor_en'] == 'Ibrahim_Aldosary']\n","al_husary = data[data['recitor_en'] == 'Al husary']"],"metadata":{"id":"KoKbTBQr7nY8","executionInfo":{"status":"ok","timestamp":1717522137153,"user_tz":-60,"elapsed":14,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["splitted_data_info_np = np.empty((0, 6))\n","models_information_np = np.empty((0, 5))"],"metadata":{"id":"_CenkKQf-AXc","executionInfo":{"status":"ok","timestamp":1717522137153,"user_tz":-60,"elapsed":11,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["max_Y = {'madd_6_Lazim': 2, 'madd_246': 3, 'madd_6': 6, 'madd_2': 5, 'Ikhfaa': 9, 'Idgham': 13, 'tafkhim': 24, 'qalqala': 6, 'imala': 7}\n","max_X = 8000"],"metadata":{"id":"H950tSVTMxlL","executionInfo":{"status":"ok","timestamp":1717522137153,"user_tz":-60,"elapsed":9,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def data_preparation(reciter_data, tajweed_rule):\n","  # Extract 'mfcc' and tajweed_rule columns as lists of strings\n","  X_raw = reciter_data['mfcc'].astype(str).tolist()\n","  Y_raw = reciter_data[tajweed_rule].astype(str).tolist()\n","\n","  # Preprocess the input data (X)\n","  X = [tf.constant(eval(x)) for x in X_raw]\n","  Y = [tf.constant(eval(x)) for x in Y_raw]\n","\n","  # Pad sequences in Y and X to ensure all have the same length\n","  Y_padded = tf.keras.preprocessing.sequence.pad_sequences(Y, maxlen=max_Y[tajweed_rule], padding='post', dtype='int32', value=-1)\n","  X_padded = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_X, padding='post', dtype='float32')\n","\n","  # Split the data into training and testing sets\n","  X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_padded, test_size=0.2, random_state=10)\n","  return X_train, X_test, Y_train, Y_test"],"metadata":{"id":"pVbn5z76-K4O","executionInfo":{"status":"ok","timestamp":1717522137153,"user_tz":-60,"elapsed":8,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def tajweed_rule_model(reciter1, reciter2, reciter3, reciter4, not_exp, tajweed_rule):\n","  global splitted_data_info_np, models_information_np, data\n","  # data preparation\n","  reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test = data_preparation(reciter1, tajweed_rule)\n","  reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test = data_preparation(reciter2, tajweed_rule)\n","  reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test = data_preparation(reciter3, tajweed_rule)\n","  reciter4_X_train, reciter4_X_test, reciter4_Y_train, reciter4_Y_test = data_preparation(reciter4, tajweed_rule)\n","  not_exp_X_train, not_exp_X_test, not_exp_Y_train, not_exp_Y_test = data_preparation(not_exp, tajweed_rule)\n","\n","\n","  # Update splitted_data_info with information about each reciter\n","  for reciter_X_train, reciter_X_test, reciter_Y_train, reciter_Y_test, reciter_data in [\n","      (reciter1_X_train, reciter1_X_test, reciter1_Y_train, reciter1_Y_test, reciter1),\n","      (reciter2_X_train, reciter2_X_test, reciter2_Y_train, reciter2_Y_test, reciter2),\n","      (reciter3_X_train, reciter3_X_test, reciter3_Y_train, reciter3_Y_test, reciter3),\n","      (reciter4_X_train, reciter4_X_test, reciter4_Y_train, reciter4_Y_test, reciter4),\n","      (not_exp_X_train, not_exp_X_test, not_exp_Y_train, not_exp_Y_test, not_exp)]:\n","\n","\n","      splitted_data_info_np = np.append(splitted_data_info_np, [[\n","              tajweed_rule,\n","              reciter_data.iloc[0]['recitor_en'],\n","              len(reciter_X_train),\n","              len(reciter_X_test),\n","              len(reciter_Y_train),\n","              len(reciter_Y_test)\n","              ]], axis=0)\n","\n","  # concatenate data\n","  # training data\n","  X_train = np.concatenate([reciter1_X_train, reciter2_X_train, reciter3_X_train, reciter4_X_train, not_exp_X_train], axis=0)\n","  Y_train = np.concatenate([reciter1_Y_train, reciter2_Y_train, reciter3_Y_train, reciter4_Y_train, not_exp_Y_train], axis=0)\n","\n","  # testing data\n","  X_test = np.concatenate([reciter1_X_test, reciter2_X_test, reciter3_X_test, reciter4_X_test, not_exp_X_test], axis=0)\n","  Y_test = np.concatenate([reciter1_Y_test, reciter2_Y_test, reciter3_Y_test, reciter4_Y_test, not_exp_Y_test], axis=0)\n","\n","  splitted_data_info_np = np.append(splitted_data_info_np, [[\n","          tajweed_rule,\n","          'all reciters',\n","          len(X_train),\n","          len(X_test),\n","          len(Y_train),\n","          len(Y_test)\n","          ]], axis=0)\n","\n","  # Normalize input data by scaling each sequence individually\n","  scaler = StandardScaler()\n","  X_train_scaled = np.array([scaler.fit_transform(seq) for seq in X_train])\n","  X_test_scaled = np.array([scaler.transform(seq) for seq in X_test])\n","\n","  # Define a simple neural network model\n","  input_shape = X_train_scaled[0].shape  # Shape of each mfcc sequence\n","  output_shape = Y_train.shape[1]  # Dimension of output (number of units in output layer)\n","\n","  input_layer = Input(shape=input_shape)\n","  flatten_layer = Flatten()(input_layer)  # Flatten the sequence to a 1D vector\n","  hidden_layer = Dense(64, activation='relu')(flatten_layer)\n","  output_layer = Dense(output_shape, activation='linear')(hidden_layer)  # Define the output layer with the correct units\n","\n","  model = Model(inputs=input_layer, outputs=output_layer)\n","\n","  # Compile the model\n","  model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n","\n","  # Train the model\n","  model.fit(X_train_scaled, Y_train, epochs=50, batch_size=32, validation_split=0.1)\n","\n","  #export the model\n","  model_filename = f'{tajweed_rule}_tajweed_rule_model'\n","  model_path = os.path.join(export_dir, model_filename)\n","  keras.models.save_model(model, model_path)\n","\n","  # Make predictions on test data\n","  predictions = model.predict(X_test_scaled)\n","\n","  # Evaluate the model with adjusted predictions\n","  predictions[predictions < 0] = -1\n","  predictions = np.round(predictions).astype('int32')\n","  loss, accuracy = model.evaluate(X_test_scaled, predictions)\n","\n","  print(f\"Test Loss: {loss:.4f}, Test accuracy : {accuracy:.4f}\")\n","  models_information_np = np.append(models_information_np, [[\n","          model_filename,\n","          \"{:.4f}\".format(loss),\n","          \"{:.4f}\".format(accuracy),\n","          \"{:.2f}\".format(accuracy*100),\n","          model_path]], axis=0)"],"metadata":{"id":"n3ycP8uz8zp8","executionInfo":{"status":"ok","timestamp":1717522137154,"user_tz":-60,"elapsed":8,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for rule in max_Y.keys():\n","  tajweed_rule_model(abdul_basit, yassin_aljazaery, ibrahim_aldosary, al_husary, safa_data, rule)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6qxa6hWLAeZ","outputId":"8d66846b-b768-4560-de0c-ee4e5b1d73a9","executionInfo":{"status":"ok","timestamp":1717524747801,"user_tz":-60,"elapsed":2610654,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","34/34 [==============================] - 4s 105ms/step - loss: 69.9991 - accuracy: 0.8562 - val_loss: 2.7399 - val_accuracy: 0.9832\n","Epoch 2/50\n","34/34 [==============================] - 3s 96ms/step - loss: 17.4064 - accuracy: 0.9570 - val_loss: 3.0071 - val_accuracy: 0.9580\n","Epoch 3/50\n","34/34 [==============================] - 5s 138ms/step - loss: 12.5249 - accuracy: 0.9869 - val_loss: 2.6437 - val_accuracy: 0.9664\n","Epoch 4/50\n","34/34 [==============================] - 4s 113ms/step - loss: 11.2033 - accuracy: 0.9860 - val_loss: 2.6011 - val_accuracy: 0.9664\n","Epoch 5/50\n","34/34 [==============================] - 3s 96ms/step - loss: 11.1099 - accuracy: 0.9888 - val_loss: 2.5854 - val_accuracy: 0.9664\n","Epoch 6/50\n","34/34 [==============================] - 3s 94ms/step - loss: 11.0280 - accuracy: 0.9907 - val_loss: 2.5873 - val_accuracy: 0.9664\n","Epoch 7/50\n","34/34 [==============================] - 4s 122ms/step - loss: 10.9636 - accuracy: 0.9925 - val_loss: 2.5772 - val_accuracy: 0.9664\n","Epoch 8/50\n","34/34 [==============================] - 4s 119ms/step - loss: 10.8868 - accuracy: 0.9897 - val_loss: 2.5500 - val_accuracy: 0.9664\n","Epoch 9/50\n","34/34 [==============================] - 3s 102ms/step - loss: 10.8200 - accuracy: 0.9925 - val_loss: 2.5290 - val_accuracy: 0.9664\n","Epoch 10/50\n","34/34 [==============================] - 3s 94ms/step - loss: 10.7500 - accuracy: 0.9916 - val_loss: 2.5082 - val_accuracy: 0.9664\n","Epoch 11/50\n","34/34 [==============================] - 4s 115ms/step - loss: 10.6799 - accuracy: 0.9925 - val_loss: 2.4842 - val_accuracy: 0.9664\n","Epoch 12/50\n","34/34 [==============================] - 4s 125ms/step - loss: 10.6137 - accuracy: 0.9925 - val_loss: 2.4697 - val_accuracy: 0.9664\n","Epoch 13/50\n","34/34 [==============================] - 3s 93ms/step - loss: 10.5527 - accuracy: 0.9925 - val_loss: 2.4514 - val_accuracy: 0.9664\n","Epoch 14/50\n","34/34 [==============================] - 3s 93ms/step - loss: 10.4838 - accuracy: 0.9925 - val_loss: 2.4453 - val_accuracy: 0.9664\n","Epoch 15/50\n","34/34 [==============================] - 3s 101ms/step - loss: 10.4229 - accuracy: 0.9907 - val_loss: 2.3941 - val_accuracy: 0.9664\n","Epoch 16/50\n","34/34 [==============================] - 7s 199ms/step - loss: 10.3945 - accuracy: 0.9907 - val_loss: 2.4253 - val_accuracy: 0.9664\n","Epoch 17/50\n","34/34 [==============================] - 5s 144ms/step - loss: 10.2915 - accuracy: 0.9925 - val_loss: 2.4299 - val_accuracy: 0.9664\n","Epoch 18/50\n","34/34 [==============================] - 3s 94ms/step - loss: 10.4118 - accuracy: 0.9907 - val_loss: 2.3065 - val_accuracy: 0.9664\n","Epoch 19/50\n","34/34 [==============================] - 4s 130ms/step - loss: 10.8685 - accuracy: 0.9813 - val_loss: 2.2667 - val_accuracy: 0.9748\n","Epoch 20/50\n","34/34 [==============================] - 4s 102ms/step - loss: 10.6275 - accuracy: 0.9916 - val_loss: 2.2496 - val_accuracy: 0.9748\n","Epoch 21/50\n","34/34 [==============================] - 3s 101ms/step - loss: 11.4925 - accuracy: 0.9907 - val_loss: 2.2330 - val_accuracy: 0.9748\n","Epoch 22/50\n","34/34 [==============================] - 3s 91ms/step - loss: 11.4817 - accuracy: 0.9907 - val_loss: 2.2191 - val_accuracy: 0.9748\n","Epoch 23/50\n","34/34 [==============================] - 4s 116ms/step - loss: 11.4707 - accuracy: 0.9907 - val_loss: 2.2046 - val_accuracy: 0.9748\n","Epoch 24/50\n","34/34 [==============================] - 4s 121ms/step - loss: 11.4599 - accuracy: 0.9907 - val_loss: 2.1908 - val_accuracy: 0.9748\n","Epoch 25/50\n","34/34 [==============================] - 3s 91ms/step - loss: 11.4496 - accuracy: 0.9907 - val_loss: 2.1768 - val_accuracy: 0.9748\n","Epoch 26/50\n","34/34 [==============================] - 3s 92ms/step - loss: 11.4400 - accuracy: 0.9907 - val_loss: 2.1617 - val_accuracy: 0.9748\n","Epoch 27/50\n","34/34 [==============================] - 3s 95ms/step - loss: 11.4297 - accuracy: 0.9907 - val_loss: 2.1491 - val_accuracy: 0.9748\n","Epoch 28/50\n","34/34 [==============================] - 5s 135ms/step - loss: 11.4204 - accuracy: 0.9907 - val_loss: 2.1375 - val_accuracy: 0.9748\n","Epoch 29/50\n","34/34 [==============================] - 3s 93ms/step - loss: 11.4117 - accuracy: 0.9907 - val_loss: 2.1236 - val_accuracy: 0.9748\n","Epoch 30/50\n","34/34 [==============================] - 3s 88ms/step - loss: 11.4028 - accuracy: 0.9907 - val_loss: 2.1115 - val_accuracy: 0.9748\n","Epoch 31/50\n","34/34 [==============================] - 3s 89ms/step - loss: 11.3948 - accuracy: 0.9907 - val_loss: 2.0992 - val_accuracy: 0.9748\n","Epoch 32/50\n","34/34 [==============================] - 4s 110ms/step - loss: 11.3865 - accuracy: 0.9907 - val_loss: 2.0885 - val_accuracy: 0.9748\n","Epoch 33/50\n","34/34 [==============================] - 4s 116ms/step - loss: 11.3790 - accuracy: 0.9907 - val_loss: 2.0772 - val_accuracy: 0.9748\n","Epoch 34/50\n","34/34 [==============================] - 3s 90ms/step - loss: 11.3721 - accuracy: 0.9907 - val_loss: 2.0683 - val_accuracy: 0.9748\n","Epoch 35/50\n","34/34 [==============================] - 3s 92ms/step - loss: 11.3650 - accuracy: 0.9907 - val_loss: 2.0563 - val_accuracy: 0.9748\n","Epoch 36/50\n","34/34 [==============================] - 3s 92ms/step - loss: 11.3581 - accuracy: 0.9907 - val_loss: 2.0461 - val_accuracy: 0.9748\n","Epoch 37/50\n","34/34 [==============================] - 4s 124ms/step - loss: 11.3516 - accuracy: 0.9907 - val_loss: 2.0365 - val_accuracy: 0.9748\n","Epoch 38/50\n","34/34 [==============================] - 3s 93ms/step - loss: 11.3459 - accuracy: 0.9907 - val_loss: 2.0304 - val_accuracy: 0.9748\n","Epoch 39/50\n","34/34 [==============================] - 3s 87ms/step - loss: 11.3407 - accuracy: 0.9907 - val_loss: 2.0205 - val_accuracy: 0.9748\n","Epoch 40/50\n","34/34 [==============================] - 3s 87ms/step - loss: 11.3352 - accuracy: 0.9907 - val_loss: 2.0114 - val_accuracy: 0.9748\n","Epoch 41/50\n","34/34 [==============================] - 4s 108ms/step - loss: 11.3302 - accuracy: 0.9907 - val_loss: 2.0060 - val_accuracy: 0.9748\n","Epoch 42/50\n","34/34 [==============================] - 4s 115ms/step - loss: 11.3255 - accuracy: 0.9907 - val_loss: 1.9980 - val_accuracy: 0.9748\n","Epoch 43/50\n","34/34 [==============================] - 3s 86ms/step - loss: 11.3206 - accuracy: 0.9907 - val_loss: 1.9892 - val_accuracy: 0.9748\n","Epoch 44/50\n","34/34 [==============================] - 3s 87ms/step - loss: 11.3159 - accuracy: 0.9907 - val_loss: 1.9844 - val_accuracy: 0.9748\n","Epoch 45/50\n","34/34 [==============================] - 3s 88ms/step - loss: 11.3122 - accuracy: 0.9907 - val_loss: 1.9724 - val_accuracy: 0.9748\n","Epoch 46/50\n","34/34 [==============================] - 4s 113ms/step - loss: 11.3074 - accuracy: 0.9916 - val_loss: 1.9656 - val_accuracy: 0.9748\n","Epoch 47/50\n","34/34 [==============================] - 4s 108ms/step - loss: 11.3043 - accuracy: 0.9916 - val_loss: 1.9607 - val_accuracy: 0.9748\n","Epoch 48/50\n","34/34 [==============================] - 3s 94ms/step - loss: 11.3007 - accuracy: 0.9916 - val_loss: 1.9553 - val_accuracy: 0.9748\n","Epoch 49/50\n","34/34 [==============================] - 3s 89ms/step - loss: 11.2974 - accuracy: 0.9916 - val_loss: 1.9488 - val_accuracy: 0.9748\n","Epoch 50/50\n","34/34 [==============================] - 3s 89ms/step - loss: 11.2940 - accuracy: 0.9916 - val_loss: 1.9442 - val_accuracy: 0.9748\n","10/10 [==============================] - 0s 16ms/step\n","10/10 [==============================] - 0s 26ms/step - loss: 180.2376 - accuracy: 1.0000\n","Test Loss: 180.2376, Test accuracy : 1.0000\n","Epoch 1/50\n","34/34 [==============================] - 6s 150ms/step - loss: 177.0952 - accuracy: 0.5948 - val_loss: 100.3174 - val_accuracy: 0.9076\n","Epoch 2/50\n","34/34 [==============================] - 4s 109ms/step - loss: 109.2887 - accuracy: 0.9225 - val_loss: 109.5249 - val_accuracy: 0.9496\n","Epoch 3/50\n","34/34 [==============================] - 4s 108ms/step - loss: 81.3007 - accuracy: 0.8889 - val_loss: 130.9542 - val_accuracy: 0.8655\n","Epoch 4/50\n","34/34 [==============================] - 5s 140ms/step - loss: 65.4354 - accuracy: 0.9337 - val_loss: 105.6258 - val_accuracy: 0.9496\n","Epoch 5/50\n","34/34 [==============================] - 4s 123ms/step - loss: 56.5279 - accuracy: 0.9468 - val_loss: 103.8661 - val_accuracy: 0.9496\n","Epoch 6/50\n","34/34 [==============================] - 4s 112ms/step - loss: 47.3570 - accuracy: 0.9188 - val_loss: 110.6510 - val_accuracy: 0.9496\n","Epoch 7/50\n","34/34 [==============================] - 3s 101ms/step - loss: 39.7343 - accuracy: 0.9253 - val_loss: 108.2329 - val_accuracy: 0.9580\n","Epoch 8/50\n","34/34 [==============================] - 5s 151ms/step - loss: 36.4620 - accuracy: 0.9234 - val_loss: 116.5566 - val_accuracy: 0.9412\n","Epoch 9/50\n","34/34 [==============================] - 3s 101ms/step - loss: 36.2371 - accuracy: 0.9066 - val_loss: 106.9508 - val_accuracy: 0.9496\n","Epoch 10/50\n","34/34 [==============================] - 3s 98ms/step - loss: 29.9778 - accuracy: 0.8926 - val_loss: 111.1470 - val_accuracy: 0.9412\n","Epoch 11/50\n","34/34 [==============================] - 3s 101ms/step - loss: 26.6790 - accuracy: 0.9057 - val_loss: 110.4016 - val_accuracy: 0.9496\n","Epoch 12/50\n","34/34 [==============================] - 5s 154ms/step - loss: 24.3364 - accuracy: 0.8693 - val_loss: 110.6137 - val_accuracy: 0.9412\n","Epoch 13/50\n","34/34 [==============================] - 4s 102ms/step - loss: 23.6807 - accuracy: 0.9281 - val_loss: 114.9505 - val_accuracy: 0.9496\n","Epoch 14/50\n","34/34 [==============================] - 3s 101ms/step - loss: 20.6582 - accuracy: 0.8768 - val_loss: 115.2982 - val_accuracy: 0.9412\n","Epoch 15/50\n","34/34 [==============================] - 4s 105ms/step - loss: 18.0733 - accuracy: 0.8889 - val_loss: 114.9531 - val_accuracy: 0.9412\n","Epoch 16/50\n","34/34 [==============================] - 5s 152ms/step - loss: 17.9428 - accuracy: 0.8637 - val_loss: 122.3273 - val_accuracy: 0.9412\n","Epoch 17/50\n","34/34 [==============================] - 4s 108ms/step - loss: 18.8281 - accuracy: 0.8936 - val_loss: 111.4059 - val_accuracy: 0.9412\n","Epoch 18/50\n","34/34 [==============================] - 4s 105ms/step - loss: 16.3329 - accuracy: 0.8889 - val_loss: 113.6729 - val_accuracy: 0.9580\n","Epoch 19/50\n","34/34 [==============================] - 4s 121ms/step - loss: 16.6703 - accuracy: 0.8805 - val_loss: 110.1943 - val_accuracy: 0.9496\n","Epoch 20/50\n","34/34 [==============================] - 5s 148ms/step - loss: 15.7323 - accuracy: 0.8571 - val_loss: 114.1212 - val_accuracy: 0.9244\n","Epoch 21/50\n","34/34 [==============================] - 3s 98ms/step - loss: 15.1442 - accuracy: 0.8627 - val_loss: 108.7946 - val_accuracy: 0.9412\n","Epoch 22/50\n","34/34 [==============================] - 4s 111ms/step - loss: 16.6276 - accuracy: 0.9010 - val_loss: 115.8778 - val_accuracy: 0.9412\n","Epoch 23/50\n","34/34 [==============================] - 9s 257ms/step - loss: 15.2337 - accuracy: 0.8852 - val_loss: 114.5789 - val_accuracy: 0.9328\n","Epoch 24/50\n","34/34 [==============================] - 5s 128ms/step - loss: 13.1904 - accuracy: 0.8749 - val_loss: 109.0164 - val_accuracy: 0.9160\n","Epoch 25/50\n","34/34 [==============================] - 4s 108ms/step - loss: 12.5282 - accuracy: 0.8431 - val_loss: 126.3001 - val_accuracy: 0.9412\n","Epoch 26/50\n","34/34 [==============================] - 4s 106ms/step - loss: 12.5425 - accuracy: 0.8525 - val_loss: 109.9244 - val_accuracy: 0.9328\n","Epoch 27/50\n","34/34 [==============================] - 5s 159ms/step - loss: 12.4353 - accuracy: 0.8553 - val_loss: 120.3194 - val_accuracy: 0.9328\n","Epoch 28/50\n","34/34 [==============================] - 3s 103ms/step - loss: 13.0608 - accuracy: 0.8506 - val_loss: 107.4761 - val_accuracy: 0.8992\n","Epoch 29/50\n","34/34 [==============================] - 4s 104ms/step - loss: 13.1285 - accuracy: 0.8665 - val_loss: 118.7421 - val_accuracy: 0.9496\n","Epoch 30/50\n","34/34 [==============================] - 4s 109ms/step - loss: 12.6608 - accuracy: 0.8301 - val_loss: 111.9070 - val_accuracy: 0.9412\n","Epoch 31/50\n","34/34 [==============================] - 5s 145ms/step - loss: 11.1749 - accuracy: 0.8543 - val_loss: 109.3836 - val_accuracy: 0.9412\n","Epoch 32/50\n","34/34 [==============================] - 4s 103ms/step - loss: 10.6432 - accuracy: 0.8599 - val_loss: 109.4722 - val_accuracy: 0.9412\n","Epoch 33/50\n","34/34 [==============================] - 4s 105ms/step - loss: 10.8151 - accuracy: 0.8674 - val_loss: 113.6594 - val_accuracy: 0.9244\n","Epoch 34/50\n","34/34 [==============================] - 4s 123ms/step - loss: 10.6629 - accuracy: 0.8749 - val_loss: 117.9702 - val_accuracy: 0.9244\n","Epoch 35/50\n","34/34 [==============================] - 5s 142ms/step - loss: 10.3166 - accuracy: 0.8609 - val_loss: 109.6116 - val_accuracy: 0.9328\n","Epoch 36/50\n","34/34 [==============================] - 4s 106ms/step - loss: 9.9233 - accuracy: 0.8768 - val_loss: 109.3841 - val_accuracy: 0.9244\n","Epoch 37/50\n","34/34 [==============================] - 4s 104ms/step - loss: 9.3205 - accuracy: 0.8777 - val_loss: 113.3440 - val_accuracy: 0.9328\n","Epoch 38/50\n","34/34 [==============================] - 4s 132ms/step - loss: 9.2853 - accuracy: 0.8609 - val_loss: 110.9572 - val_accuracy: 0.8992\n","Epoch 39/50\n","34/34 [==============================] - 5s 137ms/step - loss: 9.4378 - accuracy: 0.8599 - val_loss: 110.9816 - val_accuracy: 0.9244\n","Epoch 40/50\n","34/34 [==============================] - 4s 106ms/step - loss: 9.1438 - accuracy: 0.8394 - val_loss: 109.7518 - val_accuracy: 0.9328\n","Epoch 41/50\n","34/34 [==============================] - 4s 105ms/step - loss: 8.9565 - accuracy: 0.8366 - val_loss: 110.7355 - val_accuracy: 0.9244\n","Epoch 42/50\n","34/34 [==============================] - 5s 138ms/step - loss: 9.0778 - accuracy: 0.8739 - val_loss: 110.6959 - val_accuracy: 0.9412\n","Epoch 43/50\n","34/34 [==============================] - 4s 126ms/step - loss: 9.3294 - accuracy: 0.8627 - val_loss: 110.7684 - val_accuracy: 0.9328\n","Epoch 44/50\n","34/34 [==============================] - 3s 101ms/step - loss: 9.5424 - accuracy: 0.8655 - val_loss: 111.5087 - val_accuracy: 0.9328\n","Epoch 45/50\n","34/34 [==============================] - 4s 105ms/step - loss: 9.6217 - accuracy: 0.8422 - val_loss: 107.0912 - val_accuracy: 0.9076\n","Epoch 46/50\n","34/34 [==============================] - 5s 146ms/step - loss: 11.0893 - accuracy: 0.8599 - val_loss: 112.7304 - val_accuracy: 0.9496\n","Epoch 47/50\n","34/34 [==============================] - 4s 118ms/step - loss: 11.4469 - accuracy: 0.8515 - val_loss: 108.9878 - val_accuracy: 0.9496\n","Epoch 48/50\n","34/34 [==============================] - 4s 104ms/step - loss: 11.6863 - accuracy: 0.9206 - val_loss: 116.8972 - val_accuracy: 0.9328\n","Epoch 49/50\n","34/34 [==============================] - 4s 106ms/step - loss: 12.0562 - accuracy: 0.8898 - val_loss: 116.0904 - val_accuracy: 0.9244\n","Epoch 50/50\n","34/34 [==============================] - 5s 151ms/step - loss: 10.9699 - accuracy: 0.9141 - val_loss: 120.1797 - val_accuracy: 0.9412\n","10/10 [==============================] - 0s 20ms/step\n","10/10 [==============================] - 0s 27ms/step - loss: 259.7652 - accuracy: 0.9933\n","Test Loss: 259.7652, Test accuracy : 0.9933\n","Epoch 1/50\n","34/34 [==============================] - 5s 125ms/step - loss: 69.4914 - accuracy: 0.3184 - val_loss: 16.8099 - val_accuracy: 0.5798\n","Epoch 2/50\n","34/34 [==============================] - 4s 117ms/step - loss: 36.8325 - accuracy: 0.7246 - val_loss: 16.4075 - val_accuracy: 0.6891\n","Epoch 3/50\n","34/34 [==============================] - 3s 92ms/step - loss: 28.9487 - accuracy: 0.7722 - val_loss: 16.5055 - val_accuracy: 0.6891\n","Epoch 4/50\n","34/34 [==============================] - 3s 96ms/step - loss: 24.7767 - accuracy: 0.7535 - val_loss: 16.6288 - val_accuracy: 0.6891\n","Epoch 5/50\n","34/34 [==============================] - 4s 113ms/step - loss: 22.3068 - accuracy: 0.7899 - val_loss: 16.3838 - val_accuracy: 0.6807\n","Epoch 6/50\n","34/34 [==============================] - 4s 127ms/step - loss: 20.0752 - accuracy: 0.7731 - val_loss: 17.4636 - val_accuracy: 0.6891\n","Epoch 7/50\n","34/34 [==============================] - 3s 96ms/step - loss: 16.7496 - accuracy: 0.7946 - val_loss: 17.3954 - val_accuracy: 0.6471\n","Epoch 8/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.6690 - accuracy: 0.7974 - val_loss: 18.0205 - val_accuracy: 0.6387\n","Epoch 9/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.3436 - accuracy: 0.7834 - val_loss: 16.2749 - val_accuracy: 0.6975\n","Epoch 10/50\n","34/34 [==============================] - 4s 117ms/step - loss: 15.1127 - accuracy: 0.6433 - val_loss: 15.8282 - val_accuracy: 0.1849\n","Epoch 11/50\n","34/34 [==============================] - 4s 111ms/step - loss: 12.7491 - accuracy: 0.1466 - val_loss: 16.7464 - val_accuracy: 0.2269\n","Epoch 12/50\n","34/34 [==============================] - 3s 90ms/step - loss: 11.8279 - accuracy: 0.4855 - val_loss: 17.0151 - val_accuracy: 0.2269\n","Epoch 13/50\n","34/34 [==============================] - 3s 92ms/step - loss: 9.3785 - accuracy: 0.1307 - val_loss: 16.1358 - val_accuracy: 0.2101\n","Epoch 14/50\n","34/34 [==============================] - 4s 107ms/step - loss: 8.4067 - accuracy: 0.1186 - val_loss: 16.5674 - val_accuracy: 0.2101\n","Epoch 15/50\n","34/34 [==============================] - 4s 124ms/step - loss: 7.6944 - accuracy: 0.1204 - val_loss: 16.1495 - val_accuracy: 0.2269\n","Epoch 16/50\n","34/34 [==============================] - 3s 88ms/step - loss: 7.1986 - accuracy: 0.1074 - val_loss: 17.0865 - val_accuracy: 0.7311\n","Epoch 17/50\n","34/34 [==============================] - 3s 92ms/step - loss: 6.3045 - accuracy: 0.8469 - val_loss: 16.3195 - val_accuracy: 0.7647\n","Epoch 18/50\n","34/34 [==============================] - 3s 89ms/step - loss: 6.0142 - accuracy: 0.8515 - val_loss: 16.7403 - val_accuracy: 0.7563\n","Epoch 19/50\n","34/34 [==============================] - 4s 116ms/step - loss: 6.4038 - accuracy: 0.8338 - val_loss: 18.2775 - val_accuracy: 0.7227\n","Epoch 20/50\n","34/34 [==============================] - 4s 111ms/step - loss: 6.1908 - accuracy: 0.8385 - val_loss: 16.2861 - val_accuracy: 0.7647\n","Epoch 21/50\n","34/34 [==============================] - 3s 90ms/step - loss: 4.8369 - accuracy: 0.8422 - val_loss: 16.0932 - val_accuracy: 0.7815\n","Epoch 22/50\n","34/34 [==============================] - 3s 90ms/step - loss: 4.4047 - accuracy: 0.8553 - val_loss: 16.4991 - val_accuracy: 0.7647\n","Epoch 23/50\n","34/34 [==============================] - 3s 92ms/step - loss: 4.3684 - accuracy: 0.8469 - val_loss: 17.9660 - val_accuracy: 0.7143\n","Epoch 24/50\n","34/34 [==============================] - 4s 129ms/step - loss: 4.6939 - accuracy: 0.8441 - val_loss: 16.2835 - val_accuracy: 0.7563\n","Epoch 25/50\n","34/34 [==============================] - 3s 99ms/step - loss: 4.5859 - accuracy: 0.8525 - val_loss: 16.7397 - val_accuracy: 0.7227\n","Epoch 26/50\n","34/34 [==============================] - 3s 95ms/step - loss: 3.6723 - accuracy: 0.8525 - val_loss: 16.1622 - val_accuracy: 0.7563\n","Epoch 27/50\n","34/34 [==============================] - 3s 93ms/step - loss: 3.2579 - accuracy: 0.8571 - val_loss: 17.0201 - val_accuracy: 0.7395\n","Epoch 28/50\n","34/34 [==============================] - 4s 111ms/step - loss: 3.4952 - accuracy: 0.8609 - val_loss: 16.0953 - val_accuracy: 0.7731\n","Epoch 29/50\n","34/34 [==============================] - 4s 118ms/step - loss: 3.6686 - accuracy: 0.9440 - val_loss: 16.5500 - val_accuracy: 0.6975\n","Epoch 30/50\n","34/34 [==============================] - 3s 91ms/step - loss: 3.6686 - accuracy: 0.9384 - val_loss: 16.3439 - val_accuracy: 0.7143\n","Epoch 31/50\n","34/34 [==============================] - 3s 89ms/step - loss: 3.4920 - accuracy: 0.8739 - val_loss: 16.4078 - val_accuracy: 0.7395\n","Epoch 32/50\n","34/34 [==============================] - 3s 89ms/step - loss: 3.0500 - accuracy: 0.9421 - val_loss: 16.5272 - val_accuracy: 0.7227\n","Epoch 33/50\n","34/34 [==============================] - 4s 120ms/step - loss: 2.9574 - accuracy: 0.9514 - val_loss: 16.9895 - val_accuracy: 0.7143\n","Epoch 34/50\n","34/34 [==============================] - 4s 108ms/step - loss: 2.9996 - accuracy: 0.9552 - val_loss: 16.3482 - val_accuracy: 0.7479\n","Epoch 35/50\n","34/34 [==============================] - 3s 90ms/step - loss: 3.5672 - accuracy: 0.9421 - val_loss: 16.6046 - val_accuracy: 0.7227\n","Epoch 36/50\n","34/34 [==============================] - 3s 92ms/step - loss: 4.2021 - accuracy: 0.9412 - val_loss: 16.3060 - val_accuracy: 0.7647\n","Epoch 37/50\n","34/34 [==============================] - 3s 98ms/step - loss: 5.3623 - accuracy: 0.9468 - val_loss: 16.1418 - val_accuracy: 0.7563\n","Epoch 38/50\n","34/34 [==============================] - 4s 124ms/step - loss: 5.9282 - accuracy: 0.9533 - val_loss: 16.9205 - val_accuracy: 0.7227\n","Epoch 39/50\n","34/34 [==============================] - 3s 101ms/step - loss: 4.2385 - accuracy: 0.9486 - val_loss: 16.5230 - val_accuracy: 0.7311\n","Epoch 40/50\n","34/34 [==============================] - 3s 93ms/step - loss: 3.7311 - accuracy: 0.9505 - val_loss: 16.8058 - val_accuracy: 0.7227\n","Epoch 41/50\n","34/34 [==============================] - 3s 90ms/step - loss: 3.5875 - accuracy: 0.9561 - val_loss: 16.3076 - val_accuracy: 0.7647\n","Epoch 42/50\n","34/34 [==============================] - 4s 121ms/step - loss: 3.5287 - accuracy: 0.9505 - val_loss: 16.5630 - val_accuracy: 0.7311\n","Epoch 43/50\n","34/34 [==============================] - 4s 110ms/step - loss: 3.0932 - accuracy: 0.8936 - val_loss: 16.5376 - val_accuracy: 0.7647\n","Epoch 44/50\n","34/34 [==============================] - 3s 89ms/step - loss: 2.8899 - accuracy: 0.8609 - val_loss: 16.7796 - val_accuracy: 0.7479\n","Epoch 45/50\n","34/34 [==============================] - 3s 91ms/step - loss: 3.2580 - accuracy: 0.9216 - val_loss: 16.4995 - val_accuracy: 0.7395\n","Epoch 46/50\n","34/34 [==============================] - 4s 103ms/step - loss: 2.9542 - accuracy: 0.9608 - val_loss: 16.4262 - val_accuracy: 0.7563\n","Epoch 47/50\n","34/34 [==============================] - 5s 135ms/step - loss: 3.2054 - accuracy: 0.9076 - val_loss: 17.1205 - val_accuracy: 0.7311\n","Epoch 48/50\n","34/34 [==============================] - 3s 92ms/step - loss: 3.3421 - accuracy: 0.9533 - val_loss: 16.4985 - val_accuracy: 0.7647\n","Epoch 49/50\n","34/34 [==============================] - 3s 93ms/step - loss: 4.5090 - accuracy: 0.9617 - val_loss: 17.4395 - val_accuracy: 0.7311\n","Epoch 50/50\n","34/34 [==============================] - 3s 90ms/step - loss: 4.0274 - accuracy: 0.9346 - val_loss: 16.4987 - val_accuracy: 0.7731\n","10/10 [==============================] - 0s 17ms/step\n","10/10 [==============================] - 0s 19ms/step - loss: 279.0264 - accuracy: 0.9500\n","Test Loss: 279.0264, Test accuracy : 0.9500\n","Epoch 1/50\n","34/34 [==============================] - 4s 97ms/step - loss: 124.0645 - accuracy: 0.1905 - val_loss: 64.0886 - val_accuracy: 0.9748\n","Epoch 2/50\n","34/34 [==============================] - 3s 94ms/step - loss: 61.6470 - accuracy: 0.9393 - val_loss: 64.4033 - val_accuracy: 0.8655\n","Epoch 3/50\n","34/34 [==============================] - 3s 102ms/step - loss: 56.2904 - accuracy: 0.9197 - val_loss: 62.0067 - val_accuracy: 0.9664\n","Epoch 4/50\n","34/34 [==============================] - 4s 121ms/step - loss: 51.5933 - accuracy: 0.9318 - val_loss: 63.3377 - val_accuracy: 0.9496\n","Epoch 5/50\n","34/34 [==============================] - 3s 92ms/step - loss: 49.4890 - accuracy: 0.9542 - val_loss: 60.6277 - val_accuracy: 0.9664\n","Epoch 6/50\n","34/34 [==============================] - 3s 92ms/step - loss: 46.7487 - accuracy: 0.9384 - val_loss: 60.9868 - val_accuracy: 0.9664\n","Epoch 7/50\n","34/34 [==============================] - 3s 93ms/step - loss: 38.2365 - accuracy: 0.9533 - val_loss: 60.3637 - val_accuracy: 0.9580\n","Epoch 8/50\n","34/34 [==============================] - 4s 124ms/step - loss: 33.0582 - accuracy: 0.9617 - val_loss: 60.0665 - val_accuracy: 0.9664\n","Epoch 9/50\n","34/34 [==============================] - 4s 105ms/step - loss: 29.5849 - accuracy: 0.9589 - val_loss: 59.3116 - val_accuracy: 0.9664\n","Epoch 10/50\n","34/34 [==============================] - 3s 93ms/step - loss: 27.9105 - accuracy: 0.9552 - val_loss: 59.3330 - val_accuracy: 0.9664\n","Epoch 11/50\n","34/34 [==============================] - 3s 91ms/step - loss: 26.0470 - accuracy: 0.9533 - val_loss: 59.9708 - val_accuracy: 0.9664\n","Epoch 12/50\n","34/34 [==============================] - 4s 113ms/step - loss: 25.0712 - accuracy: 0.9486 - val_loss: 60.6118 - val_accuracy: 0.9580\n","Epoch 13/50\n","34/34 [==============================] - 4s 120ms/step - loss: 24.6553 - accuracy: 0.9524 - val_loss: 59.6974 - val_accuracy: 0.9664\n","Epoch 14/50\n","34/34 [==============================] - 3s 92ms/step - loss: 22.3846 - accuracy: 0.9589 - val_loss: 58.9597 - val_accuracy: 0.9580\n","Epoch 15/50\n","34/34 [==============================] - 3s 94ms/step - loss: 20.3247 - accuracy: 0.9617 - val_loss: 58.3727 - val_accuracy: 0.9664\n","Epoch 16/50\n","34/34 [==============================] - 4s 109ms/step - loss: 18.4780 - accuracy: 0.9608 - val_loss: 58.6483 - val_accuracy: 0.9664\n","Epoch 17/50\n","34/34 [==============================] - 7s 194ms/step - loss: 18.0793 - accuracy: 0.9627 - val_loss: 59.9318 - val_accuracy: 0.9664\n","Epoch 18/50\n","34/34 [==============================] - 4s 109ms/step - loss: 17.7876 - accuracy: 0.9664 - val_loss: 58.4034 - val_accuracy: 0.9664\n","Epoch 19/50\n","34/34 [==============================] - 3s 94ms/step - loss: 17.8643 - accuracy: 0.9673 - val_loss: 60.5959 - val_accuracy: 0.9664\n","Epoch 20/50\n","34/34 [==============================] - 3s 94ms/step - loss: 18.0036 - accuracy: 0.9627 - val_loss: 59.4515 - val_accuracy: 0.9664\n","Epoch 21/50\n","34/34 [==============================] - 4s 121ms/step - loss: 16.6839 - accuracy: 0.9673 - val_loss: 59.4418 - val_accuracy: 0.9664\n","Epoch 22/50\n","34/34 [==============================] - 4s 110ms/step - loss: 16.5416 - accuracy: 0.9655 - val_loss: 60.0348 - val_accuracy: 0.9580\n","Epoch 23/50\n","34/34 [==============================] - 3s 91ms/step - loss: 17.0142 - accuracy: 0.9664 - val_loss: 58.3111 - val_accuracy: 0.9664\n","Epoch 24/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.0996 - accuracy: 0.9664 - val_loss: 59.4821 - val_accuracy: 0.9664\n","Epoch 25/50\n","34/34 [==============================] - 3s 100ms/step - loss: 17.7387 - accuracy: 0.9655 - val_loss: 56.2502 - val_accuracy: 0.9664\n","Epoch 26/50\n","34/34 [==============================] - 4s 125ms/step - loss: 18.0653 - accuracy: 0.9655 - val_loss: 61.0214 - val_accuracy: 0.9664\n","Epoch 27/50\n","34/34 [==============================] - 3s 93ms/step - loss: 19.1381 - accuracy: 0.9645 - val_loss: 58.6462 - val_accuracy: 0.9664\n","Epoch 28/50\n","34/34 [==============================] - 3s 89ms/step - loss: 20.1580 - accuracy: 0.9645 - val_loss: 59.6406 - val_accuracy: 0.9748\n","Epoch 29/50\n","34/34 [==============================] - 3s 93ms/step - loss: 18.2108 - accuracy: 0.9673 - val_loss: 57.8760 - val_accuracy: 0.9748\n","Epoch 30/50\n","34/34 [==============================] - 4s 110ms/step - loss: 14.9019 - accuracy: 0.9673 - val_loss: 58.3308 - val_accuracy: 0.9748\n","Epoch 31/50\n","34/34 [==============================] - 4s 115ms/step - loss: 14.1341 - accuracy: 0.9664 - val_loss: 58.1566 - val_accuracy: 0.9748\n","Epoch 32/50\n","34/34 [==============================] - 3s 94ms/step - loss: 13.9881 - accuracy: 0.9673 - val_loss: 58.4346 - val_accuracy: 0.9580\n","Epoch 33/50\n","34/34 [==============================] - 3s 92ms/step - loss: 13.9529 - accuracy: 0.9664 - val_loss: 58.3277 - val_accuracy: 0.9664\n","Epoch 34/50\n","34/34 [==============================] - 3s 97ms/step - loss: 13.7442 - accuracy: 0.9673 - val_loss: 58.0739 - val_accuracy: 0.9664\n","Epoch 35/50\n","34/34 [==============================] - 4s 127ms/step - loss: 14.4128 - accuracy: 0.9664 - val_loss: 58.3964 - val_accuracy: 0.9664\n","Epoch 36/50\n","34/34 [==============================] - 3s 95ms/step - loss: 17.8041 - accuracy: 0.9627 - val_loss: 57.3540 - val_accuracy: 0.9664\n","Epoch 37/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.7758 - accuracy: 0.9673 - val_loss: 57.4190 - val_accuracy: 0.9664\n","Epoch 38/50\n","34/34 [==============================] - 3s 92ms/step - loss: 14.4874 - accuracy: 0.9664 - val_loss: 57.0529 - val_accuracy: 0.9664\n","Epoch 39/50\n","34/34 [==============================] - 4s 115ms/step - loss: 13.9247 - accuracy: 0.9673 - val_loss: 56.9309 - val_accuracy: 0.9664\n","Epoch 40/50\n","34/34 [==============================] - 4s 111ms/step - loss: 13.6839 - accuracy: 0.9664 - val_loss: 56.9752 - val_accuracy: 0.9664\n","Epoch 41/50\n","34/34 [==============================] - 3s 92ms/step - loss: 13.4654 - accuracy: 0.9664 - val_loss: 56.4022 - val_accuracy: 0.9664\n","Epoch 42/50\n","34/34 [==============================] - 3s 92ms/step - loss: 13.7388 - accuracy: 0.9664 - val_loss: 56.5300 - val_accuracy: 0.9664\n","Epoch 43/50\n","34/34 [==============================] - 3s 98ms/step - loss: 13.7040 - accuracy: 0.9664 - val_loss: 56.4436 - val_accuracy: 0.9664\n","Epoch 44/50\n","34/34 [==============================] - 4s 124ms/step - loss: 13.3869 - accuracy: 0.9655 - val_loss: 56.1867 - val_accuracy: 0.9664\n","Epoch 45/50\n","34/34 [==============================] - 3s 100ms/step - loss: 13.3767 - accuracy: 0.9655 - val_loss: 55.8774 - val_accuracy: 0.9664\n","Epoch 46/50\n","34/34 [==============================] - 3s 86ms/step - loss: 14.0851 - accuracy: 0.9655 - val_loss: 56.2432 - val_accuracy: 0.9664\n","Epoch 47/50\n","34/34 [==============================] - 3s 92ms/step - loss: 13.2361 - accuracy: 0.9655 - val_loss: 55.7191 - val_accuracy: 0.9664\n","Epoch 48/50\n","34/34 [==============================] - 4s 112ms/step - loss: 12.6109 - accuracy: 0.9673 - val_loss: 55.6318 - val_accuracy: 0.9664\n","Epoch 49/50\n","34/34 [==============================] - 4s 117ms/step - loss: 12.3513 - accuracy: 0.9673 - val_loss: 55.7796 - val_accuracy: 0.9664\n","Epoch 50/50\n","34/34 [==============================] - 3s 90ms/step - loss: 12.0072 - accuracy: 0.9673 - val_loss: 55.9498 - val_accuracy: 0.9664\n","10/10 [==============================] - 0s 23ms/step\n","10/10 [==============================] - 0s 24ms/step - loss: 141.4090 - accuracy: 0.9967\n","Test Loss: 141.4090, Test accuracy : 0.9967\n","Epoch 1/50\n","34/34 [==============================] - 5s 118ms/step - loss: 74.2689 - accuracy: 0.2007 - val_loss: 56.6128 - val_accuracy: 0.1345\n","Epoch 2/50\n","34/34 [==============================] - 4s 125ms/step - loss: 56.3248 - accuracy: 0.2474 - val_loss: 56.1636 - val_accuracy: 0.1092\n","Epoch 3/50\n","34/34 [==============================] - 3s 91ms/step - loss: 53.0042 - accuracy: 0.2521 - val_loss: 57.9184 - val_accuracy: 0.5378\n","Epoch 4/50\n","34/34 [==============================] - 3s 89ms/step - loss: 45.1012 - accuracy: 0.4790 - val_loss: 54.3831 - val_accuracy: 0.4286\n","Epoch 5/50\n","34/34 [==============================] - 3s 86ms/step - loss: 41.1076 - accuracy: 0.5247 - val_loss: 52.7470 - val_accuracy: 0.4706\n","Epoch 6/50\n","34/34 [==============================] - 4s 120ms/step - loss: 36.7702 - accuracy: 0.5275 - val_loss: 52.2408 - val_accuracy: 0.4202\n","Epoch 7/50\n","34/34 [==============================] - 4s 108ms/step - loss: 34.2625 - accuracy: 0.5434 - val_loss: 53.5164 - val_accuracy: 0.5462\n","Epoch 8/50\n","34/34 [==============================] - 3s 93ms/step - loss: 31.6901 - accuracy: 0.5322 - val_loss: 51.4069 - val_accuracy: 0.5294\n","Epoch 9/50\n","34/34 [==============================] - 3s 92ms/step - loss: 30.4042 - accuracy: 0.5462 - val_loss: 51.1177 - val_accuracy: 0.5462\n","Epoch 10/50\n","34/34 [==============================] - 4s 106ms/step - loss: 28.8315 - accuracy: 0.5826 - val_loss: 52.5255 - val_accuracy: 0.4874\n","Epoch 11/50\n","34/34 [==============================] - 4s 130ms/step - loss: 28.2194 - accuracy: 0.5714 - val_loss: 51.1701 - val_accuracy: 0.5966\n","Epoch 12/50\n","34/34 [==============================] - 3s 91ms/step - loss: 28.9117 - accuracy: 0.6013 - val_loss: 64.0692 - val_accuracy: 0.6134\n","Epoch 13/50\n","34/34 [==============================] - 3s 90ms/step - loss: 29.9878 - accuracy: 0.5938 - val_loss: 53.9116 - val_accuracy: 0.5546\n","Epoch 14/50\n","34/34 [==============================] - 3s 90ms/step - loss: 26.3732 - accuracy: 0.6275 - val_loss: 52.0742 - val_accuracy: 0.5462\n","Epoch 15/50\n","34/34 [==============================] - 4s 118ms/step - loss: 24.9583 - accuracy: 0.6275 - val_loss: 51.7329 - val_accuracy: 0.5546\n","Epoch 16/50\n","34/34 [==============================] - 4s 107ms/step - loss: 24.0866 - accuracy: 0.6265 - val_loss: 52.9186 - val_accuracy: 0.5294\n","Epoch 17/50\n","34/34 [==============================] - 3s 91ms/step - loss: 22.8944 - accuracy: 0.6433 - val_loss: 51.6171 - val_accuracy: 0.5798\n","Epoch 18/50\n","34/34 [==============================] - 3s 95ms/step - loss: 22.1529 - accuracy: 0.6452 - val_loss: 52.2198 - val_accuracy: 0.5630\n","Epoch 19/50\n","34/34 [==============================] - 3s 98ms/step - loss: 21.2798 - accuracy: 0.6480 - val_loss: 52.1742 - val_accuracy: 0.4454\n","Epoch 20/50\n","34/34 [==============================] - 4s 126ms/step - loss: 20.7252 - accuracy: 0.6415 - val_loss: 52.0504 - val_accuracy: 0.6555\n","Epoch 21/50\n","34/34 [==============================] - 3s 98ms/step - loss: 20.7477 - accuracy: 0.6405 - val_loss: 51.8195 - val_accuracy: 0.5462\n","Epoch 22/50\n","34/34 [==============================] - 3s 92ms/step - loss: 19.8399 - accuracy: 0.6433 - val_loss: 53.4534 - val_accuracy: 0.4622\n","Epoch 23/50\n","34/34 [==============================] - 3s 92ms/step - loss: 19.1017 - accuracy: 0.6471 - val_loss: 52.5719 - val_accuracy: 0.6471\n","Epoch 24/50\n","34/34 [==============================] - 4s 115ms/step - loss: 18.5934 - accuracy: 0.6312 - val_loss: 53.3721 - val_accuracy: 0.6471\n","Epoch 25/50\n","34/34 [==============================] - 4s 109ms/step - loss: 17.7430 - accuracy: 0.6573 - val_loss: 59.5229 - val_accuracy: 0.4538\n","Epoch 26/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.8020 - accuracy: 0.6331 - val_loss: 53.6976 - val_accuracy: 0.5966\n","Epoch 27/50\n","34/34 [==============================] - 3s 90ms/step - loss: 17.1759 - accuracy: 0.6583 - val_loss: 52.7566 - val_accuracy: 0.6387\n","Epoch 28/50\n","34/34 [==============================] - 3s 99ms/step - loss: 16.5968 - accuracy: 0.6611 - val_loss: 54.0998 - val_accuracy: 0.5294\n","Epoch 29/50\n","34/34 [==============================] - 4s 131ms/step - loss: 16.2666 - accuracy: 0.6639 - val_loss: 52.4946 - val_accuracy: 0.6387\n","Epoch 30/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.9639 - accuracy: 0.6648 - val_loss: 54.2105 - val_accuracy: 0.5126\n","Epoch 31/50\n","34/34 [==============================] - 3s 91ms/step - loss: 15.5304 - accuracy: 0.6601 - val_loss: 51.4173 - val_accuracy: 0.6387\n","Epoch 32/50\n","34/34 [==============================] - 3s 91ms/step - loss: 15.1288 - accuracy: 0.6723 - val_loss: 54.8102 - val_accuracy: 0.4622\n","Epoch 33/50\n","34/34 [==============================] - 4s 114ms/step - loss: 14.8877 - accuracy: 0.6732 - val_loss: 51.6149 - val_accuracy: 0.6471\n","Epoch 34/50\n","34/34 [==============================] - 4s 116ms/step - loss: 14.7599 - accuracy: 0.6695 - val_loss: 54.8061 - val_accuracy: 0.5126\n","Epoch 35/50\n","34/34 [==============================] - 3s 91ms/step - loss: 14.4755 - accuracy: 0.6685 - val_loss: 53.7765 - val_accuracy: 0.6471\n","Epoch 36/50\n","34/34 [==============================] - 3s 90ms/step - loss: 14.1516 - accuracy: 0.6611 - val_loss: 53.9137 - val_accuracy: 0.5630\n","Epoch 37/50\n","34/34 [==============================] - 3s 89ms/step - loss: 14.0195 - accuracy: 0.6629 - val_loss: 51.8417 - val_accuracy: 0.6134\n","Epoch 38/50\n","34/34 [==============================] - 4s 128ms/step - loss: 13.8846 - accuracy: 0.6685 - val_loss: 52.5253 - val_accuracy: 0.6471\n","Epoch 39/50\n","34/34 [==============================] - 3s 98ms/step - loss: 13.7695 - accuracy: 0.6648 - val_loss: 52.6159 - val_accuracy: 0.6050\n","Epoch 40/50\n","34/34 [==============================] - 3s 90ms/step - loss: 13.6105 - accuracy: 0.6639 - val_loss: 53.6432 - val_accuracy: 0.6387\n","Epoch 41/50\n","34/34 [==============================] - 3s 90ms/step - loss: 14.0213 - accuracy: 0.6676 - val_loss: 51.4355 - val_accuracy: 0.5630\n","Epoch 42/50\n","34/34 [==============================] - 4s 105ms/step - loss: 14.1499 - accuracy: 0.6685 - val_loss: 52.9003 - val_accuracy: 0.6555\n","Epoch 43/50\n","34/34 [==============================] - 4s 118ms/step - loss: 13.6571 - accuracy: 0.6685 - val_loss: 51.6012 - val_accuracy: 0.6555\n","Epoch 44/50\n","34/34 [==============================] - 3s 88ms/step - loss: 13.6561 - accuracy: 0.6704 - val_loss: 54.8687 - val_accuracy: 0.6639\n","Epoch 45/50\n","34/34 [==============================] - 3s 90ms/step - loss: 13.8666 - accuracy: 0.6639 - val_loss: 52.9108 - val_accuracy: 0.5966\n","Epoch 46/50\n","34/34 [==============================] - 3s 90ms/step - loss: 13.7805 - accuracy: 0.6704 - val_loss: 54.8235 - val_accuracy: 0.6134\n","Epoch 47/50\n","34/34 [==============================] - 4s 115ms/step - loss: 14.9950 - accuracy: 0.6769 - val_loss: 55.8627 - val_accuracy: 0.6134\n","Epoch 48/50\n","34/34 [==============================] - 4s 109ms/step - loss: 14.3520 - accuracy: 0.6657 - val_loss: 54.2793 - val_accuracy: 0.6387\n","Epoch 49/50\n","34/34 [==============================] - 3s 89ms/step - loss: 13.8150 - accuracy: 0.6704 - val_loss: 54.9182 - val_accuracy: 0.6303\n","Epoch 50/50\n","34/34 [==============================] - 3s 91ms/step - loss: 13.1978 - accuracy: 0.6732 - val_loss: 54.8695 - val_accuracy: 0.6387\n","10/10 [==============================] - 0s 18ms/step\n","10/10 [==============================] - 0s 21ms/step - loss: 632.4830 - accuracy: 0.9900\n","Test Loss: 632.4830, Test accuracy : 0.9900\n","Epoch 1/50\n","34/34 [==============================] - 4s 97ms/step - loss: 53.3949 - accuracy: 0.4314 - val_loss: 34.1351 - val_accuracy: 0.7311\n","Epoch 2/50\n","34/34 [==============================] - 3s 91ms/step - loss: 39.2084 - accuracy: 0.6900 - val_loss: 34.6151 - val_accuracy: 0.5546\n","Epoch 3/50\n","34/34 [==============================] - 4s 104ms/step - loss: 36.2178 - accuracy: 0.6181 - val_loss: 34.3266 - val_accuracy: 0.4874\n","Epoch 4/50\n","34/34 [==============================] - 4s 123ms/step - loss: 35.4844 - accuracy: 0.6368 - val_loss: 33.5510 - val_accuracy: 0.7227\n","Epoch 5/50\n","34/34 [==============================] - 3s 92ms/step - loss: 32.6457 - accuracy: 0.7087 - val_loss: 33.2237 - val_accuracy: 0.7311\n","Epoch 6/50\n","34/34 [==============================] - 3s 91ms/step - loss: 30.3407 - accuracy: 0.7171 - val_loss: 33.4099 - val_accuracy: 0.7731\n","Epoch 7/50\n","34/34 [==============================] - 3s 91ms/step - loss: 27.9592 - accuracy: 0.6835 - val_loss: 32.7911 - val_accuracy: 0.7815\n","Epoch 8/50\n","34/34 [==============================] - 4s 115ms/step - loss: 26.1004 - accuracy: 0.6900 - val_loss: 33.0032 - val_accuracy: 0.7899\n","Epoch 9/50\n","34/34 [==============================] - 4s 111ms/step - loss: 24.2653 - accuracy: 0.7162 - val_loss: 31.9812 - val_accuracy: 0.7983\n","Epoch 10/50\n","34/34 [==============================] - 3s 93ms/step - loss: 22.6391 - accuracy: 0.6788 - val_loss: 32.3737 - val_accuracy: 0.7815\n","Epoch 11/50\n","34/34 [==============================] - 3s 95ms/step - loss: 21.4593 - accuracy: 0.6517 - val_loss: 34.4285 - val_accuracy: 0.7059\n","Epoch 12/50\n","34/34 [==============================] - 3s 97ms/step - loss: 20.3967 - accuracy: 0.6713 - val_loss: 32.4784 - val_accuracy: 0.7899\n","Epoch 13/50\n","34/34 [==============================] - 4s 125ms/step - loss: 20.0364 - accuracy: 0.6993 - val_loss: 32.2691 - val_accuracy: 0.7479\n","Epoch 14/50\n","34/34 [==============================] - 3s 96ms/step - loss: 18.7482 - accuracy: 0.6723 - val_loss: 31.2447 - val_accuracy: 0.7647\n","Epoch 15/50\n","34/34 [==============================] - 3s 94ms/step - loss: 17.7004 - accuracy: 0.6863 - val_loss: 33.8580 - val_accuracy: 0.7815\n","Epoch 16/50\n","34/34 [==============================] - 3s 91ms/step - loss: 17.1061 - accuracy: 0.7049 - val_loss: 30.8951 - val_accuracy: 0.7563\n","Epoch 17/50\n","34/34 [==============================] - 4s 117ms/step - loss: 18.0875 - accuracy: 0.7096 - val_loss: 35.5167 - val_accuracy: 0.7311\n","Epoch 18/50\n","34/34 [==============================] - 4s 107ms/step - loss: 18.7641 - accuracy: 0.7021 - val_loss: 33.3080 - val_accuracy: 0.7731\n","Epoch 19/50\n","34/34 [==============================] - 3s 90ms/step - loss: 18.2604 - accuracy: 0.7442 - val_loss: 32.5958 - val_accuracy: 0.7815\n","Epoch 20/50\n","34/34 [==============================] - 3s 90ms/step - loss: 17.3350 - accuracy: 0.7358 - val_loss: 32.9818 - val_accuracy: 0.7731\n","Epoch 21/50\n","34/34 [==============================] - 3s 100ms/step - loss: 17.4042 - accuracy: 0.7750 - val_loss: 32.9557 - val_accuracy: 0.7815\n","Epoch 22/50\n","34/34 [==============================] - 4s 126ms/step - loss: 16.7893 - accuracy: 0.7768 - val_loss: 33.1614 - val_accuracy: 0.7647\n","Epoch 23/50\n","34/34 [==============================] - 3s 95ms/step - loss: 16.6381 - accuracy: 0.8021 - val_loss: 32.8953 - val_accuracy: 0.7983\n","Epoch 24/50\n","34/34 [==============================] - 3s 91ms/step - loss: 16.2367 - accuracy: 0.8067 - val_loss: 32.6114 - val_accuracy: 0.7983\n","Epoch 25/50\n","34/34 [==============================] - 5s 147ms/step - loss: 15.9397 - accuracy: 0.8217 - val_loss: 33.0759 - val_accuracy: 0.7983\n","Epoch 26/50\n","34/34 [==============================] - 6s 175ms/step - loss: 15.8534 - accuracy: 0.8273 - val_loss: 32.4448 - val_accuracy: 0.7983\n","Epoch 27/50\n","34/34 [==============================] - 3s 95ms/step - loss: 15.7744 - accuracy: 0.7993 - val_loss: 32.6911 - val_accuracy: 0.8067\n","Epoch 28/50\n","34/34 [==============================] - 3s 92ms/step - loss: 15.5622 - accuracy: 0.7946 - val_loss: 32.5537 - val_accuracy: 0.8067\n","Epoch 29/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.2585 - accuracy: 0.7965 - val_loss: 32.8069 - val_accuracy: 0.8067\n","Epoch 30/50\n","34/34 [==============================] - 4s 114ms/step - loss: 15.5373 - accuracy: 0.7983 - val_loss: 32.4758 - val_accuracy: 0.8067\n","Epoch 31/50\n","34/34 [==============================] - 4s 121ms/step - loss: 15.0496 - accuracy: 0.7974 - val_loss: 32.7631 - val_accuracy: 0.8067\n","Epoch 32/50\n","34/34 [==============================] - 3s 91ms/step - loss: 14.8737 - accuracy: 0.7965 - val_loss: 32.7250 - val_accuracy: 0.8067\n","Epoch 33/50\n","34/34 [==============================] - 3s 91ms/step - loss: 14.8037 - accuracy: 0.7993 - val_loss: 32.7127 - val_accuracy: 0.8067\n","Epoch 34/50\n","34/34 [==============================] - 3s 95ms/step - loss: 14.8287 - accuracy: 0.7983 - val_loss: 32.9758 - val_accuracy: 0.8067\n","Epoch 35/50\n","34/34 [==============================] - 4s 121ms/step - loss: 14.8593 - accuracy: 0.7983 - val_loss: 32.3273 - val_accuracy: 0.8067\n","Epoch 36/50\n","34/34 [==============================] - 3s 102ms/step - loss: 14.9960 - accuracy: 0.7993 - val_loss: 32.8745 - val_accuracy: 0.8067\n","Epoch 37/50\n","34/34 [==============================] - 3s 89ms/step - loss: 15.0794 - accuracy: 0.7993 - val_loss: 32.8294 - val_accuracy: 0.8067\n","Epoch 38/50\n","34/34 [==============================] - 3s 90ms/step - loss: 15.1199 - accuracy: 0.7983 - val_loss: 30.5836 - val_accuracy: 0.8067\n","Epoch 39/50\n","34/34 [==============================] - 4s 112ms/step - loss: 14.9747 - accuracy: 0.7974 - val_loss: 33.5488 - val_accuracy: 0.8067\n","Epoch 40/50\n","34/34 [==============================] - 4s 118ms/step - loss: 15.2080 - accuracy: 0.7993 - val_loss: 30.7920 - val_accuracy: 0.8067\n","Epoch 41/50\n","34/34 [==============================] - 3s 90ms/step - loss: 15.3239 - accuracy: 0.7993 - val_loss: 33.2568 - val_accuracy: 0.8067\n","Epoch 42/50\n","34/34 [==============================] - 3s 92ms/step - loss: 15.8073 - accuracy: 0.8002 - val_loss: 32.7939 - val_accuracy: 0.8067\n","Epoch 43/50\n","34/34 [==============================] - 3s 94ms/step - loss: 16.5662 - accuracy: 0.8011 - val_loss: 33.5512 - val_accuracy: 0.8067\n","Epoch 44/50\n","34/34 [==============================] - 4s 128ms/step - loss: 15.7033 - accuracy: 0.7993 - val_loss: 32.8299 - val_accuracy: 0.7983\n","Epoch 45/50\n","34/34 [==============================] - 3s 100ms/step - loss: 15.5029 - accuracy: 0.8002 - val_loss: 32.8422 - val_accuracy: 0.7983\n","Epoch 46/50\n","34/34 [==============================] - 3s 92ms/step - loss: 15.0782 - accuracy: 0.8030 - val_loss: 32.3138 - val_accuracy: 0.8067\n","Epoch 47/50\n","34/34 [==============================] - 3s 91ms/step - loss: 14.6228 - accuracy: 0.8011 - val_loss: 32.5596 - val_accuracy: 0.8067\n","Epoch 48/50\n","34/34 [==============================] - 4s 108ms/step - loss: 14.5622 - accuracy: 0.8030 - val_loss: 32.6872 - val_accuracy: 0.8067\n","Epoch 49/50\n","34/34 [==============================] - 4s 115ms/step - loss: 14.4397 - accuracy: 0.8049 - val_loss: 32.3434 - val_accuracy: 0.8067\n","Epoch 50/50\n","34/34 [==============================] - 3s 93ms/step - loss: 14.4021 - accuracy: 0.7993 - val_loss: 32.4712 - val_accuracy: 0.8067\n","10/10 [==============================] - 0s 18ms/step\n","10/10 [==============================] - 0s 18ms/step - loss: 241.1902 - accuracy: 0.9900\n","Test Loss: 241.1902, Test accuracy : 0.9900\n","Epoch 1/50\n","34/34 [==============================] - 4s 101ms/step - loss: 85.5620 - accuracy: 0.0784 - val_loss: 60.0375 - val_accuracy: 0.0672\n","Epoch 2/50\n","34/34 [==============================] - 4s 126ms/step - loss: 66.8992 - accuracy: 0.0840 - val_loss: 59.3534 - val_accuracy: 0.2437\n","Epoch 3/50\n","34/34 [==============================] - 3s 95ms/step - loss: 60.6307 - accuracy: 0.1746 - val_loss: 56.3508 - val_accuracy: 0.3529\n","Epoch 4/50\n","34/34 [==============================] - 3s 93ms/step - loss: 56.2230 - accuracy: 0.2232 - val_loss: 58.5459 - val_accuracy: 0.1092\n","Epoch 5/50\n","34/34 [==============================] - 3s 92ms/step - loss: 49.9925 - accuracy: 0.2670 - val_loss: 53.6191 - val_accuracy: 0.2185\n","Epoch 6/50\n","34/34 [==============================] - 4s 111ms/step - loss: 45.3559 - accuracy: 0.2820 - val_loss: 50.0738 - val_accuracy: 0.2185\n","Epoch 7/50\n","34/34 [==============================] - 4s 115ms/step - loss: 40.0471 - accuracy: 0.2848 - val_loss: 44.9689 - val_accuracy: 0.2353\n","Epoch 8/50\n","34/34 [==============================] - 3s 92ms/step - loss: 36.4252 - accuracy: 0.2876 - val_loss: 46.0277 - val_accuracy: 0.2605\n","Epoch 9/50\n","34/34 [==============================] - 3s 93ms/step - loss: 33.9969 - accuracy: 0.3007 - val_loss: 43.5035 - val_accuracy: 0.1597\n","Epoch 10/50\n","34/34 [==============================] - 3s 99ms/step - loss: 32.1347 - accuracy: 0.3016 - val_loss: 46.7920 - val_accuracy: 0.1597\n","Epoch 11/50\n","34/34 [==============================] - 4s 124ms/step - loss: 29.4099 - accuracy: 0.3175 - val_loss: 42.1873 - val_accuracy: 0.2269\n","Epoch 12/50\n","34/34 [==============================] - 3s 100ms/step - loss: 27.9017 - accuracy: 0.3203 - val_loss: 41.4402 - val_accuracy: 0.1765\n","Epoch 13/50\n","34/34 [==============================] - 3s 94ms/step - loss: 25.7235 - accuracy: 0.3287 - val_loss: 42.8740 - val_accuracy: 0.2101\n","Epoch 14/50\n","34/34 [==============================] - 3s 92ms/step - loss: 23.7601 - accuracy: 0.3100 - val_loss: 41.9541 - val_accuracy: 0.2017\n","Epoch 15/50\n","34/34 [==============================] - 4s 118ms/step - loss: 22.5538 - accuracy: 0.3389 - val_loss: 42.5734 - val_accuracy: 0.2101\n","Epoch 16/50\n","34/34 [==============================] - 4s 121ms/step - loss: 20.8626 - accuracy: 0.3249 - val_loss: 43.8573 - val_accuracy: 0.2353\n","Epoch 17/50\n","34/34 [==============================] - 3s 93ms/step - loss: 19.7385 - accuracy: 0.3417 - val_loss: 43.3032 - val_accuracy: 0.2521\n","Epoch 18/50\n","34/34 [==============================] - 3s 97ms/step - loss: 19.0468 - accuracy: 0.3333 - val_loss: 43.1474 - val_accuracy: 0.2605\n","Epoch 19/50\n","34/34 [==============================] - 3s 99ms/step - loss: 18.3917 - accuracy: 0.3501 - val_loss: 44.3051 - val_accuracy: 0.2605\n","Epoch 20/50\n","34/34 [==============================] - 4s 126ms/step - loss: 18.1345 - accuracy: 0.3389 - val_loss: 45.4198 - val_accuracy: 0.2185\n","Epoch 21/50\n","34/34 [==============================] - 3s 99ms/step - loss: 17.9890 - accuracy: 0.3371 - val_loss: 42.2108 - val_accuracy: 0.2269\n","Epoch 22/50\n","34/34 [==============================] - 3s 92ms/step - loss: 17.7474 - accuracy: 0.3464 - val_loss: 45.2843 - val_accuracy: 0.3109\n","Epoch 23/50\n","34/34 [==============================] - 3s 92ms/step - loss: 17.3925 - accuracy: 0.3501 - val_loss: 42.0552 - val_accuracy: 0.2605\n","Epoch 24/50\n","34/34 [==============================] - 4s 117ms/step - loss: 17.3463 - accuracy: 0.3492 - val_loss: 47.0910 - val_accuracy: 0.2017\n","Epoch 25/50\n","34/34 [==============================] - 4s 113ms/step - loss: 18.0407 - accuracy: 0.3399 - val_loss: 44.5339 - val_accuracy: 0.2521\n","Epoch 26/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.7601 - accuracy: 0.3352 - val_loss: 44.6019 - val_accuracy: 0.2437\n","Epoch 27/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.6216 - accuracy: 0.3287 - val_loss: 46.1972 - val_accuracy: 0.3109\n","Epoch 28/50\n","34/34 [==============================] - 3s 100ms/step - loss: 17.3486 - accuracy: 0.3399 - val_loss: 45.2084 - val_accuracy: 0.2689\n","Epoch 29/50\n","34/34 [==============================] - 4s 131ms/step - loss: 17.1090 - accuracy: 0.3408 - val_loss: 44.3979 - val_accuracy: 0.2689\n","Epoch 30/50\n","34/34 [==============================] - 3s 95ms/step - loss: 16.2892 - accuracy: 0.3408 - val_loss: 43.9628 - val_accuracy: 0.2857\n","Epoch 31/50\n","34/34 [==============================] - 3s 91ms/step - loss: 15.9537 - accuracy: 0.3427 - val_loss: 43.6504 - val_accuracy: 0.2773\n","Epoch 32/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.7896 - accuracy: 0.3417 - val_loss: 43.8299 - val_accuracy: 0.2857\n","Epoch 33/50\n","34/34 [==============================] - 4s 116ms/step - loss: 15.5786 - accuracy: 0.3212 - val_loss: 45.3878 - val_accuracy: 0.2521\n","Epoch 34/50\n","34/34 [==============================] - 4s 120ms/step - loss: 15.3542 - accuracy: 0.3287 - val_loss: 43.6567 - val_accuracy: 0.2689\n","Epoch 35/50\n","34/34 [==============================] - 3s 93ms/step - loss: 15.1860 - accuracy: 0.3175 - val_loss: 44.5754 - val_accuracy: 0.2857\n","Epoch 36/50\n","34/34 [==============================] - 3s 94ms/step - loss: 15.3580 - accuracy: 0.3231 - val_loss: 44.1098 - val_accuracy: 0.3025\n","Epoch 37/50\n","34/34 [==============================] - 3s 97ms/step - loss: 15.0668 - accuracy: 0.3231 - val_loss: 45.7086 - val_accuracy: 0.2437\n","Epoch 38/50\n","34/34 [==============================] - 4s 127ms/step - loss: 15.0393 - accuracy: 0.3193 - val_loss: 45.0465 - val_accuracy: 0.3025\n","Epoch 39/50\n","34/34 [==============================] - 3s 101ms/step - loss: 14.8602 - accuracy: 0.3249 - val_loss: 44.7717 - val_accuracy: 0.2941\n","Epoch 40/50\n","34/34 [==============================] - 3s 92ms/step - loss: 14.6426 - accuracy: 0.3352 - val_loss: 45.8304 - val_accuracy: 0.2941\n","Epoch 41/50\n","34/34 [==============================] - 3s 94ms/step - loss: 14.3096 - accuracy: 0.3231 - val_loss: 44.6130 - val_accuracy: 0.3193\n","Epoch 42/50\n","34/34 [==============================] - 4s 119ms/step - loss: 14.1462 - accuracy: 0.3296 - val_loss: 44.5657 - val_accuracy: 0.3193\n","Epoch 43/50\n","34/34 [==============================] - 4s 118ms/step - loss: 13.7907 - accuracy: 0.3324 - val_loss: 45.7616 - val_accuracy: 0.2941\n","Epoch 44/50\n","34/34 [==============================] - 3s 93ms/step - loss: 13.6367 - accuracy: 0.3324 - val_loss: 44.3371 - val_accuracy: 0.2857\n","Epoch 45/50\n","34/34 [==============================] - 3s 91ms/step - loss: 13.4020 - accuracy: 0.3352 - val_loss: 43.5421 - val_accuracy: 0.2773\n","Epoch 46/50\n","34/34 [==============================] - 3s 98ms/step - loss: 13.5496 - accuracy: 0.3296 - val_loss: 44.5310 - val_accuracy: 0.2941\n","Epoch 47/50\n","34/34 [==============================] - 4s 131ms/step - loss: 13.5714 - accuracy: 0.3305 - val_loss: 44.2009 - val_accuracy: 0.3109\n","Epoch 48/50\n","34/34 [==============================] - 3s 99ms/step - loss: 13.7208 - accuracy: 0.3324 - val_loss: 44.9476 - val_accuracy: 0.2857\n","Epoch 49/50\n","34/34 [==============================] - 3s 95ms/step - loss: 13.9356 - accuracy: 0.3277 - val_loss: 42.6500 - val_accuracy: 0.2773\n","Epoch 50/50\n","34/34 [==============================] - 3s 92ms/step - loss: 14.0106 - accuracy: 0.3315 - val_loss: 45.0699 - val_accuracy: 0.2941\n","10/10 [==============================] - 0s 25ms/step\n","10/10 [==============================] - 0s 30ms/step - loss: 388.3080 - accuracy: 0.8000\n","Test Loss: 388.3080, Test accuracy : 0.8000\n","Epoch 1/50\n","34/34 [==============================] - 4s 103ms/step - loss: 95.9270 - accuracy: 0.2661 - val_loss: 48.0788 - val_accuracy: 0.2269\n","Epoch 2/50\n","34/34 [==============================] - 3s 98ms/step - loss: 70.6904 - accuracy: 0.1821 - val_loss: 54.6142 - val_accuracy: 0.1765\n","Epoch 3/50\n","34/34 [==============================] - 4s 107ms/step - loss: 69.4926 - accuracy: 0.6443 - val_loss: 47.8828 - val_accuracy: 0.6218\n","Epoch 4/50\n","34/34 [==============================] - 5s 152ms/step - loss: 68.0623 - accuracy: 0.6387 - val_loss: 47.1329 - val_accuracy: 0.6891\n","Epoch 5/50\n","34/34 [==============================] - 3s 102ms/step - loss: 63.9177 - accuracy: 0.6097 - val_loss: 47.7785 - val_accuracy: 0.7059\n","Epoch 6/50\n","34/34 [==============================] - 3s 96ms/step - loss: 60.7260 - accuracy: 0.6797 - val_loss: 46.5079 - val_accuracy: 0.5798\n","Epoch 7/50\n","34/34 [==============================] - 3s 100ms/step - loss: 57.4831 - accuracy: 0.7106 - val_loss: 46.5177 - val_accuracy: 0.6303\n","Epoch 8/50\n","34/34 [==============================] - 5s 134ms/step - loss: 54.1962 - accuracy: 0.7320 - val_loss: 50.2750 - val_accuracy: 0.3866\n","Epoch 9/50\n","34/34 [==============================] - 3s 101ms/step - loss: 51.2450 - accuracy: 0.7414 - val_loss: 46.8481 - val_accuracy: 0.5882\n","Epoch 10/50\n","34/34 [==============================] - 3s 98ms/step - loss: 48.7440 - accuracy: 0.7834 - val_loss: 48.8290 - val_accuracy: 0.5630\n","Epoch 11/50\n","34/34 [==============================] - 3s 96ms/step - loss: 47.1285 - accuracy: 0.7918 - val_loss: 46.6991 - val_accuracy: 0.6387\n","Epoch 12/50\n","34/34 [==============================] - 4s 131ms/step - loss: 45.8302 - accuracy: 0.7983 - val_loss: 51.0378 - val_accuracy: 0.7311\n","Epoch 13/50\n","34/34 [==============================] - 4s 120ms/step - loss: 44.7731 - accuracy: 0.7993 - val_loss: 49.5934 - val_accuracy: 0.7647\n","Epoch 14/50\n","34/34 [==============================] - 3s 94ms/step - loss: 44.0549 - accuracy: 0.7974 - val_loss: 53.9418 - val_accuracy: 0.7731\n","Epoch 15/50\n","34/34 [==============================] - 3s 100ms/step - loss: 43.3288 - accuracy: 0.7937 - val_loss: 50.0931 - val_accuracy: 0.7731\n","Epoch 16/50\n","34/34 [==============================] - 5s 135ms/step - loss: 43.1053 - accuracy: 0.7918 - val_loss: 47.0388 - val_accuracy: 0.7731\n","Epoch 17/50\n","34/34 [==============================] - 4s 124ms/step - loss: 42.7688 - accuracy: 0.7955 - val_loss: 49.7712 - val_accuracy: 0.7731\n","Epoch 18/50\n","34/34 [==============================] - 3s 96ms/step - loss: 42.5840 - accuracy: 0.7955 - val_loss: 48.9654 - val_accuracy: 0.7731\n","Epoch 19/50\n","34/34 [==============================] - 4s 106ms/step - loss: 42.5903 - accuracy: 0.7955 - val_loss: 48.5249 - val_accuracy: 0.7731\n","Epoch 20/50\n","34/34 [==============================] - 4s 114ms/step - loss: 42.4942 - accuracy: 0.7955 - val_loss: 49.1373 - val_accuracy: 0.7731\n","Epoch 21/50\n","34/34 [==============================] - 4s 132ms/step - loss: 42.2022 - accuracy: 0.7955 - val_loss: 48.3911 - val_accuracy: 0.7731\n","Epoch 22/50\n","34/34 [==============================] - 3s 101ms/step - loss: 42.0994 - accuracy: 0.7946 - val_loss: 49.2596 - val_accuracy: 0.7731\n","Epoch 23/50\n","34/34 [==============================] - 4s 108ms/step - loss: 42.0075 - accuracy: 0.7955 - val_loss: 47.3767 - val_accuracy: 0.7731\n","Epoch 24/50\n","34/34 [==============================] - 4s 123ms/step - loss: 42.7223 - accuracy: 0.7937 - val_loss: 46.3016 - val_accuracy: 0.7815\n","Epoch 25/50\n","34/34 [==============================] - 4s 127ms/step - loss: 42.1754 - accuracy: 0.7927 - val_loss: 57.6196 - val_accuracy: 0.7731\n","Epoch 26/50\n","34/34 [==============================] - 3s 102ms/step - loss: 42.8370 - accuracy: 0.7955 - val_loss: 53.8718 - val_accuracy: 0.7731\n","Epoch 27/50\n","34/34 [==============================] - 3s 101ms/step - loss: 43.2101 - accuracy: 0.7955 - val_loss: 46.6912 - val_accuracy: 0.7731\n","Epoch 28/50\n","34/34 [==============================] - 4s 122ms/step - loss: 42.0836 - accuracy: 0.7955 - val_loss: 48.8395 - val_accuracy: 0.7731\n","Epoch 29/50\n","34/34 [==============================] - 4s 128ms/step - loss: 40.2564 - accuracy: 0.7955 - val_loss: 47.6127 - val_accuracy: 0.7731\n","Epoch 30/50\n","34/34 [==============================] - 4s 104ms/step - loss: 40.1942 - accuracy: 0.7955 - val_loss: 48.2157 - val_accuracy: 0.7731\n","Epoch 31/50\n","34/34 [==============================] - 3s 101ms/step - loss: 40.4996 - accuracy: 0.7955 - val_loss: 47.5947 - val_accuracy: 0.7731\n","Epoch 32/50\n","34/34 [==============================] - 4s 110ms/step - loss: 39.6365 - accuracy: 0.7955 - val_loss: 46.6325 - val_accuracy: 0.7731\n","Epoch 33/50\n","34/34 [==============================] - 4s 131ms/step - loss: 39.2378 - accuracy: 0.7955 - val_loss: 46.4909 - val_accuracy: 0.7731\n","Epoch 34/50\n","34/34 [==============================] - 3s 102ms/step - loss: 38.0628 - accuracy: 0.7965 - val_loss: 47.2896 - val_accuracy: 0.7815\n","Epoch 35/50\n","34/34 [==============================] - 3s 102ms/step - loss: 37.6930 - accuracy: 0.7955 - val_loss: 46.3539 - val_accuracy: 0.7731\n","Epoch 36/50\n","34/34 [==============================] - 4s 103ms/step - loss: 37.7395 - accuracy: 0.7965 - val_loss: 46.6720 - val_accuracy: 0.7815\n","Epoch 37/50\n","34/34 [==============================] - 5s 151ms/step - loss: 37.5863 - accuracy: 0.7937 - val_loss: 47.3239 - val_accuracy: 0.7647\n","Epoch 38/50\n","34/34 [==============================] - 4s 111ms/step - loss: 37.8173 - accuracy: 0.7946 - val_loss: 50.8488 - val_accuracy: 0.7815\n","Epoch 39/50\n","34/34 [==============================] - 3s 95ms/step - loss: 35.5417 - accuracy: 0.7955 - val_loss: 47.5125 - val_accuracy: 0.7815\n","Epoch 40/50\n","34/34 [==============================] - 4s 107ms/step - loss: 34.8952 - accuracy: 0.7965 - val_loss: 48.8806 - val_accuracy: 0.7815\n","Epoch 41/50\n","34/34 [==============================] - 5s 148ms/step - loss: 33.8669 - accuracy: 0.7955 - val_loss: 50.8194 - val_accuracy: 0.7815\n","Epoch 42/50\n","34/34 [==============================] - 3s 99ms/step - loss: 33.6260 - accuracy: 0.7965 - val_loss: 50.0039 - val_accuracy: 0.7815\n","Epoch 43/50\n","34/34 [==============================] - 5s 143ms/step - loss: 33.4310 - accuracy: 0.7955 - val_loss: 48.0584 - val_accuracy: 0.7815\n","Epoch 44/50\n","34/34 [==============================] - 6s 164ms/step - loss: 33.4060 - accuracy: 0.7965 - val_loss: 49.8400 - val_accuracy: 0.7815\n","Epoch 45/50\n","34/34 [==============================] - 5s 134ms/step - loss: 33.4550 - accuracy: 0.7955 - val_loss: 48.7249 - val_accuracy: 0.7731\n","Epoch 46/50\n","34/34 [==============================] - 3s 101ms/step - loss: 33.4136 - accuracy: 0.7965 - val_loss: 48.6813 - val_accuracy: 0.7731\n","Epoch 47/50\n","34/34 [==============================] - 3s 103ms/step - loss: 33.2839 - accuracy: 0.7955 - val_loss: 48.4981 - val_accuracy: 0.7731\n","Epoch 48/50\n","34/34 [==============================] - 4s 129ms/step - loss: 33.0721 - accuracy: 0.7965 - val_loss: 48.9006 - val_accuracy: 0.7815\n","Epoch 49/50\n","34/34 [==============================] - 4s 127ms/step - loss: 33.2444 - accuracy: 0.7955 - val_loss: 48.2994 - val_accuracy: 0.7731\n","Epoch 50/50\n","34/34 [==============================] - 3s 101ms/step - loss: 33.2977 - accuracy: 0.7965 - val_loss: 48.3234 - val_accuracy: 0.7731\n","10/10 [==============================] - 0s 19ms/step\n","10/10 [==============================] - 0s 21ms/step - loss: 327.2716 - accuracy: 1.0000\n","Test Loss: 327.2716, Test accuracy : 1.0000\n","Epoch 1/50\n","34/34 [==============================] - 5s 121ms/step - loss: 78.4871 - accuracy: 0.1830 - val_loss: 81.2250 - val_accuracy: 0.2521\n","Epoch 2/50\n","34/34 [==============================] - 4s 124ms/step - loss: 49.1781 - accuracy: 0.1317 - val_loss: 80.4297 - val_accuracy: 0.3025\n","Epoch 3/50\n","34/34 [==============================] - 3s 94ms/step - loss: 47.1181 - accuracy: 0.1905 - val_loss: 81.4290 - val_accuracy: 0.3697\n","Epoch 4/50\n","34/34 [==============================] - 3s 97ms/step - loss: 38.7962 - accuracy: 0.1653 - val_loss: 80.2678 - val_accuracy: 0.3613\n","Epoch 5/50\n","34/34 [==============================] - 3s 96ms/step - loss: 34.5317 - accuracy: 0.1895 - val_loss: 78.4831 - val_accuracy: 0.4370\n","Epoch 6/50\n","34/34 [==============================] - 4s 133ms/step - loss: 31.6292 - accuracy: 0.1951 - val_loss: 79.0585 - val_accuracy: 0.2437\n","Epoch 7/50\n","34/34 [==============================] - 3s 102ms/step - loss: 28.8703 - accuracy: 0.1849 - val_loss: 74.2256 - val_accuracy: 0.4874\n","Epoch 8/50\n","34/34 [==============================] - 3s 92ms/step - loss: 24.1357 - accuracy: 0.1718 - val_loss: 74.6326 - val_accuracy: 0.4118\n","Epoch 9/50\n","34/34 [==============================] - 3s 93ms/step - loss: 21.1625 - accuracy: 0.1727 - val_loss: 75.3545 - val_accuracy: 0.4202\n","Epoch 10/50\n","34/34 [==============================] - 4s 118ms/step - loss: 19.3977 - accuracy: 0.1699 - val_loss: 74.3200 - val_accuracy: 0.3866\n","Epoch 11/50\n","34/34 [==============================] - 4s 116ms/step - loss: 18.1622 - accuracy: 0.1727 - val_loss: 77.0560 - val_accuracy: 0.3361\n","Epoch 12/50\n","34/34 [==============================] - 3s 97ms/step - loss: 16.8874 - accuracy: 0.7283 - val_loss: 81.0538 - val_accuracy: 0.6471\n","Epoch 13/50\n","34/34 [==============================] - 3s 95ms/step - loss: 15.4164 - accuracy: 0.8357 - val_loss: 79.1926 - val_accuracy: 0.5882\n","Epoch 14/50\n","34/34 [==============================] - 3s 100ms/step - loss: 15.0932 - accuracy: 0.8599 - val_loss: 78.0809 - val_accuracy: 0.5294\n","Epoch 15/50\n","34/34 [==============================] - 4s 130ms/step - loss: 14.5070 - accuracy: 0.8599 - val_loss: 79.6430 - val_accuracy: 0.5966\n","Epoch 16/50\n","34/34 [==============================] - 3s 93ms/step - loss: 14.4171 - accuracy: 0.8796 - val_loss: 78.5723 - val_accuracy: 0.5798\n","Epoch 17/50\n","34/34 [==============================] - 3s 95ms/step - loss: 15.4254 - accuracy: 0.8730 - val_loss: 79.4009 - val_accuracy: 0.5546\n","Epoch 18/50\n","34/34 [==============================] - 3s 96ms/step - loss: 16.0722 - accuracy: 0.8506 - val_loss: 78.5553 - val_accuracy: 0.5714\n","Epoch 19/50\n","34/34 [==============================] - 4s 122ms/step - loss: 20.9825 - accuracy: 0.8618 - val_loss: 78.1790 - val_accuracy: 0.5546\n","Epoch 20/50\n","34/34 [==============================] - 4s 113ms/step - loss: 18.9512 - accuracy: 0.8599 - val_loss: 78.1025 - val_accuracy: 0.5126\n","Epoch 21/50\n","34/34 [==============================] - 3s 93ms/step - loss: 17.1159 - accuracy: 0.8711 - val_loss: 78.6234 - val_accuracy: 0.5546\n","Epoch 22/50\n","34/34 [==============================] - 3s 93ms/step - loss: 16.4413 - accuracy: 0.8543 - val_loss: 80.5507 - val_accuracy: 0.6555\n","Epoch 23/50\n","34/34 [==============================] - 4s 105ms/step - loss: 15.5920 - accuracy: 0.9048 - val_loss: 78.7695 - val_accuracy: 0.6303\n","Epoch 24/50\n","34/34 [==============================] - 4s 126ms/step - loss: 15.4625 - accuracy: 0.8926 - val_loss: 78.6349 - val_accuracy: 0.6387\n","Epoch 25/50\n","34/34 [==============================] - 3s 97ms/step - loss: 14.6093 - accuracy: 0.9010 - val_loss: 80.2476 - val_accuracy: 0.6387\n","Epoch 26/50\n","34/34 [==============================] - 3s 92ms/step - loss: 14.3748 - accuracy: 0.9132 - val_loss: 79.8432 - val_accuracy: 0.6471\n","Epoch 27/50\n","34/34 [==============================] - 3s 93ms/step - loss: 14.1319 - accuracy: 0.9197 - val_loss: 79.1968 - val_accuracy: 0.6303\n","Epoch 28/50\n","34/34 [==============================] - 4s 125ms/step - loss: 14.0754 - accuracy: 0.9150 - val_loss: 81.0735 - val_accuracy: 0.6387\n","Epoch 29/50\n","34/34 [==============================] - 4s 111ms/step - loss: 14.0677 - accuracy: 0.9132 - val_loss: 80.6990 - val_accuracy: 0.6387\n","Epoch 30/50\n","34/34 [==============================] - 3s 98ms/step - loss: 13.7922 - accuracy: 0.9178 - val_loss: 79.2454 - val_accuracy: 0.6387\n","Epoch 31/50\n","34/34 [==============================] - 3s 93ms/step - loss: 13.7756 - accuracy: 0.9206 - val_loss: 81.3509 - val_accuracy: 0.6471\n","Epoch 32/50\n","34/34 [==============================] - 4s 106ms/step - loss: 13.9888 - accuracy: 0.9122 - val_loss: 78.8635 - val_accuracy: 0.6471\n","Epoch 33/50\n","34/34 [==============================] - 4s 129ms/step - loss: 14.0899 - accuracy: 0.9113 - val_loss: 80.4193 - val_accuracy: 0.6471\n","Epoch 34/50\n","34/34 [==============================] - 3s 97ms/step - loss: 14.5439 - accuracy: 0.9020 - val_loss: 80.1415 - val_accuracy: 0.6555\n","Epoch 35/50\n","34/34 [==============================] - 3s 92ms/step - loss: 14.0411 - accuracy: 0.9057 - val_loss: 79.6429 - val_accuracy: 0.6471\n","Epoch 36/50\n","34/34 [==============================] - 3s 98ms/step - loss: 13.8331 - accuracy: 0.9216 - val_loss: 79.8875 - val_accuracy: 0.6471\n","Epoch 37/50\n","34/34 [==============================] - 4s 132ms/step - loss: 13.6193 - accuracy: 0.9197 - val_loss: 81.0312 - val_accuracy: 0.6218\n","Epoch 38/50\n","34/34 [==============================] - 4s 104ms/step - loss: 13.7570 - accuracy: 0.9253 - val_loss: 80.6749 - val_accuracy: 0.6218\n","Epoch 39/50\n","34/34 [==============================] - 3s 93ms/step - loss: 15.6447 - accuracy: 0.8992 - val_loss: 81.1540 - val_accuracy: 0.5966\n","Epoch 40/50\n","34/34 [==============================] - 3s 93ms/step - loss: 14.8968 - accuracy: 0.8992 - val_loss: 83.6545 - val_accuracy: 0.5798\n","Epoch 41/50\n","34/34 [==============================] - 4s 122ms/step - loss: 14.2658 - accuracy: 0.8964 - val_loss: 79.4534 - val_accuracy: 0.6218\n","Epoch 42/50\n","34/34 [==============================] - 4s 116ms/step - loss: 14.0059 - accuracy: 0.9122 - val_loss: 83.5045 - val_accuracy: 0.5966\n","Epoch 43/50\n","34/34 [==============================] - 3s 96ms/step - loss: 13.8964 - accuracy: 0.9141 - val_loss: 80.3970 - val_accuracy: 0.5882\n","Epoch 44/50\n","34/34 [==============================] - 3s 95ms/step - loss: 13.5962 - accuracy: 0.9150 - val_loss: 80.0961 - val_accuracy: 0.6387\n","Epoch 45/50\n","34/34 [==============================] - 4s 112ms/step - loss: 13.2509 - accuracy: 0.9262 - val_loss: 81.6173 - val_accuracy: 0.6134\n","Epoch 46/50\n","34/34 [==============================] - 4s 130ms/step - loss: 13.2189 - accuracy: 0.9094 - val_loss: 81.3126 - val_accuracy: 0.6303\n","Epoch 47/50\n","34/34 [==============================] - 3s 97ms/step - loss: 12.9717 - accuracy: 0.7694 - val_loss: 79.9874 - val_accuracy: 0.2185\n","Epoch 48/50\n","34/34 [==============================] - 3s 94ms/step - loss: 12.9489 - accuracy: 0.1569 - val_loss: 78.1483 - val_accuracy: 0.2185\n","Epoch 49/50\n","34/34 [==============================] - 3s 96ms/step - loss: 12.7940 - accuracy: 0.1541 - val_loss: 82.0986 - val_accuracy: 0.2185\n","Epoch 50/50\n","34/34 [==============================] - 4s 129ms/step - loss: 12.7066 - accuracy: 0.1559 - val_loss: 81.8609 - val_accuracy: 0.2185\n","10/10 [==============================] - 0s 22ms/step\n","10/10 [==============================] - 0s 23ms/step - loss: 397.5166 - accuracy: 0.1233\n","Test Loss: 397.5166, Test accuracy : 0.1233\n"]}]},{"cell_type":"code","source":["def print_model_summary(loaded_model, tajweed_rule):\n","  print(f'******* Tajweed rule {tajweed_rule} model *******')\n","  loaded_model.summary()\n","  print('\\n')"],"metadata":{"id":"i4FV1w-iDDFU","executionInfo":{"status":"ok","timestamp":1717524747802,"user_tz":-60,"elapsed":13,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["for rule in max_Y.keys():\n","    model_filename = f'{rule}_tajweed_rule_model'\n","    model_path = os.path.join(export_dir, model_filename)\n","\n","    # Load the saved model\n","    loaded_model = tf.keras.models.load_model(model_path)\n","\n","    print_model_summary(loaded_model, rule)"],"metadata":{"id":"WbRKnuWnLZTc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717524754587,"user_tz":-60,"elapsed":6796,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"751a6718-09a5-4033-8339-254c7d1a0321"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["******* Tajweed rule madd_6_Lazim model *******\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten (Flatten)           (None, 104000)            0         \n","                                                                 \n"," dense (Dense)               (None, 64)                6656064   \n","                                                                 \n"," dense_1 (Dense)             (None, 2)                 130       \n","                                                                 \n","=================================================================\n","Total params: 6656194 (25.39 MB)\n","Trainable params: 6656194 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_246 model *******\n","Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_3 (Dense)             (None, 3)                 195       \n","                                                                 \n","=================================================================\n","Total params: 6656259 (25.39 MB)\n","Trainable params: 6656259 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_6 model *******\n","Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_5 (Dense)             (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 6656454 (25.39 MB)\n","Trainable params: 6656454 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule madd_2 model *******\n","Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_6 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_7 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 6656389 (25.39 MB)\n","Trainable params: 6656389 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Ikhfaa model *******\n","Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_5 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_4 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_8 (Dense)             (None, 64)                6656064   \n","                                                                 \n"," dense_9 (Dense)             (None, 9)                 585       \n","                                                                 \n","=================================================================\n","Total params: 6656649 (25.39 MB)\n","Trainable params: 6656649 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule Idgham model *******\n","Model: \"model_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_6 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_5 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_10 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_11 (Dense)            (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 6656909 (25.39 MB)\n","Trainable params: 6656909 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule tafkhim model *******\n","Model: \"model_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_6 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_12 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_13 (Dense)            (None, 24)                1560      \n","                                                                 \n","=================================================================\n","Total params: 6657624 (25.40 MB)\n","Trainable params: 6657624 (25.40 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule qalqala model *******\n","Model: \"model_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_8 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_7 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_14 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_15 (Dense)            (None, 6)                 390       \n","                                                                 \n","=================================================================\n","Total params: 6656454 (25.39 MB)\n","Trainable params: 6656454 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n","******* Tajweed rule imala model *******\n","Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_9 (InputLayer)        [(None, 8000, 13)]        0         \n","                                                                 \n"," flatten_8 (Flatten)         (None, 104000)            0         \n","                                                                 \n"," dense_16 (Dense)            (None, 64)                6656064   \n","                                                                 \n"," dense_17 (Dense)            (None, 7)                 455       \n","                                                                 \n","=================================================================\n","Total params: 6656519 (25.39 MB)\n","Trainable params: 6656519 (25.39 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","\n","\n"]}]},{"cell_type":"code","source":["# how data is splitted\n","columns1 = ['tajweed_rule', 'data_of', 'X_train_nb_samples', 'X_test_nb_samples', 'Y_train_nb_samples', 'X_test_nb_samples']\n","splitted_data_info = pd.DataFrame(data=splitted_data_info_np, columns=columns1)\n","\n","# save models information\n","columns2 = ['Model', 'Loss', 'Accuracy', 'Accuracy %', 'Path_to_the_model']\n","models_information = pd.DataFrame(data=models_information_np, columns=columns2)"],"metadata":{"id":"txrZx0e_4Zmw","executionInfo":{"status":"ok","timestamp":1717524754588,"user_tz":-60,"elapsed":25,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["splitted_data_info"],"metadata":{"id":"8D4mYXj0Rcqb","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1717524754589,"user_tz":-60,"elapsed":24,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"5fd7f5b4-025f-40ef-c84d-419c5c9ec8ff"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    tajweed_rule            data_of X_train_nb_samples X_test_nb_samples  \\\n","0   madd_6_Lazim        Abdul Basit                238                60   \n","1   madd_6_Lazim  Yassin Al Jazaery                238                60   \n","2   madd_6_Lazim   Ibrahim_Aldosary                238                60   \n","3   madd_6_Lazim          Al husary                238                60   \n","4   madd_6_Lazim               safa                238                60   \n","5   madd_6_Lazim       all reciters               1190               300   \n","6       madd_246        Abdul Basit                238                60   \n","7       madd_246  Yassin Al Jazaery                238                60   \n","8       madd_246   Ibrahim_Aldosary                238                60   \n","9       madd_246          Al husary                238                60   \n","10      madd_246               safa                238                60   \n","11      madd_246       all reciters               1190               300   \n","12        madd_6        Abdul Basit                238                60   \n","13        madd_6  Yassin Al Jazaery                238                60   \n","14        madd_6   Ibrahim_Aldosary                238                60   \n","15        madd_6          Al husary                238                60   \n","16        madd_6               safa                238                60   \n","17        madd_6       all reciters               1190               300   \n","18        madd_2        Abdul Basit                238                60   \n","19        madd_2  Yassin Al Jazaery                238                60   \n","20        madd_2   Ibrahim_Aldosary                238                60   \n","21        madd_2          Al husary                238                60   \n","22        madd_2               safa                238                60   \n","23        madd_2       all reciters               1190               300   \n","24        Ikhfaa        Abdul Basit                238                60   \n","25        Ikhfaa  Yassin Al Jazaery                238                60   \n","26        Ikhfaa   Ibrahim_Aldosary                238                60   \n","27        Ikhfaa          Al husary                238                60   \n","28        Ikhfaa               safa                238                60   \n","29        Ikhfaa       all reciters               1190               300   \n","30        Idgham        Abdul Basit                238                60   \n","31        Idgham  Yassin Al Jazaery                238                60   \n","32        Idgham   Ibrahim_Aldosary                238                60   \n","33        Idgham          Al husary                238                60   \n","34        Idgham               safa                238                60   \n","35        Idgham       all reciters               1190               300   \n","36       tafkhim        Abdul Basit                238                60   \n","37       tafkhim  Yassin Al Jazaery                238                60   \n","38       tafkhim   Ibrahim_Aldosary                238                60   \n","39       tafkhim          Al husary                238                60   \n","40       tafkhim               safa                238                60   \n","41       tafkhim       all reciters               1190               300   \n","42       qalqala        Abdul Basit                238                60   \n","43       qalqala  Yassin Al Jazaery                238                60   \n","44       qalqala   Ibrahim_Aldosary                238                60   \n","45       qalqala          Al husary                238                60   \n","46       qalqala               safa                238                60   \n","47       qalqala       all reciters               1190               300   \n","48         imala        Abdul Basit                238                60   \n","49         imala  Yassin Al Jazaery                238                60   \n","50         imala   Ibrahim_Aldosary                238                60   \n","51         imala          Al husary                238                60   \n","52         imala               safa                238                60   \n","53         imala       all reciters               1190               300   \n","\n","   Y_train_nb_samples X_test_nb_samples  \n","0                 238                60  \n","1                 238                60  \n","2                 238                60  \n","3                 238                60  \n","4                 238                60  \n","5                1190               300  \n","6                 238                60  \n","7                 238                60  \n","8                 238                60  \n","9                 238                60  \n","10                238                60  \n","11               1190               300  \n","12                238                60  \n","13                238                60  \n","14                238                60  \n","15                238                60  \n","16                238                60  \n","17               1190               300  \n","18                238                60  \n","19                238                60  \n","20                238                60  \n","21                238                60  \n","22                238                60  \n","23               1190               300  \n","24                238                60  \n","25                238                60  \n","26                238                60  \n","27                238                60  \n","28                238                60  \n","29               1190               300  \n","30                238                60  \n","31                238                60  \n","32                238                60  \n","33                238                60  \n","34                238                60  \n","35               1190               300  \n","36                238                60  \n","37                238                60  \n","38                238                60  \n","39                238                60  \n","40                238                60  \n","41               1190               300  \n","42                238                60  \n","43                238                60  \n","44                238                60  \n","45                238                60  \n","46                238                60  \n","47               1190               300  \n","48                238                60  \n","49                238                60  \n","50                238                60  \n","51                238                60  \n","52                238                60  \n","53               1190               300  "],"text/html":["\n","  <div id=\"df-7f28fbdf-ee09-4bca-9633-47b3bc8e6989\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tajweed_rule</th>\n","      <th>data_of</th>\n","      <th>X_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","      <th>Y_train_nb_samples</th>\n","      <th>X_test_nb_samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_6_Lazim</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>madd_6_Lazim</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>madd_6_Lazim</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>madd_246</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>madd_246</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>madd_246</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>madd_246</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>madd_246</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>madd_246</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>madd_6</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>madd_6</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>madd_6</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>madd_6</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>madd_6</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>madd_6</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>madd_2</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>madd_2</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>madd_2</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>madd_2</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>madd_2</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>madd_2</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Ikhfaa</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Ikhfaa</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Ikhfaa</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Ikhfaa</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Ikhfaa</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Ikhfaa</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Idgham</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Idgham</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Idgham</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Idgham</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Idgham</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Idgham</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>tafkhim</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>tafkhim</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>tafkhim</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>tafkhim</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>tafkhim</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>tafkhim</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>qalqala</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>qalqala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>qalqala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>qalqala</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>qalqala</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>qalqala</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>imala</td>\n","      <td>Abdul Basit</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>imala</td>\n","      <td>Yassin Al Jazaery</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>imala</td>\n","      <td>Ibrahim_Aldosary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>imala</td>\n","      <td>Al husary</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>imala</td>\n","      <td>safa</td>\n","      <td>238</td>\n","      <td>60</td>\n","      <td>238</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>imala</td>\n","      <td>all reciters</td>\n","      <td>1190</td>\n","      <td>300</td>\n","      <td>1190</td>\n","      <td>300</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f28fbdf-ee09-4bca-9633-47b3bc8e6989')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7f28fbdf-ee09-4bca-9633-47b3bc8e6989 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7f28fbdf-ee09-4bca-9633-47b3bc8e6989');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5c362235-9ad0-4d44-9d6b-10cf9273adba\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c362235-9ad0-4d44-9d6b-10cf9273adba')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5c362235-9ad0-4d44-9d6b-10cf9273adba button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"splitted_data_info","summary":"{\n  \"name\": \"splitted_data_info\",\n  \"rows\": 54,\n  \"fields\": [\n    {\n      \"column\": \"tajweed_rule\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala\",\n          \"madd_246\",\n          \"Idgham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"data_of\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Abdul Basit\",\n          \"Yassin Al Jazaery\",\n          \"all reciters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1190\",\n          \"238\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"300\",\n          \"60\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y_train_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"1190\",\n          \"238\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"X_test_nb_samples\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"300\",\n          \"60\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["models_information"],"metadata":{"id":"h3kOCPqVuemS","colab":{"base_uri":"https://localhost:8080/","height":331},"executionInfo":{"status":"ok","timestamp":1717524754590,"user_tz":-60,"elapsed":20,"user":{"displayName":"Safa BENABDESSADOK","userId":"13084690164500108428"}},"outputId":"e48b58cf-ebca-4830-a0f9-778313dc609b"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                             Model      Loss Accuracy Accuracy %  \\\n","0  madd_6_Lazim_tajweed_rule_model  180.2376   1.0000     100.00   \n","1      madd_246_tajweed_rule_model  259.7652   0.9933      99.33   \n","2        madd_6_tajweed_rule_model  279.0264   0.9500      95.00   \n","3        madd_2_tajweed_rule_model  141.4090   0.9967      99.67   \n","4        Ikhfaa_tajweed_rule_model  632.4830   0.9900      99.00   \n","5        Idgham_tajweed_rule_model  241.1902   0.9900      99.00   \n","6       tafkhim_tajweed_rule_model  388.3080   0.8000      80.00   \n","7       qalqala_tajweed_rule_model  327.2716   1.0000     100.00   \n","8         imala_tajweed_rule_model  397.5166   0.1233      12.33   \n","\n","                                   Path_to_the_model  \n","0  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","1  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","2  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","3  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","4  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","5  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","6  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","7  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  \n","8  /content/drive/My Drive/M2 GL/PFE/new Tajweed ...  "],"text/html":["\n","  <div id=\"df-2f83c84b-0b52-4d2a-9a2c-ed2bd9cb1078\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>Loss</th>\n","      <th>Accuracy</th>\n","      <th>Accuracy %</th>\n","      <th>Path_to_the_model</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>madd_6_Lazim_tajweed_rule_model</td>\n","      <td>180.2376</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>madd_246_tajweed_rule_model</td>\n","      <td>259.7652</td>\n","      <td>0.9933</td>\n","      <td>99.33</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>madd_6_tajweed_rule_model</td>\n","      <td>279.0264</td>\n","      <td>0.9500</td>\n","      <td>95.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>madd_2_tajweed_rule_model</td>\n","      <td>141.4090</td>\n","      <td>0.9967</td>\n","      <td>99.67</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ikhfaa_tajweed_rule_model</td>\n","      <td>632.4830</td>\n","      <td>0.9900</td>\n","      <td>99.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Idgham_tajweed_rule_model</td>\n","      <td>241.1902</td>\n","      <td>0.9900</td>\n","      <td>99.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tafkhim_tajweed_rule_model</td>\n","      <td>388.3080</td>\n","      <td>0.8000</td>\n","      <td>80.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>qalqala_tajweed_rule_model</td>\n","      <td>327.2716</td>\n","      <td>1.0000</td>\n","      <td>100.00</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>imala_tajweed_rule_model</td>\n","      <td>397.5166</td>\n","      <td>0.1233</td>\n","      <td>12.33</td>\n","      <td>/content/drive/My Drive/M2 GL/PFE/new Tajweed ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f83c84b-0b52-4d2a-9a2c-ed2bd9cb1078')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2f83c84b-0b52-4d2a-9a2c-ed2bd9cb1078 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2f83c84b-0b52-4d2a-9a2c-ed2bd9cb1078');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b297adad-abc4-4eb1-a2f8-6b39b9121b3f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b297adad-abc4-4eb1-a2f8-6b39b9121b3f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b297adad-abc4-4eb1-a2f8-6b39b9121b3f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"models_information","summary":"{\n  \"name\": \"models_information\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"qalqala_tajweed_rule_model\",\n          \"madd_246_tajweed_rule_model\",\n          \"Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Loss\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"327.2716\",\n          \"259.7652\",\n          \"241.1902\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"1.0000\",\n          \"0.9933\",\n          \"0.8000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy %\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"100.00\",\n          \"99.33\",\n          \"80.00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Path_to_the_model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v4/qalqala_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v4/madd_246_tajweed_rule_model\",\n          \"/content/drive/My Drive/M2 GL/PFE/new Tajweed rule model/AI models/v4/Idgham_tajweed_rule_model\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[],"metadata":{"id":"3UUk_PhGqJdV"},"execution_count":null,"outputs":[]}]}